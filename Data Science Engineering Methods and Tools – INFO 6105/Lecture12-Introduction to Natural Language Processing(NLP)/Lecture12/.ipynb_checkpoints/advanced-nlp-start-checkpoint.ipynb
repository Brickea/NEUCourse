{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture12</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 2 December 2019, with material from Matt Rocklin, J. Nunez-Iglesias, </div>\n",
    "<div style=\"text-align: right\">Frank McSherry, Kiefer Katovich, Joseph Nelson, Charlie Greenbacker, Dan Jurafsky, the NLTK people,</div>\n",
    "<div style=\"text-align: right\">and the European Digital Research Infrastructure for the Arts and Humanities consortium, DARIAH-DE initiative, German branch of DARIAH-EU</div>\n",
    "\n",
    "<br />\n",
    "\n",
    "I like good plots, and I think this one depicts **Data Science** pretty well (movies to movie genres):\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/movie-genres.png\" width=400 />\n",
    "    <center>Dimensionality Reduction</center>\n",
    "</left>\n",
    "\n",
    "This week, we study Natural Language Processing in the context of Data Science, to help you with your final project. This is a loooooooooooooooooooooooooong notebook..\n",
    "\n",
    "## Notebook Contents:\n",
    "- Part 1: [Introduction to Natural Language Processing (NLP)](#section1)\n",
    "- Part 2: [Big Data processing: Optimizations often required for NLP](#section2)\n",
    "- Part 3: [NLP with the Reuters dataset](#section3)\n",
    "- Part 4: [Words 2 Vectors](#section4)\n",
    "- Part 5: [NLP-ing Litterature](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/nlp.jpg\" width=600 />\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is the task of **extracting** information and ***meaning*** (semantics) from text documents.\n",
    "\n",
    "These tasks may range from simple classification tasks, such as deciding what category a piece of text falls into, to more complex tasks like translating or summarizing text.\n",
    "\n",
    "Here are some examples:\n",
    "- [Chatbots](https://dialogflow.com/)\n",
    "- [Text to speech](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "\n",
    "For most tasks, a fair amount of pre-processing is required to make the text digestible for algorithms.  We typically need to add **structure** to our *unstructured* data. [Here](http://www.aaai.org/Magazine/Watson/watson.php) is why, about [IBM's Watson](https://en.wikipedia.org/wiki/Watson_(computer)).\n",
    "\n",
    "> **SIRI**: A system like Siri uses voice-to-transcription to record a command and then various **advanced** NLP algorithms to identify the question asked and possible answers\n",
    "\n",
    "The main **challenges** of NLP are **ambiguity** and **knowledge**:\n",
    "- **Ambiguity**:\n",
    "    - *Hospitals Are Sued by 7 Foot Doctors*\n",
    "    - *Juvenile Court to Try Shooting Defendant*\n",
    "    - *Local High School Dropouts Cut in Half*\n",
    "- **Knowledge:** \n",
    "    - *Mary and Sue are sisters, Mary and Sue are mothers*.\n",
    "\n",
    "**Tokenization** is the task of separating a sentence into its constituent parts, or **tokens**. *Example*: Data Science is the future! → [`Data Science`, `is`, `the future`].\n",
    "\n",
    "**Stemming** and **lemmatization** help identify common roots of words. Stemming is a crude process of removing common endings from sentences, such as `s`, `es`, `ly`, `ing`, and `ed`. *Example*: `badly` → `bad`. Lemmatization is a more refined process that uses specific language and grammar rules to derive the root of a word. *Example*: Best and better are the enemy of good → [`Most good` and `more good` are the enemy of good].  \n",
    "\n",
    "**Tagging** and **parsing** is the process of understanding the **grammar** of a sentence: We need to **tag** important topics and **parse** their dependencies.  \n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/john-hit-the-ball.png\" width=250 />\n",
    "</left>\n",
    "\n",
    "Our goal is to identify the actors and actions in the text in order to extract meaning.\n",
    "\n",
    "Tagging and parsing is made up of a few overlapping subproblems:\n",
    "\n",
    "- **Parts of speech** (POS) tagging: What are the parts of speech in a sentence (e.g. noun, verb, adjective, etc)?\n",
    "\n",
    "- **Chunking**: Can we identify the pieces of the sentence that go together in meaningful chunks (e.g. noun or verb phrases)?\n",
    "\n",
    "- **Named entity recognition**: Can we identify specific proper nouns?  Can we pick out people and locations?\n",
    "\n",
    "The consortium [Universal Dependencies](https://universaldependencies.org/) a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. UD is an open community effort with over 200 contributors producing more than 100 treebanks in over 70 languages, including english, hindi, chinese, and ancient and modern greek: ***One*** framework, ***many*** languages. [Google](https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html) and [Microsoft](https://www.microsoft.com/en-us/research/group/natural-language-processing/) dominate this field with implementations. Google's implementation has a *funny* name: Parsey McParseface (`Parse`y Mc`Parse`face). It is a joke from the [Boaty McBoatface](https://www.nytimes.com/2016/03/22/world/europe/boaty-mcboatface-what-you-get-when-you-let-the-internet-decide.html) debacle. [Try it out](https://deepai.org/machine-learning-model/parseymcparseface) on a Web page. Here's how to try it out on a command line:\n",
    "\n",
    ">**CURLing**: curl \\\n",
    "    -F 'sentence=YOUR_TEXT_URL' \\\n",
    "    -H 'api-key:quickstart-QUdJIGlzIGNvbWluZy4uLi4K' \\\n",
    "    https://api.deepai.org/api/parseymcparseface "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text classification with BAG-OF-WORDS\n",
    "\n",
    "**Text classification** is the task of predicting which category or topic a text sample is from. Typically, this is done by using the text as features and the label as the target output.  This is referred to as [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) classification.\n",
    "\n",
    "For example, we may want to identify:\n",
    "- Is an article a sports or business story?\n",
    "- Does an email have positive or negative sentiment?\n",
    "- Is the rating of a recipe 1, 2, 3, 4, or 5 stars?\n",
    "\n",
    "To include text as features, we usually create a binary feature for each word, i.e. does this piece of text contain that word? To create binary text features, we first create a vocabulary to account for all possible words in our universe. But you need to consider several things to decide if bag-of-words is appropriate:\n",
    "\n",
    "- Does order of words matter?\n",
    "- Does punctuation matter?\n",
    "- Does upper or lower case matter?\n",
    "\n",
    "`Scikit-learn`, in package `sklearn.preprocessing.text`, has many pre-processing utilities that simplify tasks required to convert text into features for a model.\n",
    "\n",
    "`CountVectorizer` converts a collection of text into a matrix of features.  Each row will be a sample (an article or piece of text) and each column will be a text feature (usually a count or binary feature per word).\n",
    "\n",
    "> **OVERFITTING**: Using all of the words can be useful, but YOU may need to use regularization to avoid overfitting.  Otherwise, rare words may cause the model to overfit and not generalize.\n",
    "\n",
    "`CountVectorizer` code:\n",
    "```(PYTHON)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\u000b",
    "\u000b",
    "vectorizer = CountVectorizer(max_features = 1000, \u000b",
    "                             ngram_range=(1, 2), \u000b",
    "                             stop_words='english',\u000b",
    "                             binary=True)\n",
    "\n",
    "ngram_range - a range of word phrases to use\n",
    "(1,1) means use all single words\n",
    "(1,2) means use all singles +  contiguous pairs of word\n",
    "(1,3) means use all singles + pairs + triples\n",
    "\n",
    "stop_words=’english’\n",
    "Stop words are non-content words (e.g. ‘to’, ‘the’, ‘it’, etc).  They aren’t helpful for prediction, so they get removed!\n",
    "\n",
    "max_features=1000\n",
    "Maximum number of words to consider (uses the first N most frequent)\n",
    "\n",
    "binary=True\n",
    "To use a dummy column as the entry (1 or 0, as opposed to the count).  This is useful if you think a word appearing 10 times is no more important than whether the word appears at all.\n",
    "```\n",
    "\n",
    "`Vectorizer`s are like other transformers in scikit-learn (e.g. `StandardScaler`). We create a vectorizer object with the parameters of our feature space. We fit a vectorizer to learn the vocabulary. We transform a set of text into that feature space.\n",
    "\n",
    "> Note: there is a distinction between `fit` and `transform`. We `fit` from our training set.  This is part of the model building process, so we don’t look at our test set. We `transform` our test set using our model fit on the training set.\n",
    "\n",
    "`fit` and `transform` code:\n",
    "```(python)\n",
    "titles = data['title'].fillna('')\u000b",
    "\u000b",
    "from sklearn.feature_extraction.text import CountVectorizer\u000b",
    "\u000b",
    "vectorizer = CountVectorizer(max_features = 1000, \n",
    "                ngram_range=(1, 2), \n",
    "                stop_words='english',\n",
    "                binary=True)\n",
    "               \n",
    "# Use `fit` to learn the vocabulary of the titles \n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text classification with TFIDF\n",
    "\n",
    "An alternative bag-of-words approach to CountVectorizer is a [Term Frequency - Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (TF-IDF) representation.\n",
    "\n",
    "TF-IDF uses the product of two intermediate values, the **Term Frequency** and **Inverse Document Frequency**.\n",
    "\n",
    "Term Frequency is equivalent to CountVectorizer features, just the number of times a word appears in the document (i.e. count).\n",
    "\n",
    "Document Frequency is the percentage of documents that a particular word appears in and Inverse Document Frequency is just 1/Document Frequency.\n",
    "\n",
    "Combining, TF-IDF = Term Frequency * Inverse Document Frequency or TF-IDF = Term Frequency / Document Frequency\n",
    "\n",
    "The intuition is that the words that have high weight are those that either appear frequently in this document or appear rarely in other documents (and are therefore unique to this document).  \n",
    "\n",
    "This is a good alternative to using a static set of “stop” words.\n",
    "```(python)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a random forest model to predict words\n",
    "\n",
    "We may want to build a **predictive text box**, the way Google does it on their Web page. In that case, the **independent variable** is a partial (incomplete) sequence, and the **dependent variable** (y) is a word: \n",
    "```(python)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators = 20)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles (titles is a vector of sentences)\n",
    "titles = data['title'].fillna('')\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample x word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)\n",
    "\n",
    "# learn\n",
    "y = data['label']\n",
    "model.fit(X, y)\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. *More* than Binary: Vectorization\n",
    "\n",
    "When building a machine learning model, we typically must transform our data into **numeric** features. We already talked about the simple example of creating a binary feature (0 or 1, True or False) per word.If you do this in ***multiple dimensions***, then this process of transforming non-numeric data into numeric features is called [**vectorization**](https://en.wikipedia.org/wiki/Word_embedding). \n",
    "\n",
    "### Install TextBlob\n",
    "\n",
    "This is an **introductory** class to Data Science. So we learn the ***simplest*** things (sometimes you think professor is crazy because GPs are not the simplest things). But when you'll start working you will understand that professor was very gentle on you, teaching you the simplest things, first!\n",
    "\n",
    "The `TextBlob` Python library provides a simplified interface for exploring common NLP tasks including `part-of-speech tagging`, `noun phrase extraction`, `sentiment analysis`, `classification`, `translation`, and more. Another popular python NLP library is [Spacy](https://spacy.io).\n",
    "\n",
    "First, install TextBlob.\n",
    "\n",
    "**To install textblob run:**\n",
    "\n",
    "> `conda install -c conda-forge textblob`\n",
    "\n",
    "**Or:**\n",
    "\n",
    "> `pip install textblob`\n",
    "\n",
    "> `python -m textblob.download_corpora lite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classic NLP use case: Yelp reviews and spam filtering\n",
    "\n",
    "A [corpus](https://en.wikipedia.org/wiki/Corpus) is a collection of documents (derived from the Latin word for \"body\"). **corpora** is the plural form of corpus. We'll use a CountVectorizer on the yelp dataset on blackboard. [Here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is CountVectorizer's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yelp.csv into a DataFrame.\n",
    "path = r'data/yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "\n",
    "# Create a new DataFrame that only contains the 5-star (good) and 1-star (bad) reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# Define X and y.\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head of the original data\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) is a mathematical matrix that describes the **frequency** of **terms** (words) that occur in a collection of documents. In a document-term matrix, **rows** correspond to **documents** in the collection and **columns** correspond to **terms**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vectorizer options.\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building models, **dimensionality reduction** is paramount. Brain cells, whether natural are artificial are *limited*!\n",
    "\n",
    "One common method of reducing the number of features is converting all text to lowercase before generating features! Note that to a computer, `aPPle` is a different token/\"word\" than `apple`. So, by converting both to lowercase letters, it ensures fewer features will be generated. \n",
    "\n",
    "Sometimes though, when capitalization matters, it is better ***not*** to convert them to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't convert to lowercase.\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape\n",
    "[word for word in vect.get_feature_names() if word.startswith('Z')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Let's use [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) to predict star rating. Naive Bayes is a simple probabilistic classifier based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. We'll first learn star rating. Then we'll be able to predict just by reading a review.\n",
    "\n",
    "Naive Bayes is a popular classifier because it has minimal storage requirements, is fast, can be tuned easily with more data and has found very useful applications in text classificaton. Paul Graham originally proposed using Naive Bayes to detect spam in his [Plan for Spam](http://www.paulgraham.com/spam.html).\n",
    "\n",
    ">**NOTE**: The ***naive*** assumption of Naive Bayes, that features are **conditionally independent**, that the normalization constant (the denominator) can be ignored since it's the same for all classes, and that t The prior probability is much less relevant once you have a lot of features are ***critical*** to making these calculations simple\n",
    "\n",
    "Recall that Bayes is all about adjusting probabilities as more data is gathered:\n",
    "\n",
    "$$P(A \\ | \\ B) = \\frac {P(B \\ | \\ A) \\times P(A)} {P(B)}$$\n",
    "\n",
    "- **$P(A \\ | \\ B)$** : Probability of `Event A` occurring given `Event B` has occurred.\n",
    "- **$P(B \\ | \\ A)$** : Probability of `Event B` occurring given `Event A` has occurred.\n",
    "- **$P(A)$** : Probability of `Event A` occurring.\n",
    "- **$P(B)$** : Probability of `Event B` occurring.\n",
    "\n",
    "Advantages of Naive Bayes are:\n",
    "\n",
    "- Model training and prediction are very fast.\n",
    "- It's somewhat interpretable.\n",
    "- No tuning is required.\n",
    "- Features don't need scaling.\n",
    "- It's insensitive to irrelevant features (with enough observations).\n",
    "- It performs better than logistic regression when the training set is very small.\n",
    "\n",
    "Disadvantages of Naive Bayes are:\n",
    "\n",
    "- If \"spam\" is dependent on non-independent combinations of individual words, it may not work well.\n",
    "- Predicted probabilities are not well calibrated.\n",
    "- Correlated features can be problematic (due to the independence assumption).\n",
    "- It can't handle negative features (with Multinomial Naive Bayes).\n",
    "- It has a higher \"asymptotic error\" than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "Let's pretend we have an email with three words: \"Send money now.\" We'll use Naive Bayes to classify it as **ham or spam** (*ham* means not spam; it can include emails that look like spam but you opt into).\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) = \\frac {P(\\text{send money now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "By assuming that features (words) are **conditionally independent**, we can simplify the likelihood function:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {P(\\text{send} \\ | \\ spam) \\times P(\\text{money} \\ | \\ spam) \\times P(\\text{now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "Note that each conditional probability in the numerator is easily calculated directly from training data!\n",
    "\n",
    "So, we can calculate all of the values in the numerator by examining a corpus of spam email:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
    "\n",
    "Now repeat this process with a corpus of ham email:\n",
    "\n",
    "$$P(ham \\ | \\ \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
    "\n",
    "All we care about is whether spam or ham has the higher probability, $P(\\text{send money now}))$ is the same on both equations, and so we predict that the email is spam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default options for CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# Use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# Calculate accuracy\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test==5, 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent 5 Stars:', y_test_binary.mean())\n",
    "print('Percent 1 Stars:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Naive Bayes model a few cells back predicted ~92% accuracy, better than `fit_transform`'s 82% accuracy!\n",
    "\n",
    " Notice how the data was transformed into this sparse matrix with 1,022 datapoints and 16,825 features:\n",
    "   - Recall that vectorizations of text will be **mostly zeros**, since only a few unique words are in each document.\n",
    "   - For that reason, instead of storing all the zeros we only store non-zero values (inside the 'sparse matrix' data structure!)\n",
    "   - We have 3064 Yelp reviews in our training set\n",
    "   - 16,825 unique words were found across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the vocabulary that was generated, containing 16,825 unique words. 'vocabulary_' is a dictionary that converts each word to its index in the sparse matrix. For example, the word `only` is index \\#10362 in the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's convert the sparse matrix to a typical ndarray using .toarray(). Remember, this takes up ***a lot*** more memory than the sparse matrix! However, this conversion is sometimes necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dtm_dense = X_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function below for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts a vectorizer and calculates accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print(('Features: ', X_train_dtm.shape[1]))\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print(('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`min_df` ignores words that occur less than twice (`df` means **document frequency**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=2, max_features=10000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "\n",
    "N-grams are features which consist of N consecutive words. This is useful because, for example, when using the bag-of-words model, treating `data scientist` as a single feature has more meaning than having two independent features `data` and `scientist`.\n",
    "\n",
    "Example:\n",
    "```\n",
    "my cat is awesome\n",
    "Unigrams (1-grams): 'my', 'cat', 'is', 'awesome'\n",
    "Bigrams (2-grams): 'my cat', 'cat is', 'is awesome'\n",
    "Trigrams (3-grams): 'my cat is', 'cat is awesome'\n",
    "4-grams: 'my cat is awesome'\n",
    "```\n",
    "\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see how supplementing our features with n-grams can lead to more feature columns: When we produce n-grams from a document with $W$ words, we add an additional $(n-W+1)$ features (at most)! This is similar to what is traditionally done with CNN ANNs, where we additionally train the model with rotated pictures of the stock training data set.\n",
    "\n",
    "That said, be careful — when we compute n-grams from an entire corpus, the number of ***unique*** n-grams could be vastly higher than the number of unique unigrams! This could cause an undesired feature explosion.\n",
    "\n",
    "Although we sometimes add important new features that have meaning such as `data scientist`, many of the new features could just be noise. So, particularly if we do not have much data, adding n-grams can actually decrease model performance. This is because if each n-gram is only present once or twice in the training set, we are effectively adding mostly noisy features to the mix. So, be careful.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-Word Removal\n",
    "\n",
    "[Stop-word](https://en.wikipedia.org/wiki/Stop_words) removal is used to remove common words that will likely appear in any text. Because common words exist in ***most*** documents, they likely only add noise to your model and should be removed.\n",
    "\n",
    "**Stop words** are some of the most common words in a language. They are used so that a sentence makes sense grammatically, such as prepositions and determiners, e.g., \"to,\" \"the,\" \"and.\" However, they are so commonly used that they are generally worthless for predicting the class of a document. Since `a` appears in spam and non-spam emails, for example, it would only contribute noise to our model.\n",
    "\n",
    "Example: \n",
    "\n",
    "> 1. Original sentence: \"The dog jumped over the fence\"  \n",
    "> 2. After stop-word removal: \"dog jumped over fence\"\n",
    "\n",
    "The fact that there is a fence and a dog jumped over it can be derived with or without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vectorizer options.\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {`english`}, list, or None (default)\n",
    "- If `english`, a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. `max_df` can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. (If `max_df` = 0.7, then if > 70% of documents contain a word it will not be included in the feature set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of stop words\n",
    "print((vect.get_stop_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove English stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)\n",
    "vect.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other CountVectorizer Options\n",
    "\n",
    "- `max_features`: int or None, default=None\n",
    "\n",
    "If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. This allows us to keep more common n-grams and remove ones that may appear once. If we include words that only occur once, this can lead to said features being highly associated with a class and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove English stop words and only keep 100 features.\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 100 features\n",
    "print((vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with all other models, more features does not mean a better model. So, we must tune our feature generator to remove features whose predictive capability is none or very low.\n",
    "\n",
    "In this case, there is roughly a 1.6% increase in accuracy when we double the n-gram size and increase our max features by 1,000-fold. Note that if we restrict it to only unigrams, then the accuracy increases even more! So, bigrams were very likely adding more noise than signal. \n",
    "\n",
    "In the end, by only using 16,000 unigram features we came away with a much smaller, simpler, and easier-to-think-about model which also resulted in higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include 1-grams and 2-grams, and limit the number of features\n",
    "\n",
    "print('1-grams and 2-grams, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)\n",
    "\n",
    "print()\n",
    "print('1-grams only, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 1), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `min_df`: Float in range [0.0, 1.0] or int, default=1\n",
    "\n",
    "When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. This value is also called **cut-off** in the literature. If float, the parameter represents a *proportion of documents*, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include 1-grams and 2-grams, and only include terms that appear at least two times.\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textblob'></a>\n",
    "### Introduction to TextBlob\n",
    "\n",
    "`TextBlob`, which we already installed, is a Python library used to explore common NLP tasks. You can read more about TextBlob [here](https://textblob.readthedocs.io/en/dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first review.\n",
    "print((yelp_best_worst.text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save it as a `TextBlob` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list the words and sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some string methods are immediately available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "As we saw before, **stemming** is a crude process of removing common endings from sentences, such as \"s\", \"es\", \"ly\", \"ing\", and \"ed\". This intelligently reduces the number of features by grouping together (hopefully) related words. Stemming uses a simple and fast rule-based approach, stemmed words are usually not shown to users (used for analysis/indexing), and some search engines treat words with the same stem as synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer.\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Stem each word.\n",
    "stemmed1 = [stemmer.stem(word) for word in review1.words]\n",
    "print(len(stemmed1))\n",
    "print(stemmed1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n",
    "\n",
    "This is useful for words that do not share an obvious root such as \"better\" and \"best\". Lemmatization derives the **canonical form** (\"lemma\") of a word. It can be better than stemming. It uses a dictionary-based approach (slower than stemming). But we need to first assume whether words are nouns or verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume every word is a noun\n",
    "lemmatized1 = [word.lemmatize() for word in review1.words]\n",
    "print(len(lemmatized1))\n",
    "print(lemmatized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume every word is a verb\n",
    "lemmatized1v = [word.lemmatize(pos='v') for word in review1.words]\n",
    "print(len(lemmatized1v))\n",
    "print(lemmatized1v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are `took` lemmatized to `take` and `was` lemmatized to `be`.\n",
    "\n",
    "Assuming Parts of Speech, means we need to do a POS analysis, first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns a list of lemmas.\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]\n",
    "\n",
    "# Use split_into_lemmas as the feature extraction function (Warning: SLOW!).\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas, decode_error='replace')\n",
    "tokenize_test(vect)\n",
    "\n",
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all available options for `CountVectorizer()`, you may wonder how to decide which to use! It's true that you can sometimes reason about which preprocessing techniques might work best. However, you will often not know for sure without trying out many different combinations and comparing their accuracies. \n",
    "\n",
    "> **NOTE**: Keep in mind that you should constantly be thinking about the result of each preprocessing step instead of blindly trying them without thinking. Does each type of preprocessing \"makes sense\" with the input data you are using? Is it likely to keep intact the signal and remove noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency–Inverse Document Frequency (TF–IDF)\n",
    "\n",
    "While a Count Vectorizer simply totals up the number of times a word appears in a document, the more complex **TF-IDF Vectorizer** analyzes the uniqueness of words ***between documents** to find distinguishing characteristics. \n",
    "     \n",
    "Term frequency–inverse document frequency (TF–IDF) computes the \"relative frequency\" with which a word appears in a document, compared to its frequency across all documents. It's more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents). It's most often arch-engine scoring, text summarization, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']\n",
    "\n",
    "# Term frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency–inverse document frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***higher*** the TF–IDF value, the more ***important*** the word is to that specific document. \n",
    "\n",
    "Here, `cab` is the most important and unique word in document 1, while `please` is the most important and unique word in document 2. TF–IDF is often used for training as a replacement for word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use TF–IDF to Summarize a Yelp Review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term matrix using TF–IDF.\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit transform Yelp data.\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # Choose a random review that is at least 300 characters long\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = yelp.text[review_id]\n",
    "        #review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # Create a dictionary of words and their TF–IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # Print words with the top five TF–IDF scores\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(list(word_scores.items()), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "    \n",
    "    # Print five random words\n",
    "    print(('\\n' + 'RANDOM WORDS:'))\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # Print the review\n",
    "    print(('\\n' + review_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis is understanding how **positive** or **negative** a review is. There are many ways in practice to compute a sentiment value. For example:\n",
    "\n",
    "- Have a list of positive words and a list of negative words and count how many occur in a document. \n",
    "- Train a classifier given many examples of positive documents and negative documents. \n",
    "    - Note that this technique is often just an automated way to derive the first (e.g., using bag-of-words with logistic regression, a coefficient is assigned to each word!).\n",
    "\n",
    "For the most accurate sentiment analysis, you will want to train a custom sentiment model based on documents that are particular ***to your application***. Generic models (such as the one we are about to use) often do not work as well as hoped.\n",
    "\n",
    "As we will do below, always make sure you double-check that the algorithm is working by manually verifying that scores correctly correspond to positive/negative reviews! Otherwise, you may be using numbers that are not accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity ranges from -1 (most negative) to 1 (most positive).\n",
    "review1.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the apply method\n",
    "yelp['length'] = yelp.text.apply(len)\n",
    "yelp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame column for sentiment (Warning: SLOW!).\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widen the column display.\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sentiment in a 5-star review\n",
    "yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive sentiment in a 1-star review\n",
    "yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the column display width.\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features to a Document-Term Matrix\n",
    "\n",
    "Let's add additional features to our `CountVectorizer()`-generated feature set to hopefully improve our model.\n",
    "\n",
    "To make the best models, you will want to supplement the auto-generated features with new features you think might be important. After all, `CountVectorizer()` typically lowercases text and removes all associations between words. Or, you may have metadata to add in addition to just the text.\n",
    "\n",
    "> **NOTE**: Although you may have hundreds of thousands of features, each data point is extremely sparse. So, if you add in a new feature, e.g., one that detects if the text is all capital letters, this new feature can still have a huge effect on  model outcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst[feature_cols]\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer with text column only.\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print((X_train_dtm.shape))\n",
    "print((X_test_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sparse matrices\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for testing set\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language identification\n",
    "TextBlob('’Είπε ο γάιδαρος τον πετεινό κεφάλα').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation\n",
    "TextBlob('’Είπε ο γάιδαρος τον πετεινό κεφάλα').translate(to=\"en\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE**: The correct translation is *the donkey called the rooster big-headed*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob('जंगल में मोर नाचा किस ने देखा ?').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob('जंगल में मोर नाचा किस ने देखा ?').translate(to=\"en\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE**: The correct translation is *Who saw a peacock dance in the woods?*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob('授人以鱼不如授人以渔').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob('授人以鱼不如授人以渔').translate(to=\"en\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE**: The more litteral translation is *Giving a man a fish is not equal to teaching a man to fish*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Contents:\n",
    "- Part 1: [Introduction to Natural Language Processing (NLP)](#section1)\n",
    "- Part 2: [Big Data processing: Optimizations often required for NLP](#section2)\n",
    "- Part 3: [NLP with the Reuters dataset](#section3)\n",
    "- Part 4: [Words 2 Vectors](#section4)\n",
    "- Part 5: [NLP-ing Litterature](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 2: Big Data processing: Optimizations often required for NLP\n",
    "\n",
    "Artificial Neural Networks (ANN) learning to do Natural Language Processing (NLP) require tons of data to learn correctly. So I need to show you some Big Data techniques.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/big-data.png\" width=600 />\n",
    "</left>\n",
    "\n",
    "**Streaming** is not a `SciPy` feature per se, but rather an approach that\n",
    "allows you to efficiently process large datasets, like those often\n",
    "seen in science or in Natural Langiage Processing. \n",
    "\n",
    "The Python language contains some useful primitives\n",
    "for streaming data processing, and these can be combined with the\n",
    "`Toolz` library to generate elegant, concise code that is extremely\n",
    "memory-efficient. \n",
    "\n",
    "Let's apply these streaming concepts to enable you to \n",
    "handle much larger datasets than can fit in your computer's RAM.\n",
    "\n",
    "You have probably already done some streaming, perhaps without thinking about it in these terms.\n",
    "The simplest form is probably iterating through lines in a files, processing each line without ever reading the entire file into memory.\n",
    "\n",
    "For example a loop like this to calculate the mean of each row and sum them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('data/expr.tsv') as f:\n",
    "    sum_of_means = 0\n",
    "    for line in f:\n",
    "        sum_of_means += np.mean(np.fromstring(line, dtype=int, sep='\\t'))\n",
    "print(sum_of_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy works really well for cases where your problem can be neatly solved with by-row processing.\n",
    "But things can quickly get out of hand when your code becomes more sophisticated.\n",
    "\n",
    "In streaming programs, a function processes *some* of the input data, returns the\n",
    "processed chunk, then, while downstream functions are dealing with that chunk,\n",
    "the function receives a bit more, and so on...\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/processing.jpg\" width=400 />\n",
    "</left>\n",
    "\n",
    "Constructs in the `toolz` library make streaming programs easy and elegant to write.\n",
    "\n",
    "Suppose you have some data in a text file, and you want to compute the column-wise average of $\\log(x+1)$ of the values.\n",
    "The common way to do this would be to use `NumPy` to load the values, compute the `log` function for all values in the full matrix, and then take the mean over the 1st axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "expr = np.loadtxt('data/expr.tsv')\n",
    "logexpr = np.log(expr + 1)\n",
    "np.mean(logexpr, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, and it follows a reassuringly familiar input-output model of computation.\n",
    "But it's pretty ***inefficient**:\n",
    "\n",
    "- We load the full matrix into memory (1), \n",
    "- then make a copy with 1 added to each value (2), \n",
    "- then make another copy to compute the log (3), \n",
    "- before finally passing it on to `np.mean`.\n",
    "\n",
    "That's ***three*** instances of the data array, to perform an operation that doesn't require keeping even *one* instance in memory.\n",
    "\n",
    "For any kind of **big data** operation, this approach won't work.\n",
    "\n",
    "Python's creators knew this, and built-in the **yield** keyword (first made widely available in C#), which enables a function to process just one ***sip*** of the data, pass the result on to the next process, and *let the chain of processing complete* for that one piece of data before moving on to the next one.\n",
    "\n",
    "**Yield** is a rather nice name for it: the function *yields* control to the next function, waiting to resume processing the data until all the downstream steps have processed that data point. The yield construct is what makes *generators* or *coroutines* possible: the ability to leave a funcion midstream, and then when you call the function again you don't enter through the start of the function, but where you left off: at *yield return*. That is why generators and coroutines return collections: each item of that collection is the return of a single *yield return*.\n",
    "\n",
    "## Streaming with `yield`\n",
    "\n",
    "Here's another way to think about it: for every processing function that would normally take a list (a collection of data) and transform that list, you can rewrite that function as taking a *stream* and *yielding* the result of *every element* in that stream.\n",
    "\n",
    "Here's an example where we take the log of each element in a list, using either a standard data-copying method or a streaming method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_all_standard(input):\n",
    "    output = []\n",
    "    for elem in input:\n",
    "        output.append(np.log(elem))\n",
    "    return output\n",
    "\n",
    "def log_all_streaming(input_stream):\n",
    "    for elem in input_stream:\n",
    "        yield np.log(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we get the same result with both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the random seed so we will get consistent results\n",
    "np.random.seed(seed=7)\n",
    "# Set print options to show only 3 significant digits\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "arr = np.random.rand(1000) + 0.5\n",
    "result_batch = sum(log_all_standard(arr))\n",
    "print('Batch result: ', result_batch)\n",
    "result_stream = sum(log_all_streaming(arr))\n",
    "print('Stream result: ', result_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the streaming approach is that elements of a stream ***aren't processed until they're needed***, whether it's for computing a running sum, or for writing out to disk, or something else. This is called **lazy processing**, or [lazy initialization](https://en.wikipedia.org/wiki/Lazy_initialization).\n",
    "\n",
    "This can conserve a lot of memory when you have many input items, or when each item is very big.\n",
    "This quote from one of Matt Rocklin's (author of `Toolz`) blog posts very succinctly summarizes the utility of streaming data analysis:\n",
    "\n",
    "> In my brief experience people rarely take this [streaming] route.\n",
    "They use single-threaded in-memory Python until it breaks, and then seek out Big Data Infrastructure like Hadoop/Spark at relatively high productivity overhead.\n",
    "\n",
    "Yup, pretty classic: *Professor, we need to learn spark because that's the only way we can do bigdata processing*. Nope, not the right answer. The answer is streaming, sparse matrices, and elegant algorithms in `SciPy`.\n",
    "\n",
    "In some cases, streaming can get you there even faster than the supercomputing approach, by eliminating the overhead of multi-core communication and random-access to databases. See this [post](http://www.frankmcsherry.org/graph/scalability/cost/2015/02/04/COST2.html) by Frank McSherry, and then [this](http://www.frankmcsherry.org/graph/scalability/cost/2015/02/04/COST2.html) one, where he processes a 128 billion edge graph on his laptop *faster* than using a graph database on a supercomputer (open the Web page, but don't read it now because if i say what's going to be on your final exam next, you're going to miss it).\n",
    "\n",
    "Then stop and think.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/snake-oil-salesman.jpg\" width=300 />\n",
    "    Buddy, you really need to use xyz framework, it will solve all your problems bro!\n",
    "</center>\n",
    "\n",
    "To clarify the flow of control when using streaming-style functions, it's useful to make *verbose* versions of the functions, which print out a message with each operation, slow down the throttle to give you the time to read the messages, and use `tqdm` to show progress.\n",
    "```(python)\n",
    "pip install tqdm\n",
    "```\n",
    "\n",
    "Here below, we ***simulate*** reading from a big file using python `sleep()` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tsv_line_to_array(line):\n",
    "    lst = [float(elem) for elem in line.rstrip().split('\\t')]\n",
    "    return np.array(lst)\n",
    "\n",
    "def readtsv(filename):\n",
    "    print('starting readtsv')\n",
    "    num_lines = 0\n",
    "    #statinfo = os.stat(filename)\n",
    "    #statinfo.st_size\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            num_lines += 1\n",
    "    with tqdm(total=num_lines) as pbar:\n",
    "        with open(filename) as fin:\n",
    "            for i, line in enumerate(fin):\n",
    "                print(f'reading line {i}')\n",
    "                sleep(2)\n",
    "                pbar.update(1)\n",
    "                yield tsv_line_to_array(line)\n",
    "    print('finished readtsv')\n",
    "\n",
    "def add1(arrays_iter):\n",
    "    print('starting adding 1')\n",
    "    for i, arr in enumerate(arrays_iter):\n",
    "        print(f'adding 1 to line {i}')\n",
    "        yield arr + 1\n",
    "    print('finished adding 1')\n",
    "\n",
    "def log(arrays_iter):\n",
    "    print('starting log')\n",
    "    for i, arr in enumerate(arrays_iter):\n",
    "        print(f'taking log of array {i}')\n",
    "        yield np.log(arr)\n",
    "    print('finished log')\n",
    "\n",
    "def running_mean(arrays_iter):\n",
    "    print('starting running mean')\n",
    "    for i, arr in enumerate(arrays_iter):\n",
    "        if i == 0:\n",
    "            mean = arr\n",
    "        mean += (arr - mean) / (i + 1)\n",
    "        print(f'adding line {i} to the running mean')\n",
    "    print('returning mean')\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see it in action for a small sample file. We slow everything down with `sleep()`, and leverage `tqdm`. For efficiency, you would go in and remove the `sleep` calls when reading in a big file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = 'data/expr.tsv'\n",
    "print('Creating lines iterator')\n",
    "lines = readtsv(fin)\n",
    "print('Creating loglines iterator')\n",
    "loglines = log(add1(lines))\n",
    "print('Computing mean')\n",
    "mean = running_mean(loglines)\n",
    "print(f'the mean log-row is: {mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- None of the computation is run when creating the lines and loglines iterators. This is because iterators are *lazy*, meaning they are not evaluated (or *consumed*) until a result is needed.\n",
    "- When the computation is finally triggered, by the call to `running_mean`, it jumps back and forth between all the functions, as various computations are performed on each line, before moving on to the next line.\n",
    "\n",
    "## Use case: The Toolz streaming library\n",
    "\n",
    "I'm giving you this use case to study, but we won't do this together in class today (no time). I taught a class in **Life sciences with python** at Harvard, and this is an example I studied with Harvard students.\n",
    "\n",
    "Matt Rocklin wrote `Toolz` and illustrates the library with a Markov model from a human genome in under 5 minutes on a laptop, using a few lines of code.\n",
    "\n",
    "J. Nunez Iglesias applied the algorithm to a fly genome (about 1/20 the size), so this is better for class.\n",
    "\n",
    "Please download the genome from [here](http://hgdownload-test.cse.ucsc.edu/goldenPath/dm6/bigZips/): the file `dm6.fa.gz`, and uncompress it with 7-zip to `dm6.fa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolz as tz\n",
    "from toolz import curried as c\n",
    "from glob import glob\n",
    "import itertools as it\n",
    "\n",
    "# Adenine, Cytosine, Guanine, Tyramine:\n",
    "LDICT = dict(zip('ACGTacgt', range(8)))\n",
    "PDICT = {(a, b): (LDICT[a], LDICT[b])\n",
    "         for a, b in it.product(LDICT, LDICT)}\n",
    "\n",
    "def is_sequence(line):\n",
    "    return not line.startswith('>')\n",
    "\n",
    "def is_nucleotide(letter):\n",
    "    return letter in LDICT  # ignore 'N'\n",
    "\n",
    "@tz.curry\n",
    "def increment_model(model, index):\n",
    "    model[index] += 1\n",
    "\n",
    "    \n",
    "def genome(file_pattern):\n",
    "    \"\"\"Stream a genome, letter by letter, from a list of FASTA filenames.\"\"\"\n",
    "    return tz.pipe(file_pattern, glob, sorted,  # Filenames\n",
    "                   c.map(open),  # lines\n",
    "                   # concatenate lines from all files:\n",
    "                   tz.concat,\n",
    "                   # drop header from each sequence\n",
    "                   c.filter(is_sequence),\n",
    "                   # concatenate characters from all lines\n",
    "                   tz.concat,\n",
    "                   # discard newlines and 'N'\n",
    "                   c.filter(is_nucleotide))\n",
    "\n",
    "\n",
    "def markov(seq):\n",
    "    \"\"\"Get a 1st-order Markov model from a sequence of nucleotides.\"\"\"\n",
    "    model = np.zeros((8, 8))\n",
    "    tz.last(tz.pipe(seq,\n",
    "                    c.sliding_window(2),        # each successive tuple\n",
    "                    c.map(PDICT.__getitem__),   # location in matrix of tuple\n",
    "                    c.map(increment_model(model))))  # increment matrix\n",
    "    # convert counts to transition probability matrix\n",
    "    model /= np.sum(model, axis=1)[:, np.newaxis]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then do the following to obtain a Markov model of repetitive sequences\n",
    "in the fruit-fly genome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "dm = 'data/dm6.fa'\n",
    "model = tz.pipe(dm, genome, c.take(10**7), markov)\n",
    "# we use `take` to just run on the first 10 million bases, to speed things up.\n",
    "# the take step can just be removed if you have ~5-10 mins to wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many functions in the [Toolz](http://toolz.readthedocs.org/en/latest/) library.\n",
    "For example: `pipe`, `sliding_window`, `frequencies`, and a curried version of `map`.\n",
    "Toolz is written specifically to take advantage of Python's iterators and easily manipulate streams.\n",
    "\n",
    "`pipe` is simply syntactic sugar to make nested function calls easier to read.\n",
    "This is important because that pattern becomes increasingly common when dealing with iterators.\n",
    "\n",
    "As a simple example, let's rewrite the running mean example using `pipe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolz as tz\n",
    "filename = 'data/expr.tsv'\n",
    "mean = tz.pipe(filename, readtsv, add1, log, running_mean)\n",
    "\n",
    "# This is equivalent to nesting the functions like this:\n",
    "# running_mean(log(add1(readtsv(filename))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was originally multiple lines, or an unwieldy mess of parentheses, is now a clean description of the sequential transformations of the input data.\n",
    "\n",
    "Much easier to understand! Note that this operator is not the genius of Matt Rockland, but it's actually a port of the `pipe` operator in `R`. I briefly mentionned it in our first lecture together, and told you that it's a *rockin'* operator.\n",
    "\n",
    "This strategy also has an advantage over the original NumPy implementation: if we scale our data to millions or billions of rows, our computer might struggle to hold all the data in memory.\n",
    "In contrast, here we are only loading lines from disk one at a time, and maintaining only a single line's worth of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-mer counting and error correction\n",
    "\n",
    "Your genetic information is encoded as a sequence of chemical *bases* in your *genome*.\n",
    "These are really tiny and you also can't read a long string of them: errors accumulate and the readout becomes unusable.\n",
    "Luckily, every one of your cells has an identical copy of your genome, so what we can do is shred those copies into tiny segments (about 100 bases long), and then assemble those like an enormous puzzle of 30 million pieces.\n",
    "\n",
    "Before performing assembly, it is vital to perform read correction.\n",
    "During DNA sequencing some bases are incorrectly read out, and must be fixed, or they will mess up the assembly.\n",
    "\n",
    "One correction strategy is to find similar reads in your dataset and fix the error by grabbing the correct information from those reads. Or alternatively, you may choose to completely discard the reads containing errors.\n",
    "\n",
    "However, this is a very inefficient way to do it, because finding similar reads means you would compare each read to every other read.\n",
    "This takes $N^2$ operations, or $9 \\times 10^{14}$ for a 30 million read dataset!\n",
    "\n",
    "There is another way.\n",
    "[Pavel Pevzner et al](http://www.pnas.org/content/98/17/9748.full) realized that reads could be broken down into smaller, overlapping *k-mers*, substrings of length k, which can then be stored in a hash table (a dictionary).\n",
    "\n",
    "The main advantage is that instead of computing on the total number of reads, which can be arbitrarily large, we can compute on the total number of k-mers, which can only be as large as the genome itself, usually 1-2 orders of magnitude smaller than the reads.\n",
    "\n",
    "If we choose a value for k that is large enough to ensure any k-mer appears only once in the genome, the number of times a k-mer appears is exactly the number of reads that originate from that part of the genome. This is called the *coverage* of that region.\n",
    "\n",
    "If a read has an error in it, there is a high probability that the k-mers overlapping the error will be unique or close to unique in the genome.\n",
    "\n",
    "Think of the equivalent in English: if you were to take reads from Shakespeare, and one read was \"to be or knot to be\", the 7-mer \"knot to\" will appear rarely or not at all, whereas \"not to\" will be very frequent.\n",
    "\n",
    "This is the basis for k-mer error correction: split the reads into k-mers, count the occurrence of each k-mer, and use some logic to replace rare k-mers in reads with similar common ones\n",
    "(or, alternatively, discard reads with erroneous k-mers, possible because reads are so abundant that we can afford to toss out erroneous data).\n",
    "\n",
    "By the way, this is also the subject of a good interview question ;-)\n",
    "\n",
    "And you actually already did this exercise when you did `R` word clouds and you discarded words that appear only a few times in the corpus. \n",
    "\n",
    "This is also an example in which streaming is *essential*. As mentioned before, the number of reads can be enormous, so we don't want to store them in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNA sequence data is commonly represented in `FASTA` format.\n",
    "This is a plaintext format, consisting of one or many DNA sequences per file, each with a name and the actual sequence.\n",
    "\n",
    "A sample FASTA file from [here](http://molb7621.github.io/workshop/Miscellaneous/data.html):\n",
    "```python\n",
    ">derice\n",
    "ACTGACTAGCTAGCTAACTG\n",
    ">sanka\n",
    "GCATCGTAGCTAGCTACGAT\n",
    ">junior\n",
    "CATCGATCGTACGTACGTAG\n",
    ">yul\n",
    "ATCGATCGATCGTACGATCG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the required information to convert a stream of lines from a FASTA file to a count of k-mers:\n",
    "\n",
    "- filter lines so that only sequence lines are used\n",
    "- for each sequence line, produce a stream of k-mers\n",
    "- add each k-mer to a dictionary counter\n",
    "\n",
    "Here's how you would do this in pure Python, using nothing but built-ins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sequence(line):\n",
    "    line = line.rstrip()  # remove '\\n' at end of line\n",
    "    return len(line) > 0 and not line.startswith('>')\n",
    "\n",
    "def reads_to_kmers(reads_iter, k=7):\n",
    "     for read in reads_iter:\n",
    "         for start in range(0, len(read) - k):\n",
    "             yield read[start : start + k]  # note yield, so this is a generator\n",
    "\n",
    "def kmer_counter(kmer_iter):\n",
    "    counts = {}\n",
    "    for kmer in kmer_iter:\n",
    "        if kmer not in counts:\n",
    "            counts[kmer] = 0\n",
    "        counts[kmer] += 1\n",
    "    return counts\n",
    "\n",
    "with open('data/sample.fasta') as fin:\n",
    "    reads = filter(is_sequence, fin)\n",
    "    kmers = reads_to_kmers(reads)\n",
    "    counts = kmer_counter(kmers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads are loaded from disk one at a time and piped through the k-mer converter and to the k-mer counter. We can then plot a histogram of the counts, and confirm that there are indeed two well-separated populations of correct and erroneous k-mers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots appear inline, set custom plotting style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('style/elegant.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_histogram(counts, normed=True, xlim=[], ylim=[],\n",
    "                      *args, **kwargs):\n",
    "    hist = np.bincount(counts)\n",
    "    if normed:\n",
    "        hist = hist / np.sum(hist)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(hist.size), hist, *args, **kwargs)\n",
    "    ax.set_xlabel('counts')\n",
    "    ax.set_ylabel('frequency')\n",
    "    ax.set_xlim(*xlim)\n",
    "    ax.set_ylim(*ylim)\n",
    "\n",
    "counts_arr = np.fromiter(counts.values(), dtype=int, count=len(counts))\n",
    "integer_histogram(counts_arr, xlim=(-1, 250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the nice distribution of k-mer frequencies, along with a spike of very unique (small count) of k-mers at the left of the plot. Such low frequency k-mers are likely to be errors.\n",
    "\n",
    "We can leverage Principal Component Analysis (PCA) and Latent Dirichlet Allocation (LDA) to continue this use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up a computation with cython\n",
    "\n",
    "We're going to showcase NLP computations using the `spacy` package, which relies on `cython` to speed up computations. I got most of the code from [this](https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced) post. So get to an anaconda prompt and install the package:\n",
    "```(python)\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/spacy.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "[Here's](https://spacy.io/usage) how to use spacy.\n",
    "\n",
    "Now, suppose we have a large set of rectangles that we store as a list of Python objects, e.g. instances of a Rectangle class. The main job of our module is to iterate over this list in order to count how many rectangles have an area larger than a specific threshold.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/mondrian.jpg\" width=500 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "class Rectangle:\n",
    "    def __init__(self, w, h):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "    def area(self):\n",
    "        return self.w * self.h\n",
    "\n",
    "def check_rectangles_py(rectangles, threshold):\n",
    "    n_out = 0\n",
    "    for rectangle in rectangles:\n",
    "        if rectangle.area() > threshold:\n",
    "            n_out += 1\n",
    "    return n_out\n",
    "\n",
    "def main_rectangles_slow():\n",
    "    n_rectangles = 10000000\n",
    "    rectangles = list(Rectangle(random(), random()) for i in range(n_rectangles))\n",
    "    n_out = check_rectangles_py(rectangles, threshold=0.25)\n",
    "    print(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's run it:\n",
    "main_rectangles_slow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `check_rectangles()` function which loops over a large number of Python objects is the bottleneck.\n",
    "\n",
    "Let's write it in [Cython](https://cython.org/). If you are really excited about this possibility, here are some [labs](https://alan-turing-institute.github.io/rsd-engineeringcourse/ch08performance/040cython.html).\n",
    "\n",
    "We indicate the cell is a Cython cell by using the %%cython magic command. When the cell is run, the cython code will be written in a temporary file, compiled and reimported in the iPython space. The Cython code thus has to be self-contained.\n",
    "\n",
    "But before, if your cython is version < **29.14**, ***uprade*** your cython:\n",
    "```(python)\n",
    "pip install --upgrade --user cython\n",
    "```\n",
    "\n",
    "Then, load the cython extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reload:\n",
    "```(python)\n",
    "%reload_ext Cython\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your cython is version >= **29.14**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cython as c\n",
    "c.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it isn't, then **run the following** on an anaconda prompt:\n",
    "```(python)\n",
    "pip install --upgrade --user cython\n",
    "```\n",
    "If the cell below fails, then **exit anaconda navigator** (or your notebook), restart, then reload the current notebook. Unfortunately, the C-engine needs to be restarted from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from cymem.cymem cimport Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%%cython` magic command is what prompts the notebook to run the cell using the C engine. You need to add it to every cell you want to supercharge with C code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "from cymem.cymem cimport Pool\n",
    "from random import random\n",
    "\n",
    "cdef struct Rectangle:\n",
    "    float w\n",
    "    float h\n",
    "\n",
    "cdef int check_rectangles_cy(Rectangle* rectangles, int n_rectangles, float threshold):\n",
    "    cdef int n_out = 0\n",
    "    # C arrays contain no size information => we need to state it explicitly\n",
    "    for rectangle in rectangles[:n_rectangles]:\n",
    "        if rectangle.w * rectangle.h > threshold:\n",
    "            n_out += 1\n",
    "    return n_out\n",
    "\n",
    "def main_rectangles_fast():\n",
    "    cdef int n_rectangles = 10000000\n",
    "    cdef float threshold = 0.25\n",
    "    cdef Pool mem = Pool()\n",
    "    cdef Rectangle* rectangles = <Rectangle*>mem.alloc(n_rectangles, sizeof(Rectangle))\n",
    "    for i in range(n_rectangles):\n",
    "        rectangles[i].w = random()\n",
    "        rectangles[i].h = random()\n",
    "    n_out = check_rectangles_cy(rectangles, n_rectangles, threshold)\n",
    "    print(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main_rectangles_fast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are about ***20 times faster*** in Cython!\n",
    "\n",
    "The ratio of improvement depends a lot on the specific syntax of the Python program. While the speed in Cython is rather predictible once your code make only use of C level objects (it is usually directly the fastest possible speed), the speed of Python can vary a lot depending on how your program is written and how much overhead the interpreter will add.\n",
    "\n",
    "How can you be sure you Cython program makes only use of C level structures? Use the `-a` or `--annotate` flag in the `%%cython` magic command to display a code analysis with the line accessing and using Python objects highlighted in yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "from cymem.cymem cimport Pool\n",
    "from random import random\n",
    "\n",
    "cdef struct Rectangle:\n",
    "    float w\n",
    "    float h\n",
    "\n",
    "cdef int check_rectangles_cy(Rectangle* rectangles, int n_rectangles, float threshold):\n",
    "    cdef int n_out = 0\n",
    "    # C arrays contain no size information => we need to state it explicitly\n",
    "    for rectangle in rectangles[:n_rectangles]:\n",
    "        if rectangle.w * rectangle.h > threshold:\n",
    "            n_out += 1\n",
    "    return n_out\n",
    "\n",
    "cpdef main_rectangles_fast():\n",
    "    cdef int n_rectangles = 10000000\n",
    "    cdef float threshold = 0.25\n",
    "    cdef Pool mem = Pool()\n",
    "    cdef Rectangle* rectangles = <Rectangle*>mem.alloc(n_rectangles, sizeof(Rectangle))\n",
    "    for i in range(n_rectangles):\n",
    "        rectangles[i].w = random()\n",
    "        rectangles[i].h = random()\n",
    "    n_out = check_rectangles_cy(rectangles, n_rectangles, threshold)\n",
    "    print(n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on a line that starts with a `+` to see the C code that Cython generated for it!\n",
    "\n",
    "Here is an example of the previous cython program ***not optimized*** (with Python objects in the loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "from cymem.cymem cimport Pool\n",
    "from random import random\n",
    "\n",
    "cdef struct Rectangle:\n",
    "    float w\n",
    "    float h\n",
    "\n",
    "cdef int check_rectangles_cy(Rectangle* rectangles, int n_rectangles, float threshold):\n",
    "    # ========== MODIFICATION ===========\n",
    "    # We changed the following line from `cdef int n_out = 0` to\n",
    "    n_out = 0\n",
    "    # n_out is not defined as an `int` anymore and is now thus a regular Python object\n",
    "    # ===================================\n",
    "    for rectangle in rectangles[:n_rectangles]:\n",
    "        if rectangle.w * rectangle.h > threshold:\n",
    "            n_out += 1\n",
    "    return n_out\n",
    "\n",
    "cpdef main_rectangles_not_so_fast():\n",
    "    cdef int n_rectangles = 10000000\n",
    "    cdef float threshold = 0.25\n",
    "    cdef Pool mem = Pool()\n",
    "    cdef Rectangle* rectangles = <Rectangle*>mem.alloc(n_rectangles, sizeof(Rectangle))\n",
    "    for i in range(n_rectangles):\n",
    "        rectangles[i].w = random()\n",
    "        rectangles[i].h = random()\n",
    "    n_out = check_rectangles_cy(rectangles, n_rectangles, threshold)\n",
    "    print(n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how line 16 in the loop of check_rectangles_cy is ***highlighted***? This indicates that the Cython compiler had to add some Python API overhead.\n",
    "\n",
    "The important point here is that lines that run in a loop (e.g. 11 to 13 in the optimized version) should not be highlighted, meaning they will be running at the fastest possible speed. It's ok to have yellow lines in the main_rectangle_fast function as this function will only be called once when we execute our program anyway. The yellow lines 22 and 23 in the optimized version are initialization lines that we could avoid by using a C level random function like `stdlib rand()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `spacy` package\n",
    "\n",
    "The official [Cython documentation](https://cython.readthedocs.io/en/latest/) advises against the use of C strings: \n",
    "```(python)\n",
    "Generally speaking: unless you know what you are doing, avoid using C strings where possible and use Python string objects instead.\n",
    "```\n",
    "`spaCy` allows us us overcome this problem by converting all strings to 64-bit hashes using a look up between Python unicode strings and 64-bit hashes called the **StringStore**, giving us access to fully populated C level structures of the document and vocabulary called **TokenC** and **LexemeC**,\n",
    "\n",
    "The **StringStore** object is accessible from everywhere in `spaCy` and every object, e.g. as `nlp.vocab.strings`, `doc.vocab.strings` or `span.doc.vocab.string`:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/spacy-nlp.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "Here is now a simple example of NLP processing, albeit with lotsa text, in Cython.\n",
    "\n",
    "Let's build a list of big documents from `pytorch` data, and parse them using spaCy (this will take a few minutes).\n",
    "\n",
    "First, just like with `nltk`, we need to download the already-built (english) model. **Open an anaconda prompt and run**:\n",
    "```(python)\n",
    "python -m spacy download en\n",
    "```\n",
    "\n",
    "Then run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import spacy\n",
    "\n",
    "# Build a dataset of 10 parsed document extracted from the Wikitext-2 dataset\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt') as response:\n",
    "   text = response.read()\n",
    "nlp = spacy.load('en')\n",
    "doc_list = list(nlp(text[:800000].decode('utf8')) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(doc) for doc in doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 1.7 million tokens (words) in our dataset!\n",
    "\n",
    "Let's perform an NLP task on this dataset: Let's count the number of times the word `run` is used as a **noun** (not a verb) in the dataset (i.e. tagged with a \"NN\" Part-Of-Speech (POS) tag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_loop(doc_list, word, tag):\n",
    "    n_out = 0\n",
    "    for doc in doc_list:\n",
    "        for tok in doc:\n",
    "            if tok.lower_ == word and tok.tag_ == tag:\n",
    "                n_out += 1\n",
    "    return n_out\n",
    "\n",
    "def main_nlp_slow(doc_list):\n",
    "    n_out = slow_loop(doc_list, 'run', 'NN')\n",
    "    print(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# quite slow!\n",
    "main_nlp_slow(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About one second on my ASUS STRIX Nvidia laptop. Let's try to speed this up with `spaCy` and `Cython`.\n",
    "\n",
    "First, we have to think about the data structure. We will need a C level array for the dataset, with pointers to each document's **TokenC** array. We'll also need to convert the strings we use for testing to 64-bit hashes: \"run\" and \"NN\". When all the data required for our processing is in C level objects, we can then iterate at full C speed over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n",
    "sp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -+\n",
    "import numpy\n",
    "from spacy.tokens.doc cimport Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I have a problem with my `cimport` above, which makes the code below moot. I have not been able to fix this :-( If ***you*** can, let me know, i'll give you bonus hw points :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -+\n",
    "#import numpy # Sometime we have a fail to import numpy compilation error if we don't import numpy\n",
    "from cymem.cymem cimport Pool\n",
    "from spacy.tokens.doc cimport Doc\n",
    "from spacy.typedefs cimport hash_t\n",
    "from spacy.structs cimport TokenC\n",
    "\n",
    "cdef struct DocElement:\n",
    "    TokenC* c\n",
    "    int length\n",
    "\n",
    "cdef int fast_loop(DocElement* docs, int n_docs, hash_t word, hash_t tag):\n",
    "    cdef int n_out = 0\n",
    "    for doc in docs[:n_docs]:\n",
    "        for c in doc.c[:doc.length]:\n",
    "            if c.lex.lower == word and c.tag == tag:\n",
    "                n_out += 1\n",
    "    return n_out\n",
    "\n",
    "cpdef main_nlp_fast(doc_list):\n",
    "    cdef int i, n_out, n_docs = len(doc_list)\n",
    "    cdef Pool mem = Pool()\n",
    "    cdef DocElement* docs = <DocElement*>mem.alloc(n_docs, sizeof(DocElement))\n",
    "    cdef Doc doc\n",
    "    for i, doc in enumerate(doc_list): # Populate our database structure\n",
    "        docs[i].c = doc.c\n",
    "        docs[i].length = (<Doc>doc).length\n",
    "    word_hash = doc.vocab.strings.add('run')\n",
    "    tag_hash = doc.vocab.strings.add('NN')\n",
    "    n_out = fast_loop(docs, n_docs, word_hash, tag_hash)\n",
    "    print(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main_nlp_fast(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some more cool NLP with `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The big grey dog ate all of the chocolate, but fortunately he wasn't sick!\")\n",
    "doc.text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token’s `.orth_` method returns a string representation of the token rather than a `spaCy` token object (`.orth` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to avoid returning tokens that are punctuation or white space, SpaCy provides convienence methods for this (as well as many other common text cleaning tasks — for example, to remove stop words you can call the `.is_stopmethod`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc if not token.is_punct | token.is_space] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's **lemmatization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.lemma_ for word in doc] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "Here's an example of Part-of-Speech (POS) tagging with `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as \n",
    "the 44th President of the United States from 2009 to 2017. He is the first  \n",
    "African American to have served as president, as well as the first born outside the contiguous United States.\"\"\" \n",
    "\n",
    "nlp_obama = nlp(wiki_obama) \n",
    "\n",
    "pos_tags = [(i, i.tag_) for i in nlp_obama]\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"Conor's dog's toy was hidden under the man's sofa in the woman's house\")\n",
    "\n",
    "pos_tags = [(i, i.tag_) for i in doc2]\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By exploiting possessives, we can determine who owns what (providing the text is grammatically sound). SpaCy uses the popular [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) POS tags. With SpaCy you can access coarse and fine-grained POS tags with the .pos_ and .tag_ methods, respectively (here we access the fine grained POS tag).\n",
    "\n",
    "You can see that the `’s` tokens above are labelled as **POS**. This is how you can exploit this tag to extract the **owner** and the **object** they own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners_possessions = [] \n",
    "for i in pos_tags: \n",
    "    if i[1] == \"POS\":\n",
    "        owner = i[0].nbor(-1) \n",
    "        possession = i[0].nbor(1) \n",
    "        owners_possessions.append((owner, possession))\n",
    "owners_possessions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! ***That's*** NLP!\n",
    "\n",
    "Here's an example of **named entity recognition** with `spacy`. \n",
    "\n",
    ">**NAMED ENTITY RECOGNITION**: the process of classifying named entities found in a text into pre-defined categories, such as persons, places, organizations, dates, etc. spaCy uses a statistical model to classify a broad range of entities, including persons, events, works-of-art and nationalities / religions (see [documentation](https://spacy.io/docs/usage/entity-recognition) for the full list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, i.label_, i.label) for i in nlp_obama.ents] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not uncommon in NLP tasks to want to split a document into sentences. It is simple to do this with `spaCy` by accessing a Doc's .sents method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, sent in enumerate(nlp_obama.sents, 1): \n",
    "    print(\"Sentence number {}: {}\".format(ix, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find more interesting project ideas with `spaCy` [here](https://spacy.io/usage/processing-pipelines#_title)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Contents:\n",
    "- Part 1: [Introduction to Natural Language Processing (NLP)](#section1)\n",
    "- Part 2: [Big Data processing: Optimizations often required for NLP](#section2)\n",
    "- Part 3: [NLP with the Reuters dataset](#section3)\n",
    "- Part 4: [Words 2 Vectors](#section4)\n",
    "- Part 5: [NLP-ing Litterature](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: NLP with the Reuters dataset\n",
    "\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/wordcloud.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "Microsoft's [NLTK](https://en.wikipedia.org/wiki/Natural_Language_Toolkit) (Natural Language Toolkit) is one of the better HLP frameworks and has many popular corpora available for download directly from the API. Let's use the Brown and Reuters corpora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the line `nltk.download()` below is ***uncommented*** (i.e. will run). Run the cell, this will produce a dialog prompting you what to download (I hope). Navigate to the `corpora` tab, and download `Brown`, `reuters`, and `stopwords`, then go to the `Models` tab and download `punkt`. Then comment the `nltk.download()` line out again, so if you rerun the cell below you won't have to do the downloads again. \n",
    "\n",
    "Then go to your corpora folder, and unzip the reuters folder if unzipped (in the same folder). For some reason, it did not automatically unzip on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Reuters corpus](https://martin-thoma.com/nlp-reuters/) is a collection of more than 10,000 news documents published in 1987 and categorized into 90 different topics. The corpus contains over 1.3 million words in total.\n",
    "\n",
    "NLTK's feature set is focused more on the linguistic aspect of natural language processing than the machine learning aspect, with functions for tasks such as tokenization, stemming, tagging, and parsing. Even so, there are some very useful functions we can take advantage of to get a sense of what's in this corpus.  Let's start by tabulating the number of unique words in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "len(reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(reuters.words())\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters.paras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.paras()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_l = reuters.paras()[3][0]\n",
    "one_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = ' '.join(reuters.paras()[0][0])\n",
    "one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the first word in the subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ele in enumerate(one_l): \n",
    "    if any(c in \"abcdefghijklmnopqrstuvwxyz\" for c in ele):\n",
    "        print(i, ele)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating title and subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ele in enumerate(one_l): \n",
    "    if any(c in \"abcdefghijklmnopqrstuvwxyz\" for c in ele):\n",
    "        end_of_subject = i\n",
    "        break\n",
    "print(' '.join(one_l[0:end_of_subject]))\n",
    "print(' '.join(one_l[end_of_subject:])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this for every post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = dict()\n",
    "\n",
    "for p in reuters.paras():\n",
    "    # build subject\n",
    "    first_sentence = p[0]\n",
    "    end_of_subject = 0\n",
    "    for i, ele in enumerate(first_sentence): \n",
    "        if any(c in \"abcdefghijklmnopqrstuvwxyz\" for c in ele):\n",
    "            end_of_subject = i\n",
    "            break\n",
    "    subject = ' '.join(first_sentence[0:end_of_subject])\n",
    "    body = ' '.join(first_sentence[end_of_subject:])\n",
    "    # complete with rest of body\n",
    "    for s in p[1:]:\n",
    "        body += ' '.join(s)\n",
    "    # buld dictionary\n",
    "    items[subject] = body\n",
    "    \n",
    "print(list(items.keys())[0])\n",
    "print(list(items.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(items.keys())[1])\n",
    "print(list(items.values())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reuters_df = pd.DataFrame.from_dict(items, orient='index')\n",
    "reuters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(reuters.words())\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that there are 41,600 unique tokens in the corpus.  This doesn't tell us anything about the distribution of these tokens though.  NLTK has a built-in function to compute a frequency distribution for a text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(reuters.words())\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot these cumulatively to get a sense for how much of the corpus they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "ax = fdist.plot(30, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just 30 tokens make up around 35% of the entire corpus!  Moreover, most of these are things like punctuation and articles such as `and`, `to`, `of` and so on.  This is useful to know as we may want to strip out tokens like these.  You might also notice that the word 'the' appears on the list twice.  That's because the corpus contains both upper-case and lower-case words, and they are each counted separately.  Before we attempt to do anything with this data we'll need to correct these issues. The cell below will take a good minute to run, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words()\n",
    "cleansed_words = [w.lower() for w in reuters.words() if w.isalnum() and w.lower() not in stopwords]\n",
    "vocabulary = set(cleansed_words)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting everything to lowercase, removing punctuation, and removing **stop words** using a pre-defined list of words that do not add any semantic value, we've reduced the vocabulary from almost 42,000 to just over 30,000.  \n",
    "\n",
    "Note that we still didn't address things like singular vs. plural being different words. To handle this we'd have to get into topics like stemming, but for now let's leave as-is. Let's look at the top 30 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(cleansed_words)\n",
    "fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list is more interesting now!  There's a lot more that we could do with NLTK, but since we're interested in using this data to build statistical models, we need to find ways to \"vectorize\" this data.  \n",
    "\n",
    "Recall from our first lecture that one common way to represent text data is called \"bag of words\" (BOW) representation.  \n",
    "\n",
    "A bag of words represents each document in a corpus as a series of features that ask a question about the document.  Most commonly, the features are the collection of all distinct words the vocabulary of the entire corpus.  The values are usually either binary (representing the presence or absence of that word in the document) or a count of the number of times that word appears in the document.  A corpus is then represented as a matrix with one row per document and one column per unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off using `scikit-learn`'s `CountVectorizer` class to transform our corpus into a **sparse** bag of words representation. This uses `scipy`'s sparse matrix calculus. `CountVectorizer` expects as input a list of raw strings containing the documents in the corpus.  It takes care of tokenization, transformation to lowercase, filtering stop words, building the vocabulary etc.  It also tabulates occurrance counts per document for each feature.\n",
    "\n",
    "Since `CountVectorizer` expects raw data as input, rather than the pre-processed data we were working with in NLTK, we need to create a list of documents to pass to the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in reuters.fileids() if 'training' in f]\n",
    "corpus = [reuters.raw(fileids=[f]) for f in files]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[3211]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the training corpus defined as a list of raw text documents.  We can pass this to our vectorizer to build our bag of words *matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer stores the data as a sparse matrix since a dense matrix would use way too much space and most of the values would be zero anyway (because each document only contains a small number of the total words in the vocabulary). \n",
    "\n",
    "X is very similar to the matrix we built last lecture, so we're not going to explore it anymore. You see, data science all comes down to translating nodes/words/etc. into *numbers* (vectors, to be exact), so we can use **math** on the data. It's not about the software packages you use, it's *about the math algorithms*. \n",
    "\n",
    "We can transform X to a numpy array if necessary though, with:\n",
    "```python\n",
    "X.toarray()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer stores the feature names (words) that map to the matrix column indexes.  We can inspect those if desired.  Note that we're skipping to index 2000 with a python slice because if you look at the beginning of the index, it's all numbers.  The reuters corpus, being news articles, contains quite a high volume of numeric symbols.  It's debatable whether or not we should really include these in the vocabulary, but for now they're there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[4000:4100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential issue with this representation is that it holds an in-memory mapping of the vocabulary-to-document-matrix that can get unwieldy on large datasets.  This approach also doesn't work when training in an on-line fashion since it needs to build the entire vocabulary *ahead* of time.  \n",
    "\n",
    "There's another vectorization algorithm implemented in scikit-learn that uses *feature hashing* to build the matrix in a *stateless* manner.  This `HashingVectorizer` class solves both of the above problems, however it comes with some tradeoffs - it's not possible to \"inverse transform\" the vector back to the original words, and there's a possibility of *collisions* that could cause some information to be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer()\n",
    "X_hash = hv.transform(corpus)\n",
    "X_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's continue using our `CountVectorizer`. Keep in mind that for very large corpora `HashingVectorizer` would be faster and more efficient.\n",
    "\n",
    "We now have a bag of words matrix, however there's another problem - some words appear much more frequently across the corpora as a whole than other words, so their presence in a document should carry less weight than a word that is very infrequent in general.  To adjust for this, we'll use **TF-IDF weighting**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted = tfidf.fit_transform(X)\n",
    "X_weighted.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted term-document matrix, let's do something with it.  A common NLP task is to classify documents as belonging to a particular category.  Since the `reuters` corpus is labeled (categorized by humans), we can used a supervised learning algorithm to attempt to learn how to categorize similar news articles.\n",
    "\n",
    "To do this we need a few additional pieces of information.  We need a set of labels, and we need a test set to evaluate performance of the model.  Fortunately we have both available to us for the `reuters` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the term-document matrix for the test set using the existing transforms\n",
    "test_files = [f for f in reuters.fileids() if 'test' in f]\n",
    "test_corpus = [reuters.raw(fileids=[f]) for f in test_files]\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "X_test_weighted = tfidf.transform(X_test)\n",
    "\n",
    "# get the categories for each document in both the train and test sets\n",
    "train_labels = [reuters.categories(fileids=[f]) for f in files]\n",
    "test_labels = [reuters.categories(fileids=[f]) for f in test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 90 distinct categories (document `cats.txt` in `reuters`), and each document can be assigned to more than one category, we probably don't have enough documents per category to build a really good document classifier.  We're going to simplify the problem a bit and reduce the classification to a binary problem - wether or not the document belongs to the 'gold' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray([1 if 'gold' in label else 0 for label in train_labels])\n",
    "y_test = np.asarray([1 if 'gold' in label else 0 for label in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted.shape, y.shape, X_test_weighted.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "Now we're ready to train a classifier.  We'll stick with (multinomial) Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_weighted, y)\n",
    "\n",
    "# predict labels for the test set\n",
    "predictions = classifier.predict(X_test_weighted)\n",
    "\n",
    "# output the classification report\n",
    "label_names = ['not gold', 'gold']\n",
    "print(classification_report(y_test, predictions, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So precision and f-score are ill-defined, which means we don't really have enough data to train. There are too few articles about `gold`. Let's try a different category. How about `earn`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray([1 if 'earn' in label else 0 for label in train_labels])\n",
    "y_test = np.asarray([1 if 'earn' in label else 0 for label in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted.shape, y.shape, X_test_weighted.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_weighted, y)\n",
    "\n",
    "# predict labels for the test set\n",
    "predictions = classifier.predict(X_test_weighted)\n",
    "\n",
    "# output the classification report\n",
    "label_names = ['not earn', 'earn']\n",
    "print(classification_report(y_test, predictions, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so that's better, although the recall is not as high as we would like it to be.  There are a number of ways we could work to improve this result, such as experimenting with removing extraenous tokens such as numbers from our vocabulary or constructing additional high-level features about the documents.  For a simple bag-of-words model though it's not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "Supervised learning is nice when we have a *labeled* dataset, but the vast majority of text in the wild does not come with any sort of label so its usefulness in natural language processing is often limited.  \n",
    "\n",
    "What about *unsupervised techniques* to categorize documents? \n",
    "\n",
    "In a Machine Learning class, you will learn the difference between **recommendation**, **classification**, and **clustering**, the main categories of Machine Learning.\n",
    "\n",
    "Scikit-learn packages a decomposition technique called `non-negative matrix factorization` (NMF) that we can use for topic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=10).fit(X_weighted)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([feature_names[i] for i in topic.argsort()[:-20 - 1:-1]]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output takes the components derived from the factorization (here assumed to model a **topic** from the corpus) and extracts the 20 words that most significantly contributed to that topic.  Although it's not perfect, we can see some commonalites among the groups of words.\n",
    "\n",
    "NMF gives some interesting results, but there are more advanced algorithms for topic modeling. [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) is a technique that models documents as though they are composed of some undefined number of topics. Each of the words in the document are then said to be attributed to some combination of those topics. LDA is the equivalent of k-means for NLP. \n",
    "\n",
    "`Scikit-learn` does [implement](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) LDA, but there's a library called [gensim](https://en.wikipedia.org/wiki/Gensim), by [Radim Rehurek](https://radimrehurek.com/gensim/install.html), that I researched 2 years ago and really liked. So we'll study *that* in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Contents:\n",
    "- Part 1: [Introduction to Natural Language Processing (NLP)](#section1)\n",
    "- Part 2: [Big Data processing: Optimizations often required for NLP](#section2)\n",
    "- Part 3: [NLP with the Reuters dataset](#section3)\n",
    "- Part 4: [Words 2 Vectors](#section4)\n",
    "- Part 5: [NLP-ing Litterature](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Words 2 Vectors\n",
    "\n",
    ">**WORD EMBEDDINGS**: A word embedding $W:words→R^nW$ is a parametrized function mapping words in some language to high-dimensional **vectors** (e.g 500 dimensions).\n",
    "For example, we might find: $W(\\text{cat})=(0.2, -0.4, 0.7, ...)$ and $W(\\text{mat})=(0.0, 0.6, -0.1, ...)$\n",
    "Typically, the function is a lookup table, parameterized by a matrix $\\Theta$ with a row-vector for each word: $W_θ(w_n)=θ_n$. The matrix $W$ is initialized to have random vectors for each word. It then learns to have **meaningful** vectors in order to perform some task.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/gensim.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "The [gensim](https://en.wikipedia.org/wiki/Gensim) python package, by [Radim Rehurek](https://radimrehurek.com/gensim/install.html) is focused on [**topic modeling**](https://en.wikipedia.org/wiki/Topic_model). Google has a similar library, called [word2vec](https://en.wikipedia.org/wiki/Word2vec), and Stanford another similar one called [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning)), but I think the one-man library gensim is *better*!\n",
    "\n",
    "First, open your Anaconda prompt and run:\n",
    "```python\n",
    "pip install gensim\n",
    "```\n",
    "\n",
    "or:\n",
    "\n",
    "```python\n",
    "conda install -c conda-forge gensim\n",
    "```\n",
    "If that doesn't work (it should), run:\n",
    "```python\n",
    "pip install --upgrade gensim\n",
    "```\n",
    "\n",
    "To start off we need our corpus in a format that gensim models can use as input. Gensim implements a lot of the same transforms that we just applied to the data, but rather than re-create the same transforms, we can re-use what we've already done and convert our term-document matrix into gensim's expected format, by levevraging our TF-IDF X_weighted matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# create the corpus using a conversion utility\n",
    "gensim_corpus = matutils.Sparse2Corpus(X_weighted)\n",
    "\n",
    "# build the LDA model\n",
    "lda = models.LdaModel(gensim_corpus, num_topics=100)\n",
    "lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our LDA model we can now examine the words that most contribute to each topic (as we did with NMF), or we can compare *new* documents to the model to identify either the topics that make up that document or the existing documents that they are most similar to. E.g. spam? \n",
    "\n",
    "But let's move on from NMF/LDA to another algorithm implemented in gensim called **word2vec**.  Word2vec is an **nsupervised neural network** model that runs on a corpus of text and learns vector representations for the individual words in the text. The word vectors are modeled in a way such that words that are **semantically close** to each other are also close in the vector space, so neighbors in the vector space equates to similar semantics. \n",
    "\n",
    "***Amazing*** how vector space context maps to semantic similarity! That is what fascinated me about word2vec (which Google actually uncovered first with their version of `word2vec`).\n",
    "\n",
    "Let's explore some of the implications on the reuters dataset. Since word2vec expects a list of sentences as input, we'll need to go back to the pre-transformed sentence list provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Word2Vec(reuters.sents(), size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained word2vec model.  It's possible to look at the vector for a word directly, although it won't mean much to you:\n",
    "```python\n",
    "model['market']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every word in the vocabulary has a vector representation!\n",
    "\n",
    "Since we're dealing with vectors, it's possible to compare words using vector math such as `cosine similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('dollar', 'yen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('dollar', 'potato')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the model, `dollar` and `yen` are much more similar to each other (both being currencies) than `dollar` and `potato`.  \n",
    "\n",
    "Right?\n",
    "\n",
    "The relationship is ***deeper*** than just a similarity measure though! The word2vec model is capable of capturing abstract concepts as well.  The ubiquitous example is `woman + king - man = queen`.  When properly trained on a large enough amount of text, the model is able to detect that the relationship between `woman` and `queen` is similar to the relationship between `man` and `king`!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/wow.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "Let's see if the model we just trained can do something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Japan', 'dollar'], negative=['US'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although (deutsche) `mark`, (pound) `sterling`, and (japanese) `yen` are on the list, there's some noise too, like `back`, `fall`, `further`, and `levels`.  This is mostly likely due to the relatively small size of the dataset.  Word2vec needs a **huge** amount of training data to work really well, and that's a problem inherent to ANNs. \n",
    "\n",
    "Some parameter tuning might help too - for example, a size of 100 dimensions might be way too big for the amount of data in the reuters dataset.\n",
    "\n",
    "Let's see if there's a way to visualize some of the information captured by the model. Since the vectors are high-dimensional we can't visualize them directly, but we can apply a dimension reduction technique like PCA and use the first two principal components as coordinates.  We can try this with a group of words that should be somewhat similar, such as countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# we use 'Soviet' instead of 'Russia' because reuters is an old dataset\n",
    "words = ['US', 'China', 'Japan', 'England', 'France', 'Germany', 'Soviet']\n",
    "word_vectors = [model[word] for word in words]\n",
    "\n",
    "# create and apply PCA transform\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(word_vectors)\n",
    "\n",
    "# slice the 2D array\n",
    "x = principal_components[:, 0]\n",
    "y = principal_components[:, 1]\n",
    "\n",
    "# plot with text annotation\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "ax.scatter(x, y, s=0)\n",
    "\n",
    "for i, label in enumerate(words):\n",
    "    ax.annotate(label, (x[i], y[i]), size='x-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a spread between some words, but a few such as `US` and `China` are very close. \n",
    "\n",
    "But semantic positioning does not make as much sense in the absolute, We need to include different semantic categories. So let's introduce 4 semantic categories: **Countries**, **currencies**, **finances**, and **oil**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# we use 'Soviet' instead of 'Russia' because reuters is an old dataset\n",
    "words = ['US', 'China', 'Japan', 'England', 'France', 'Germany', 'dollar', 'yen', 'mark', 'sterling', 'stock', 'company', 'dividend', 'shares', 'oil', 'pumping']\n",
    "word_vectors = [model[word] for word in words]\n",
    "\n",
    "# create and apply PCA transform\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(word_vectors)\n",
    "\n",
    "# slice the 2D array\n",
    "x = principal_components[:, 0]\n",
    "y = principal_components[:, 1]\n",
    "\n",
    "# plot with text annotation\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "ax.scatter(x, y, s=0)\n",
    "\n",
    "for i, label in enumerate(words):\n",
    "    ax.annotate(label, (x[i], y[i]), size='x-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that **Countries** (and strangely, **oil**) are all well grouped together! **Financial** terms carve out their own region of space, and **currencies** another.\n",
    "\n",
    "Do not forget that we only plotted ***2*** PCA vectors, so we could visualize in 2D.\n",
    "\n",
    "Terms with more correct, closer semantic positioning may have appeared frequently in the corpus, so there may have been a larger amount of training data for them, too.  \n",
    "\n",
    "The results become more interesting when applied to very large datasets, and indeed Google and others have done just that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 2D projection of a larger dataset from my bigdata lecture series, with more terms plotted:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/word2vec.png\" width=950 />\n",
    "</center>\n",
    "\n",
    "and this is zooming in on a region:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/word2vec2.png\" width=950 />\n",
    "</center>\n",
    "\n",
    "Isn't it amazing how a machine-learning algotihm figured out that `dancing` and `singing` makes me happy? And how did it figure this out? Simple, by listening to my blog posts. I often cite words related to `happy` when I talk about `dancing` and `singing`, because.. dancing and singing ***make me happy***! So all a computer needs to do is to read what I say and classify!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Word embeddings (word2vec) are used in statistical language translation, because words with similar meaning but in different languages map to the same region in vector space.\n",
    "\n",
    "So applying this methodology to words is only the beginning.  It's already been extended to phrases and even entire documents.  It's a very promising research area and why I got so excited researching this 2 years ago.\n",
    "\n",
    "So now we need to learn about [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA).\n",
    "\n",
    "LDA is a great way for obtaining synoptical document semantics. It's a kind of unsupervised classification. It was first proposed by J. K. Pritchard, M. Stephens and P. Donnelly in 2000, in the context of machine learning.\n",
    "\n",
    ">**DEFINITION**: LDA is a **generative statistical model** that allows sets of observations to be explained by **unobserved groups** (**latent**) that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics.\n",
    "\n",
    "For example, assuming there are 4 reasons Hillary lost the elections, what are those reasons based on all emails gathered from national correspondents?\n",
    "\n",
    "Here's a (good) classical layman's description of the LDA algorithm. It should immediately remind you of **k-means**!\n",
    ">**ALGORITHM (LDA)**: Suppose you’ve just moved to a new city. You’re a hipster and an anime fan, so you want to know where the other hipsters and anime geeks tend to hang out. Of course, as a hipster, you know you can’t just ask, so what do you do?\n",
    "\n",
    ">Here’s the scenario: you scope out a bunch of different establishments (documents) across town, making note of the people (words) hanging out in each of them (e.g., Alice hangs out at the mall and at the park, Bob hangs out at the movie theater and the park, and so on). Crucially, you don’t know the typical interest groups (topics) of each establishment, nor do you know the different interests of each person.\n",
    "\n",
    ">So you pick some number K of categories to learn (i.e., you want to learn the K most important kinds of categories people fall into), and start by making a guess as to why you see people where you do. For example, you initially guess that Alice is at the mall because people with interests in X like to hang out there; when you see her at the park, you guess it’s because her friends with interests in Y like to hang out there; when you see Bob at the movie theater, you randomly guess it’s because the Z people in this city really like to watch movies; and so on.\n",
    "\n",
    ">Of course, your random guesses are very likely to be incorrect (they’re random guesses, after all!), so you want to improve on them. One way of doing so is to:\n",
    "\n",
    ">Pick a place and a person (e.g., Alice at the mall).\n",
    "Why is Alice likely to be at the mall? Probably because other people at the mall with the same interests sent her a message telling her to come.\n",
    "In other words, the more people with interests in X there are at the mall and the stronger Alice is associated with interest X (at all the other places she goes to), the more likely it is that Alice is at the mall because of interest X.\n",
    "So make a new guess as to why Alice is at the mall, choosing an interest with some probability according to how likely you think it is.\n",
    "Go through each place and person over and over again. Your guesses keep getting better and better (after all, if you notice that lots of geeks hang out at the bookstore, and you suspect that Alice is pretty geeky herself, then it’s a good bet that Alice is at the bookstore because her geek friends told her to go there; and now that you have a better idea of why Alice is probably at the bookstore, you can use this knowledge in turn to improve your guesses as to why everyone else is where they are), and eventually you can stop updating. Then take a snapshot (or multiple snapshots) of your guesses, and use it to get all the information you want:\n",
    "\n",
    "Note that there is another technique called [non-negative matrix factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) (NMF) that strongly resembles Latent Dirichlet Allocation (LDA), where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Whereas LDA is a **probabilistic model** capable of expressing uncertainty about the placement of topics across texts and the assignment of words to topics, NMF is a **deterministic algorithm** which arrives at a single representation of the corpus (and for this reason, NMF is often characterized as a machine learning algorithm). Like LDA, NMF arrives at its representation of a corpus in terms of something resembling **latent topics**.\n",
    "\n",
    "Now, let's go back to our reuters dataset and do **LDA** on it!\n",
    "\n",
    ">**WARNING**: The `nlp` of the reuters dataset took 3 minutes on my laptop. Caveat Emptor (you may want to do this at home if your laptop is not data science-strong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = reuters_df[0]\n",
    "%time reuters_docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of spaCy documents, we transform them to lists of tokens. Instead of the original tokens, we're going to work with lemmas instead. This will allow our model to generalize better, as it will be able to see that \"traded\" and \"trade\" are actually just two forms of the same words. This is the full list of our initial preprocessing steps:\n",
    "\n",
    "- We remove all words shorter than 3 characters (these are often fairly uninteresting from a topical point of view),\n",
    "- We drop all stopwords, and\n",
    "- We take them lemmas of the remaining words and lowercase them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[t.lemma_.lower() for t in doc if len(t.orth_) > 3 and not t.is_stop] for doc in reuters_docs]\n",
    "print(docs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also want to take frequent bigrams into account. After all, if you;re from New England, \"Tom Brady\" is not the same thing as \"Tom\", nor \"Brady\". Let's use `Gensim` for this!\n",
    "\n",
    "First we identify the frequent bigrams in the corpus, then we append them to the list of tokens for the documents in which they appear. This means the bigrams will not be in their correct position in the text, but that's fine: topic models are bag-of-word models that ignore word position anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:  # bigrams can be recognized by the \"_\" that joins the invidual words\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move on to the final Gensim-specific preprocessing steps. \n",
    "\n",
    "First, we create a dictionary representation of the documents. This dictionary will map each word to a unique ID and help us create **bag-of-word** representations of each document. These bag-of-word representations contain the ids of the words in the document, together with their frequency. Additionally, we can remove the least and most frequent words from the vocabulary. This improves the quality of our topic model and speeds up its training. The minimum frequency of a word is expressed as an absolute number, the maximum frequency is the proportion of documents a word is allowed to occur in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in original documents:', len(dictionary))\n",
    "\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.25)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "print(\"Example representation of document 1:\", dictionary.doc2bow(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example representation of document 2:\", dictionary.doc2bow(docs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create bag-of-word representations for each document in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now it's time to train our topic model. We do this with the following parameters:\n",
    "\n",
    "- corpus: the bag-of-word representations of our documents\n",
    "- id2token: the mapping from indices to words\n",
    "- num_topics: the number of topics we want the model to identify\n",
    "- chunksize: the number of documents the model sees for every update\n",
    "- passes: the number of times we show the total corpus to the model during training\n",
    "- random_state: we use a seed to ensure reproducibility.\n",
    "- On a corpus of this size, the training will typically take one or two minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, chunksize=1000, passes=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We see that each reuters document is a combination of topics, and that the main topics in the corpus are themselves a (latent) mix of keywords.\n",
    "\n",
    "Let's take a look at what the model has learnt, by printing out the ten words that are most characteristic for each of the topics. \n",
    "\n",
    "This shows some interesting patterns already: while some topics are more general (such as 7), others point to some very relevant recurring themes: energy production (topic 1), world economy (topic 3), financials (topic 4), and banking (topic 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (topic, words) in model.print_topics():\n",
    "    print(topic+1, \":\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of inspecting the topics is by visualizing them. \n",
    "\n",
    "This can be done with the `pyLDAvis` library. `PyLDAvis` will show us how popular the topics are in our corpus, how similar the topics are, and which are the most salient words for this topic. \n",
    "```(python)\n",
    "pip install PyLDAvis\n",
    "```\n",
    "\n",
    "Note it's important to set `sort_topics=False` on the call to `pyLDAvis`. If you don't, it will order the topics differently than Gensim.\n",
    "\n",
    ">**WARNING**: Try this at home. It will take a loooooooooooooooooong time.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover over every one of gensim's 10 LDA latent decompositions to see what proportion of words it's constituted by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect the topics the model recognizes in some of the individual documents. Here we see how LDA tends to assign a high probability to a low number of topics for each documents, which makes its results very interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (text, doc) in zip(texts[:10], docs[:10]):\n",
    "    print(text)\n",
    "    print([(topic+1, prob) for (topic, prob) in model[dictionary.doc2bow(doc)] if prob > 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Many collections of unstructured texts don't come with any labels. Topic models such as **Latent Dirichlet Allocation** (LDA) are a useful technique to discover the most prominent topics in such documents. \n",
    "\n",
    "`Gensim` makes training these topics model ***easy***, and `pyLDAvis` presents the results in a visually attractive way. \n",
    "\n",
    "Together, `gensim` and `pyLDAvis` form a powerful toolkit to better understand what's ***inside*** large sets of documents, and to explore subsets of related texts. \n",
    "\n",
    "While these results are often very revealing already, it's also possible to **use them as a starting point, for example for a labelling exercise for *supervised* text classification**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Contents:\n",
    "- Part 1: [Introduction to Natural Language Processing (NLP)](#section1)\n",
    "- Part 2: [Big Data processing: Optimizations often required for NLP](#section2)\n",
    "- Part 3: [NLP with the Reuters dataset](#section3)\n",
    "- Part 4: [Words 2 Vectors](#section4)\n",
    "- Part 5: [NLP-ing Litterature](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: NLP-ing Litterature\n",
    "\n",
    "Treating texts as a list of word frequencies (i.e. a vector) allows us to use all the mathematical tools we developed in class: From linear algebra to GPs. I wanted to also spend some time on less structured texts, because some of you may choose, like I did with your midterm, to analyze litterature.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/sense-sensibility-2.jpg\" width=600 />\n",
    "</center>\n",
    "\n",
    "So let's use texts by [Jane Austen](https://en.wikipedia.org/wiki/Jane_Austen) and [Charlotte Brontë](https://en.wikipedia.org/wiki/Charlotte_Bront%C3%AB), downloaded from [project Gutenberg](https://www.gutenberg.org/). Jane and Charlotte wrote very seminal english romances that were made into movies, such as [Sense and sensibility](https://en.wikipedia.org/wiki/Sense_and_Sensibility).\n",
    "\n",
    "## Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # a conventional alias\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['data/austen-brontë/Austen_Emma.txt',           \n",
    "             'data/austen-brontë/Austen_Pride.txt',          \n",
    "             'data/austen-brontë/Austen_Sense.txt',          \n",
    "             'data/austen-brontë/CBronte_Jane.txt',          \n",
    "             'data/austen-brontë/CBronte_Professor.txt',     \n",
    "             'data/austen-brontë/CBronte_Villette.txt']      \n",
    "                                                             \n",
    "vectorizer = CountVectorizer(input='filename')               \n",
    "dtm = vectorizer.fit_transform(filenames)  # a sparse matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = vectorizer.get_feature_names()\n",
    "vocab_list[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a **document-term matrix** and a **vocabulary list**. \n",
    "\n",
    "Before we query the matrix and find out how many times the word\n",
    "'love' occurs in `Austen_Emma` (the first book in `filenames`), we need to\n",
    "convert this matrix from a [sparse\n",
    "matrix](http://docs.scipy.org/doc/scipy/reference/sparse.html) into a\n",
    "normal NumPy array. \n",
    "\n",
    "We will also convert `vocab`, a list of vocabulary, to an array of strings, as an array supports a greater variety of\n",
    "operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vocab_list)\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first file, indexed by 0 in Python, is **Austen_Emma**\n",
    "```(python)\n",
    "filenames[0] == 'data/austen-brontë/Austen_Emma.txt'\n",
    "````\n",
    "How many times the word `love` occurs in the first file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the standard Python list method index(...)                    \n",
    "love_idx = vocab_list.index('love')                               \n",
    "dtm[0, love_idx]                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, use NumPy indexing                                 \n",
    "dtm[0, vocab == 'love']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that this is the right result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[love_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, _ = dtm.shape\n",
    "dist = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        x, y = dtm[i, :], dtm[j, :]\n",
    "        dist[i, j] = np.sqrt(np.sum((x - y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "dist = euclidean_distances(dtm)\n",
    "np.round(dist, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *Pride and Prejudice* is index 1 and *Jane Eyre* is index 3\n",
    "filenames[1] == 'data/austen-brontë/Austen_Pride.txt'\n",
    "filenames[3] == 'data/austen-brontë/CBronte_Jane.txt'\n",
    "\n",
    "# the distance between *Pride and Prejudice* and *Jane Eyre*\n",
    "dist[1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is greater than the distance between *Jane Eyre* and *Villette* (index 5)\n",
    "dist[1, 3] > dist[3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to use a measure of distance that takes into consideration the length of the novels (an excellent idea), we can calculate the cosine similarity by importing sklearn.metrics.pairwise.cosine_similarity and use it in place of euclidean_distances.\n",
    "\n",
    "Keep in mind that cosine similarity is a measure of similarity (rather than distance) that ranges between 0 and 1 (as it is the cosine of the angle between the two vectors). In order to get a measure of distance (or dissimilarity), we need to “flip” the measure so that a larger angle receives a larger value. The distance measure derived from cosine similarity is therefore one minus the cosine similarity between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(dtm)\n",
    "np.round(dist, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the distance between *Pride and Prejudice* (index 1) and *Jane Eyre* (index 3) is\n",
    "dist[1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is greater than the distance between *Jane Eyre* and *Villette* (index 5)\n",
    "dist[1, 3] > dist[3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often desirable to visualize the pairwise distances between our texts. A general approach to visualizing distances is to assign a point in a plane to each text, making sure that the distance between points is proportional to the pairwise distances we calculated. This kind of visualization is common enough that it has a name, “multidimensional scaling” (MDS) and family of functions in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # for os.path.basename\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "# short versions of filenames:\n",
    "# convert 'data/austen-brontë/Austen_Emma.txt' to 'Austen_Emma'\n",
    "names = [os.path.basename(fn).replace('.txt', '') for fn in filenames]\n",
    "\n",
    "# color-blind-friendly palette\n",
    "for x, y, name in zip(xs, ys, names):\n",
    "    color = 'orange' if \"Austen\" in name else 'skyblue'\n",
    "    plt.scatter(x, y, c=color)\n",
    "    plt.text(x, y, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=3, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pos[:, 0], pos[:, 1], pos[:, 2])\n",
    "\n",
    "for x, y, z, s in zip(pos[:, 0], pos[:, 1], pos[:, 2], names):\n",
    "    ax.text(x, y, z, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering texts based on distance\n",
    "\n",
    "Clustering texts into discrete groups of similar texts is often a useful exploratory step. For example, a researcher may be wondering if certain textual features partition a collection of texts by author or by genre. Pairwise distances alone do not produce any kind of classification. To put a set of distance measurements to work in classification requires additional assumptions, such as a definition of a group or cluster.\n",
    "\n",
    "The ideas underlying the transition from distances to clusters are, for the most part, common sense. Any clustering of texts should result in texts that are closer to each other (in the distance matrix) residing in the same cluster. There are many ways of satisfying this requirement; there no unique clustering based on distances that is the “best”. One strategy for clustering in circulation is called Ward’s method. Rather than producing a single clustering, Ward’s method produces a hierarchy of clusterings, as we will see in a moment. All that Ward’s method requires is a set of pairwise distance measurements–such as those we calculated a moment ago. Ward’s method produces a hierarchical clustering of texts via the following procedure:\n",
    "\n",
    "Start with each text in its own cluster\n",
    "Until only a single cluster remains,\n",
    "Find the closest clusters and merge them. The distance between two clusters is the change in the sum of squared distances when they are merged.\n",
    "Return a tree containing a record of cluster-merges.\n",
    "The function scipy.cluster.hierarchy.ward performs this algorithm and returns a tree of cluster-merges. The hierarchy of clusters can be visualized using scipy.cluster.hierarchy.dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "# match dendrogram to that returned by R's hclust()\n",
    "dendrogram(linkage_matrix, orientation=\"right\", labels=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, stemming, and chunking\n",
    "\n",
    "Languages that do not mark word boundaries present a challenge. Chinese and Classical Greek provide two important examples.\n",
    "\n",
    "Consider the following sequence of Chinese characters: 爱国人. This sequence could be broken up into the following tokens: [“爱”， 国人”] (to love one’s compatriots) or [“爱国”, “人”] (a country-loving person). Resolving this kind of ambiguity (when it can be resolved) is challenging. For Chinese and for other languages with this feature there are a number of tokenization strategies in circulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that CountVectorizer discards \"words\" that contain only one character, such as \"s\"\n",
    "# CountVectorizer also transforms all words into lowercase\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CountVectorizer().build_tokenizer()(\"A chain is only as strong as its weakest link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer().build_tokenizer()(\"授人以鱼不如授人以渔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk word_tokenize uses the TreebankWordTokenizer and needs to be given\n",
    "# a single sentence at a time.\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"A chain is only as strong as its weakest link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "# note that the stem function works one word at a time\n",
    "words = [\"word\", \"words\"]\n",
    "[stemmer.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to split a text is to read through it and create a chunk every n words, where n is a number such as 500, 1,000 or 10,000. The  function below  accomplishes this. \n",
    "\n",
    "To divide up the books, we simply apply this function to each text in the corpus. We do need to be careful to record the original file name and chunk number as we will need them later. One way to keep track of these details is to collect them in a list of Python dictionaries. There will be one dictionary for each chunk, containing the original filename, a number for the chunk, and the text of the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Python standard library package 'glob'\n",
    "import glob\n",
    "romance_filenames = glob.glob('data/austen-brontë/' + '*.txt')\n",
    "romance_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(filename, n_words):\n",
    "    \"\"\"Split a text into chunks approximately `n_words` words in length.\"\"\"\n",
    "    input = open(filename, 'r')\n",
    "    words = input.read().split(' ')\n",
    "    input.close()\n",
    "    chunks = []\n",
    "    current_chunk_words = []\n",
    "    current_chunk_word_count = 0\n",
    "    for word in words:\n",
    "        current_chunk_words.append(word)\n",
    "        current_chunk_word_count += 1\n",
    "        if current_chunk_word_count == n_words:\n",
    "            chunks.append(' '.join(current_chunk_words))\n",
    "            current_chunk_words = []\n",
    "            current_chunk_word_count = 0\n",
    "    chunks.append(' '.join(current_chunk_words) )\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = 1000\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for filename in romance_filenames:\n",
    "    chunk_counter = 0\n",
    "    texts = split_text(filename, chunk_length)\n",
    "    for text in texts:\n",
    "        chunk = {'text': text, 'number': chunk_counter, 'filename': filename}\n",
    "        chunks.append(chunk)\n",
    "        chunk_counter += 1\n",
    "\n",
    "# we started with this many files ...\n",
    "len(romance_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and now we have this many\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These chunks may be saved in a directory for reference or for analysis in another program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the directory exists\n",
    "output_dir = 'data/austen-brontë/chunks'\n",
    "\n",
    "for chunk in chunks:\n",
    "    basename = os.path.basename(chunk['filename'])\n",
    "    fn = os.path.join(output_dir, \"{}{:04d}\".format(basename, chunk['number']))\n",
    "    with open(fn, 'w') as f:\n",
    "        f.write(chunk['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to split a document into paragraph-length chunks. Finding the appropriate character (sequence) that marks a paragraph boundary requires familiarity with how paragraphs are encoded in the text file. For example, the version of Jane Eyre provided in the austen-brontë corpus, contains no line breaks within paragraphs inside chapters, so the paragraph marker in this case is simply the newline. Using the split string method with the newline as the argument (split('\\n')) will break the text into paragraphs. That is, if the text of Jane Eyre is contained in the variable text then the following sequence will split the document into paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"There was no possibility of taking a walk that day. We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no company, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so penetrating, that further out-door exercise was now out of the question.\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the coming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the chidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to Eliza, John, and Georgiana Reed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = text.split('\\n')\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, in the Project Gutenberg edition of Brontë’s novel, paragraphs are set off by two newlines in sequence. We still use the split method but we will use two newlines \\n\\n as our delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"There was no possibility of taking a walk that day.  We had been\\nwandering, indeed, in the leafless shrubbery an hour in the morning; but\\nsince dinner (Mrs. Reed, when there was no company, dined early) the cold\\nwinter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly\\nafternoons: dreadful to me was the coming home in the raw twilight, with\\nnipped fingers and toes, and a heart saddened by the chidings of Bessie,\\nthe nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = text.split('\\n\\n')\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "One of the benefits of chunking is the ability to easily carry out sentiment analysis on a corpus: You analyze the words in each chunk, counting positive and negative sentiment words, using something like the [NRC corpus](https://nrc.canada.ca/en/research-development/products-services/technical-advisory-services/sentiment-emotion-lexicons) to help identify positive and negative words. The figure below is a sentiment analysis of Shakespeare's [Romeo and Juliet](https://peerchristensen.netlify.com/post/fair-is-foul-and-foul-is-fair-a-tidytext-entiment-analysis-of-shakespeare-s-tragedies/) in **R**, which shows quite nicely how the story begins full of love and hope, and ends in tragedy.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/romeo-juliet.png\" width=600 />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing trends\n",
    "\n",
    "Texts often have a **sequence**. Newspapers and periodicals have volumes. Novels have chapters. Personal diaries have dated entries. Visualizations of topic models may benefit from incorporating information about ***where*** a text falls in a sequence, just like a temporal (rise and fall of) sentiment analysis of Romeo and Juliet conveys important information about the kind of novel it is (tragedy).\n",
    "\n",
    "There are many NLP packages that help identify such sequences, and one I like for its simplicity is a local product of Massachusetts called **Mallet**. Even though **2.0.8** is the latest version as of this writing, I have only used the **2.0.7** version, so mae sure you download ***that*** version with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pwd\n",
    "curl http://mallet.cs.umass.edu/dist/mallet-2.0.7.zip > mallet.zip # download the zip archive\n",
    "unzip mallet.zip #unzip the archive into the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!c:/Users/Dino/mallet-2.0.7/bin/mallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mallet\n",
    "\n",
    "[MALLET](http://mallet.cs.umass.edu/download.php) is a Java LDA application written by [David Mimno](https://twitter.com/dmimno) at the University of Massachusetts. So you have to have the **Java JDK** installed, but otherwise there are no technical dependencies.\n",
    "\n",
    "\n",
    "## Installing Mallet\n",
    "\n",
    "The cell above automatically downloads and unzip version 2.0.7 of MALLET into your `C:\\Users\\<your-username>` folder. You can also download MALLET separately and unzip it yourself. The rest of this notebook expects the MALLET application to be in your `C:/Users/<your-username>/mallet-2.0.7` folder.\n",
    "\n",
    "**You need to create** a MALLET_HOME environment value with the value `C:/Users/<your-username>/mallet-2.0.7`.\n",
    "\n",
    "\n",
    "## Running Mallet\n",
    "\n",
    "We run the MALLET program below, but we don't give it any commands so it complains with `Unrecognized command` and then spit out the list of commands it expects. That means it is working! Now we can actually do something!\n",
    "\n",
    "Now that we have installed MALLET and we have some data, we can start topic modeling. 😊\n",
    "\n",
    "Topic modeling with MALLET involves two steps:\n",
    "\n",
    "- Importing your data\n",
    "- Training the model\n",
    "\n",
    "Importing the data involves re-shaping the narrative text into a **bag-of-words**. It also performs some cleaning such as removing stopwords.\n",
    "\n",
    "Training the model is the **topic modeling** part of the exercise that performs the algorithmic magic to tell you ***what it all means***.\n",
    "\n",
    "## Importing data\n",
    "\n",
    "There is one final bit of data processing that needs to happen for us to be able to topic model. We need to **import** our Austen-Bronte text files using the MALLET application and create a `.mallet` file. \n",
    "\n",
    "For command-line simplicity let's copy all out austen-bronte text files in the `d:/austen-bronte` folder, and let's **mkdir** a `d:/uasten-bronte-mallet` output folder. You many replace `d` with `c`.\n",
    "\n",
    "Let's import the java JDK path and test our java:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Java/jdk1.8.0_151/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!D:/Java/jdk1.8.0_151/bin/java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will probably work on Mac and ***not*** on Windows. So on WIndows, ***run the cell code on a terminal***! Don't forget to create your folders `austen-bronte` and `austen-bronte-mallet`, and copy austen-bronte `.txt` files into your `austen-bronte` folder.\n",
    "\n",
    "The cell below does ***not*** run on my WIndows, so I run it on a terminal instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd mallet-2.0.7/bin\n",
    "mallet import-dir --input d:/austen-bronte --output d:/austen-bronte-mallet/austen-bronte --keep-sequence --remove-stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created a new mallet file, `austen-bronte-mallet/austen-bronte`. This is the file that we will feed back to the MALLET application to **train** a topic modeller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The command below trains a model on the text with 100 topics. You can tweak that number by changing the `--num-topics` parameter. Additionally, we are writing those keywords to a file (rather than just spitting them out on the command line) with the `--output-topic-keys` parameter.\n",
    "\n",
    "Run the cell below if you ***only want** keywords. Run the cell ***following the one below*** for a more complete output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mallet-2.0.7/bin/mallet train-topics --input d:/austen-bronte-mallet/austen-bronte --num-topics 100 --output-topic-keys d:/austen-bronte-mallet/keywords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a minute and a half on my laptop, were done! Now take a look at `keywords.txt` and see if you can get any insights about Austen Bronte romantic novels!\n",
    "\n",
    "For a more thorough analysis, we run the following cell instead, with 50 topics. From mallet's `bin` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet train-topics --input D:/austen-bronte-mallet/austen-bronte --num-topics 20 --output-doc-topics D:/austen-bronte-mallet/doc-topics-austen-bronte.txt --output-topic-keys D:/austen-bronte-mallet/topic-keys-austen-bronte.txt --word-topic-counts-file d:/austen-bronte-mallet/austen-bronte-word-topic.txt --random-seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After about 2 minutes on my laptop, we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "doctopic_triples = []\n",
    "\n",
    "with open(\"D:/austen-bronte-mallet/doc-topics-austen-bronte.txt\") as f:\n",
    "    f.readline()  # read one line in order to skip the header\n",
    "    for line in f:\n",
    "        docnum, docname, *values = line.rstrip().split('\\t')\n",
    "        for topic, share in grouper(2, values):\n",
    "            #print(docname, \", \", topic, \", \", share)\n",
    "            triple = (docname, int(float(topic)), float(share))\n",
    "            doctopic_triples.append(triple)\n",
    "\n",
    "\n",
    "# sort the triples\n",
    "doctopic_triples.sort(key=operator.itemgetter(0,1))\n",
    "docnames = sorted(set([triple[0] for triple in doctopic_triples]))\n",
    "docnames_base = np.array([os.path.splitext(os.path.basename(n))[0] for n in docnames])\n",
    "num_topics = len(doctopic_triples) // len(docnames)\n",
    "doctopic = np.empty((len(docnames), num_topics))\n",
    "for i, (doc_name, triples) in enumerate(itertools.groupby(doctopic_triples, key=operator.itemgetter(0))):\n",
    "    doctopic[i, :] = np.array([share for _, _, share in triples])\n",
    "\n",
    "docnames = docnames_base\n",
    "\n",
    "# get the topic words\n",
    "with open('D:/austen-bronte-mallet/topic-keys-austen-bronte.txt') as input:\n",
    "    topic_keys_lines = input.readlines() \n",
    "\n",
    "bronte_topic_words = []\n",
    "\n",
    "for line in topic_keys_lines:\n",
    "    #print(line)\n",
    "    n1, n2, thewords = line.rstrip().split('\\t')  # tab-separated\n",
    "    #print(thewords)\n",
    "    thewords2 = thewords.split(' ')  # remove the trailing '\\n'\n",
    "    #print(thewords2)\n",
    "    bronte_topic_words.append(thewords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join(bronte_topic_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the documents are ordered in a sequence, we can plot the fate, so to speak, of this topic over time with the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys_lines[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the most interesting topic profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = doctopic[:, 3]\n",
    "plt.plot(series, '.')  # '.' specifies the type of mark to use on the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys_lines[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = doctopic[:, 9]\n",
    "plt.plot(series, '.')  # '.' specifies the type of mark to use on the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization communicates the essential information about the prevalence of a **topic** (*unsupervisably detected via LDA*) in the **corpus**.\n",
    "\n",
    "It would be useful to include an indication of where the various volumes start and end. Another enhancement would add some kind of “smoothing” to the time series in order to better communicate the underlying trend.\n",
    "\n",
    "A rolling average of the topic shares turns out be a useful form of smoothing in this case. We are interested in the prevalence of a topic over time and whether a topic disappears completely in one 500-word chunk of text (only to reappear in the next) does not interest us. We want to visualize the underlying trend, that is, we need some model or heuristic capable of capturing the idea that the topic (or any similar feature) has an underlying propensity to appear at varying points of the novel and that while this propensity may change over time it does not fluctuate wildly.\n",
    "\n",
    "A rolling or [moving average](https://en.wikipedia.org/wiki/Moving_average) of a time series associates with each point in the series the average of some fixed number of previous observations (including the current observation). This fixed number of observations is often called a **window**. The idea of a rolling mean (implemented in `pandas.rolling_mean()`) is effectively communicated visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = doctopic[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# the values on the x-axis (xs) are simply a sequence of integers\n",
    "# corresponding to the texts (also the rows in the document topic matrix)\n",
    "xs = np.arange(len(series))\n",
    "series_smooth = pd.Series(series).rolling(window=15).mean()\n",
    "#series_std = pd.Series(series).rolling(window=15).std()\n",
    "\n",
    "# now we need to calculate at what index each volume starts\n",
    "\n",
    "# method #1\n",
    "#volume_names = [\"Austen_Emma\", \"Austen_Pride\", \"Austen_Sense\", \"CBronte_Jane\", \"CBronte_Professor\", \"CBronte_Villette\"]\n",
    "#volume_indexes = []\n",
    "\n",
    "#for volname in volume_names:\n",
    "#    for i, docname in enumerate(docnames):\n",
    "#        if volname in docname:\n",
    "#            volume_indexes.append(i)\n",
    "#            break\n",
    "            \n",
    "# method #2, uses NumPy functions\n",
    "# hmmm... not too sure here...\n",
    "volume_indexes = []\n",
    "for volname in volume_names:\n",
    "    volume_indexes.append(np.min(np.nonzero([volname in docname for docname in docnames])))\n",
    "    \n",
    "\n",
    "# now we can assemble the plot\n",
    "plt.plot(series, '.', alpha=0.3)\n",
    "plt.plot(series_smooth, '-', linewidth=2)\n",
    "plt.vlines(volume_indexes, ymin=0, ymax=np.max(series))\n",
    "text_xs = np.array(volume_indexes) + np.diff(np.array(volume_indexes + [max(xs)]))/2\n",
    "text_ys = np.repeat(max(series), len(volume_names)) - 0.05\n",
    "for x, y, s in zip(text_xs, text_ys, volume_names):\n",
    "    plt.text(x, y, s, horizontalalignment='center')\n",
    "\n",
    "plt.title('Austen-Bronte, Topic #3 (madame pupils english desk garden school lesson words teacher class)')\n",
    "plt.ylabel(\"Topic share\")\n",
    "plt.xlabel(\"Novel segment\")\n",
    "plt.ylim(0, max(series))\n",
    "#plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
