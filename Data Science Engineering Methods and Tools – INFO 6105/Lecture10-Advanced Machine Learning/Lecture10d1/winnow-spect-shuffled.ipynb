{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 9 Day 1, 18 November 2019</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: The winnow\n",
    "\n",
    "Ok, that was a tough notebook, right? Building a simple neural network is not that tough, however. All you need is some knowledge of object-oriented python and linear algebra, matrices, and matrix multiplication. For fun, we're going to program an **Artificial Neural Network** that is of the simplest kind.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"images/winnow.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "### Winnowing \n",
    "[Winnowing](https://en.wikipedia.org/wiki/Winnowing) means **removing unwanted items**. Its purpose as an algorthm is to train a binary classifier based on binary features, using a *linear* decision boundary.\n",
    "\n",
    "In other words, the goal is to predict one of two states, using a collection of features which are all binary.\n",
    "\n",
    "Unlike classical ANN training procedures, Winnow uses *multiplicative* rather than *additive* updates on the weights between nodes.\n",
    "\n",
    "The prediction model assigns weights to each feature. To predict the state of an observation, it checks all the features that are **active** (true, or detected in an observation) and sums up the weights assigned to these features. If the total is ***above*** a certain threshold, the result is true, otherwise it’s false. \n",
    "\n",
    "I like the Winnow algorithm a lot because it shows you how even a General Linear Regression (GLM) with a **single** layer and a **linear** activation function can model (and thus *learn from*) a dataset. Now imagine a number of these layers put together in a chain, with non-linear activation functions, and you have yourself an artificial neural network (ANN). Imagine what it can do!\n",
    "\n",
    "### The algorithm: \n",
    "\n",
    "We create a fully-connected network, and initialize weights $w_1 = w_2 = \\cdots = w_n = 1$.\n",
    "\n",
    "Then we iterate on each observation consisting of a vector of dimension $n$: $ = [x_1, x_2, \\cdots, x_n]$, representing $n$ features.\n",
    "\n",
    "We predict (for each iteration **epoch**): Output is 1 if *{some condition}*. Output is 0 otherwise.\n",
    "\n",
    "Then, we get the **true** (binary) label corresponding to that observation, and we update the weights **only if we make a mistake**:\n",
    "- **False-positive** error (we predict 0 wheras the label is really 1): Then for each $x_i == 1$, we set $w_i = 2*w_i$.\n",
    "- **False-negative** error (we predict 1 wheras the label is really 0): Then for each $x_i == 1$, we set $wi = wi/2$.\n",
    "\n",
    "### The english:\n",
    "Here is the *english* behind the algorithm:\n",
    "If our network predicts true but should predict false, it is **over-shooting**, so weights that were used in the prediction (i.e. the weights attached to active features) should be ***reduced***.\n",
    "Conversely, if the prediction is false but the correct result should be true, it is **under-shooting** and the active features are not used enough to reach the threshold, so weights should be bumped ***up***.\n",
    "\n",
    "Our goal is to minimze the number of mistakes. When we're down to the minimum we can achieve, we say we have **converged**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The dataset: Single Proton Emission Computed Tomography data\n",
    "\n",
    "The Machine learning repository at the University of California, Irvine, has some great data sets. [Here](https://archive.ics.uci.edu/ml/datasets/SPECT+Heart) are data on cardiac Single Proton Emission Computed Tomography (SPECT) images. Each patient classified into two categories: **normal** and **abnormal**.\n",
    "\n",
    "- *The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal and abnormal. The database of 267 SPECT image sets (patients) was processed to extract features that summarize the original SPECT images. As a result, 44 continuous feature pattern was created for each patient. The pattern was further processed to obtain 22 binary feature patterns. The CLIP3 algorithm was used to generate classification rules from these patterns. The CLIP3 algorithm generated rules that were 84.0% accurate (as compared with cardilogists' diagnoses)*. \n",
    "\n",
    "SPECT is a good data set for testing ML algorithms; it has 267 instances that are descibed by 23 binary attribute\n",
    "\n",
    "Our goal is to predict whether a person is categorized normal or abnormal based on 22 binary feature patterns. \n",
    "\n",
    "We should strive to ensure that our accuracy on test data is 70% or above (anything approaching 50% is junk: just a guess!). Rerun your training (which shuffles the data), or change your random number generator seed maybe? Add another layer? Those are all ANN hyperparameters!\n",
    "\n",
    "### The Winnow algorithm:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "initialize weights\n",
    "loop until done\n",
    "  for each training item\n",
    "    compute Y\n",
    "    if correct do nothing\n",
    "    else if incorrect\n",
    "      if computed Y is too large\n",
    "        divide all relevant weights by 2.0\n",
    "      else if computed Y is too small\n",
    "        multiply all relevant weights by 2.0\n",
    "  end for\n",
    "end loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Crazy professor\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/crazy.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "Oh no! Crazy professor tried to create a new class to add the Winnow algorithm and he edited the .ipynb files manually, and he completely **$&!&#~ed up the cells**! Based on what I told you above, can you reconstruct the cells so that it works?\n",
    "\n",
    "Start with:\n",
    "```(python)\n",
    "class Winnow:\n",
    "\n",
    "```\n",
    "\n",
    "and use the code below. Careful, though, the code below is ***not*** in the right order. You may want to create a copy of this notebook using the `File` | `Make a Copy...` menu above, so you can keep an original reference notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int[][] matrix\n",
    "def ShowMatrix(matrix, decimals, numRows, indices):\n",
    "    frmt = '%.' + str(decimals) + 'f'\n",
    "    for i in range(numRows):\n",
    "        if (indices):\n",
    "            print(\"[\" + '%02d' % i + \"]   \", end='')\n",
    "        for j in range(len(matrix[i])):\n",
    "            print(frmt % matrix[i][j] + \" \", end='')\n",
    "        print(\"\")\n",
    "    lastIndex = len(matrix) - 1\n",
    "    if (indices):\n",
    "        print(\"[\" + '%02d' % lastIndex + \"]   \", end='')\n",
    "    for j in range(len(matrix[lastIndex])):\n",
    "        print(frmt % matrix[lastIndex][j] + \" \", end='')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAcc = w.Accuracy(trainData)\n",
    "testAcc = w.Accuracy(testData)\n",
    "\n",
    "print(\"Prediction accuracy on training data = \" + str(trainAcc))\n",
    "print(\"Prediction accuracy on test data = \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final model weights are:\")\n",
    "ShowVector(weights, 4, 8, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # returns double, int[][] trainData\n",
    "    def Accuracy(self, trainData):\n",
    "        numCorrect = 0\n",
    "        numWrong = 0\n",
    "        xValues = [0] *numInput\n",
    "        \n",
    "        for i in range(len(trainData)):\n",
    "            xValues = np.copy(trainData[i])\n",
    "            target = trainData[i][numInput] #last value is target\n",
    "            computed = self.ComputeY(xValues)\n",
    "\n",
    "            if computed == target:\n",
    "                numCorrect += 1\n",
    "            else:\n",
    "                numWrong += 1\n",
    "                \n",
    "        return (numCorrect * 1.0) / (numCorrect + numWrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double[] vector\n",
    "def ShowVector(vector, decimals, valsPerRow, newLine):\n",
    "    frmt = '%.' + str(decimals) + 'f'\n",
    "    for i in range(len(vector)):\n",
    "        if (i % valsPerRow == 0): print(\"\", end='')\n",
    "        print(frmt % vector[i] + \" \", end='')\n",
    "    if (newLine): print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fisher-Yates shuffle algorithm int[][] trainData\n",
    "    def ShuffleObservations(self, trainData):\n",
    "        for i in range(len(trainData)):\n",
    "            r = randint(i, len(trainData) - 1)\n",
    "            tmp = []\n",
    "            tmp = trainData[r]\n",
    "            trainData[r] = trainData[i]\n",
    "            trainData[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # returns double[], int[][] trainData\n",
    "    def TrainWeights(self, trainData):\n",
    "        xValues = [] * numInput\n",
    "        self.ShuffleObservations(trainData)\n",
    "        for i in range(len(trainData)):\n",
    "            #  get the inputs\n",
    "            xValues = np.copy(trainData[i])\n",
    "            \n",
    "            #  last value is target\n",
    "            target = trainData[i][numInput] \n",
    "            \n",
    "            computed = self.ComputeY(xValues)\n",
    "\n",
    "            if (computed == 1 and target == 0):\n",
    "                # need to decrease weight:\n",
    "                for j in range(numInput):\n",
    "                    if (xValues[j] == 0): continue\n",
    "                    self.weights[j] = self.weights[j] / self.alpha #demotion\n",
    "            elif (computed == 0 and target == 1):\n",
    "                # need to increase weight:\n",
    "                for j in range(numInput):\n",
    "                    if (xValues[j] == 0): continue\n",
    "                    self.weights[j] = self.weights[j] * self.alpha #promotion\n",
    "\n",
    "        result = [0.0] *numInput # = number weights\n",
    "        result = self.weights\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of training data are:\")\n",
    "ShowMatrix(trainData, 0, 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/SPECT.test\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, numInput, rndSeed):\n",
    "        self.numInput = numInput\n",
    "        self.weights = [0] *numInput\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = numInput / 2.0\n",
    "        self.threshold = 1.0 * numInput\n",
    "        self.alpha = 2.0\n",
    "        random.seed( rndSeed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of all data are:\n",
      "[00]   0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 \n",
      "[01]   0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1 \n",
      "[02]   0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 \n",
      "[03]   0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 \n",
      "[99]   0 0 0 1 1 1 0 0 0 1 0 1 1 1 0 0 1 \n"
     ]
    }
   ],
   "source": [
    "print(\"First few lines of all data are:\")\n",
    "ShowMatrix(data, 0, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# int[][] data, seed, out int[][] trainData, out int[][] testData\n",
    "def MakeTrainTest(data, pct, seed):\n",
    "    totRows = data.shape[0] #compute number of rows in each result\n",
    "    numTrainRows = int(totRows * pct)\n",
    "    numTestRows = totRows - numTrainRows\n",
    "    #trainData = new int[numTrainRows][]\n",
    "    trainData = np.empty(data.shape)\n",
    "    #testData = new int[numTestRows][]\n",
    "    testData = np.empty(data.shape)\n",
    "    copy = np.empty(data.shape)\n",
    "\n",
    "    # int[][] copy = new int[data.Length][] #  make a copy of data\n",
    "    for i in range(copy.shape[0]):\n",
    "        # by reference to save space\n",
    "        copy[i] = data[i]\n",
    "    for i in range(copy.shape[0]):\n",
    "        # scramble row order of copy\n",
    "        r = randint(i, copy.shape[0] - 1)\n",
    "        tmp = copy[r]\n",
    "        copy[r] = copy[i]\n",
    "        copy[i] = tmp\n",
    "    for i in range(numTrainRows):\n",
    "        # create training\n",
    "        trainData[i] = copy[i]\n",
    "    for i in range(numTestRows):\n",
    "        # create test\n",
    "        testData[i] = copy[i + numTrainRows]\n",
    "        \n",
    "    return trainData, testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis = data['1']\n",
    "diagnosis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first column from training data: It's our label\n",
    "data2 = data.drop(labels='1', axis=1)\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into 80% train and 20% test matrices\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting data into 80% train and 20% test matrices\")\n",
    "trainData, testData = MakeTrainTest(data2.values, 0.8, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # int[] xValues\n",
    "    def ComputeY(self, xValues):\n",
    "        sum = 0.0\n",
    "        for i in range(numInput):\n",
    "            sum += self.weights[i] * xValues[i]\n",
    "        if sum > self.threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of testing data are:\n",
      "[00]   0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 \n",
      "[01]   1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 \n",
      "[02]   0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 \n",
      "[99]   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of testing data are:\")\n",
    "ShowMatrix(testData, 0, 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training using Winnow algorithm\n",
      "Training complete\n",
      "Wall time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Begin training using Winnow algorithm\")\n",
    "numInput = 22\n",
    "w = Winnow(numInput, 0) #rndSeed = 0\n",
    "weights = w.TrainWeights(trainData)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy on training data = 0.99\n",
      "Prediction accuracy on test data = 0.61\n"
     ]
    }
   ],
   "source": [
    "trainAcc = w.Accuracy(trainData)\n",
    "testAcc = w.Accuracy(testData)\n",
    "\n",
    "print(\"Prediction accuracy on training data = \" + str(trainAcc))\n",
    "print(\"Prediction accuracy on test data = \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few lines of all data are:\")\n",
    "ShowMatrix(data, 0, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "class Winnow:\n",
    "    numInput = 0\n",
    "    weights = []\n",
    "    threshold = 0.0\n",
    "    alpha = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting diagnosis of patient with all abnormal readings: \", end='')\n",
    "yays = [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]\n",
    "predicted = w.ComputeY(yays)\n",
    "if predicted == 0:\n",
    "    print(\"normal\")\n",
    "else:\n",
    "    print(\"abnormal\")\n",
    "\n",
    "print(\"Predicting diagnosis of patient with all normal readings: \", end='')\n",
    "nays = [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]\n",
    "predicted2 = w.ComputeY(nays)\n",
    "if predicted2 == 0:\n",
    "    print(\"normal\")\n",
    "else:\n",
    "    print(\"abnormal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign weights based on each observation. Some abnormal features are going to be very discriminative about overall diagnosis, others much less so. The discriminative ones should acquire higher weights. The “frivolous” ones much less.\n",
    "\n",
    "The algorithm uses only incorrect information. Correct information (correct guess of a label) yields a `nop`. So the algorithm essentially drives the weights of irrelevant predictors to 0, *winnowing them out*. This makes Winnow classification effective in situations where many of the predictor variables are irrelevant (frivolous). It's a good way to figure out the important (discriminative) factors. Much like how regression forests told us what doctors should be looking at in correctly diagnosing breast cancer.\n",
    "\n",
    "It is *unclear* how long to train for. We iterate just once through the training data. An alternative would be to \n",
    "Iterate multiple times through the training data, stopping after a fixed number of iterations, or when some desired accuracy has been reached.\n",
    "\n",
    "Believe it or not, **Machine Learning** algorithms are not much different in methodology. Their modeling power is of course much higher due to the artificial neuron’s inherent non-linearities. But the methodology is very similar:\n",
    "\n",
    "- Train model with feature observations in order to refine weights (synapse strength)\n",
    "- Partition data set into 80% observations and 20% test\n",
    "\n",
    "Once your weights are determined, you can reuse them for any other observation with similar features. Most of the Machine Learning research in the last 20 years was about how to update the weights and what kind of non-linear function to use as a neural transfer function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Reshuffle crazy-professor-shuffled cells so that the Winnow works again. You may want to create a copy of this notebook using the `File` | `Make a Copy...` menu above, so you can keep an original reference notebook.\n",
    "\n",
    "How many layers and how many neurons in this fully connected network?\n",
    "\n",
    "Modify the Winnow algorithm so that instead of dividing or multiplying by 2, the increase or decrease in the weights is **proportional to the error**. Does this improve accuracy on the same dataset? Can you improve accuracy by using random forest classification instead?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
