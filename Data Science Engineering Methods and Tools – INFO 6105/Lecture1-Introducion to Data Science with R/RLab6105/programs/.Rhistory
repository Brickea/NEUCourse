x <- c(1,3,2,5)
x
y <- c(4,1,1,2)
y
z <- 4
z
x+y
x*y
x <- rnorm(50)
x
plot(x)
y <- x+rnorm(50,mean=50,sd=.1)
y
plot(x,y)
cor(x,y)
mm <- matrix(1:16,nrow=4,ncol=4)
mm
mm[c(3,4),d(1,2)]
mm
mm[c(1,2),]
mm[c(3,4),c(1,2)]
college <- read.csv("../data/College.csv")
ls
ls()
college <- read.csv("data/College.csv")
ls()
ls
install.packages("dplyr")
college <- read.csv("../data/College.csv")
library(dplyr)
vignette("introduction","dplyr")
install.packages("dplyr")
UPDATE: We may need other packages, and we'll install them
along the way, but caret should ask us if we want to load
them.
If you are having problems with  packages, you can install
the caret packages and allpackages that you might need by typing:
install.packages("caret", dependencies=c("Depends", "Suggests"))
Now, let's load the package that we are going to use in this
## Brickea ML script for wine
library(caret)
## ===============================================
## Part1 load the data and name the col
## ===============================================
# set the file direction
fileFullDirection <- "../data/wine.cvs"
# get the data from csv file
dataSet <- read.csv(fileFullDirection,header=FALSE)
# set all columns names
colnames(dataSet) <- c("classIdentifier","alcohol","malicAcid","ash","alcalinityOfAsh","magnesium","totalPhenols","flavanoids",
"nonflavanoidPhenols","proanthocyanins","colorIntensity","hue","diluted","proline")
## ===============================================
## Part2 create validation set
## ===============================================
# create a list of 80%?of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataSet$classIdentifier, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataSet[-validation_index,]
# use the remaining 80% of data to ?raining and testing the models
dataset <- dataSet[validation_index,]
# use the remaining 80% of data to ?raining and testing the models
dataset <- dataSet[validation_index,]
library(caret)
## Brickea ML script for wine
library(caret)
## ===============================================
## Part1 load the data and name the col
## ===============================================
# set the file direction
fileFullDirection <- "../data/wine.cvs"
# get the data from csv file
dataSet <- read.csv(fileFullDirection,header=FALSE)
# set all columns names
colnames(dataSet) <- c("classIdentifier","alcohol","malicAcid","ash","alcalinityOfAsh","magnesium","totalPhenols","flavanoids",
"nonflavanoidPhenols","proanthocyanins","colorIntensity","hue","diluted","proline")
## ===============================================
## Part2 create validation set
## ===============================================
# create a list of 80%?of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataSet$classIdentifier, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataSet[-validation_index,]
# use the remaining 80% of data to ?raining and testing the models
dataset <- dataSet[validation_index,]
# get the data from csv file
dataSet <- read.csv(fileFullDirection,header=FALSE)
library(caret)
# set the file direction
fileFullDirection <- "../data/wine.cvs"
# get the data from csv file
dataSet <- read.csv(fileFullDirection,header=FALSE)
setwd("D:/NEUCourses/Data Science Engineering Methods and Tools â€“ INFO 6105/Lecture1-Introducion to Data Science with R/RLab6105/programs")
# get the data from csv file
dataSet <- read.csv(fileFullDirection,header=FALSE)
# set all columns names
colnames(dataSet) <- c("classIdentifier","alcohol","malicAcid","ash","alcalinityOfAsh","magnesium","totalPhenols","flavanoids",
"nonflavanoidPhenols","proanthocyanins","colorIntensity","hue","diluted","proline")
# create a list of 80%?of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataSet$classIdentifier, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataSet[-validation_index,]
# use the remaining 80% of data to ?raining and testing the models
dataset <- dataSet[validation_index,]
## Brickea ML script for wine
library(caret)
## ===============================================
## Part1 load the data and name the col
## ===============================================
# set the file direction
fileDirection <- "../data/wine.cvs"
# get the data from csv file
dataSet <- read.csv(fileDirection,header=FALSE)
# set all columns names
colnames(dataSet) <- c("classIdentifier","alcohol","malicAcid","ash","alcalinityOfAsh","magnesium","totalPhenols","flavanoids",
"nonflavanoidPhenols","proanthocyanins","colorIntensity","hue","diluted","proline")
## ===============================================
## Part2 create validation set
## ===============================================
# create a list of 80%?of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataSet$classIdentifier, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataSet[-validation_index,]
# use the remaining 80% of data to ?raining and testing the models
dataset <- dataSet[validation_index,]
## ===============================================
## Part3 Summarize dataset
## ===============================================
dim(dataSet)
View(dataset)
View(dataset)
View(dataSet)
## Brickea ML script for wine
library(caret)
## ===============================================
## Part1 load the data and name the col
## ===============================================
# set the file direction
fileDirection <- "../data/wine.cvs"
# get the data from csv file
dataSet <- read.csv(fileDirection,header=FALSE)
# set all columns names
colnames(dataSet) <- c("classIdentifier","alcohol","malicAcid","ash","alcalinityOfAsh","magnesium","totalPhenols","flavanoids",
"nonflavanoidPhenols","proanthocyanins","colorIntensity","hue","diluted","proline")
## ===============================================
## Part2 create validation set
## ===============================================
# create a list of 80%?of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataSet$classIdentifier, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataSet[-validation_index,]
# use the remaining 80% of data to ?raining and testing the models
dataset <- dataSet[validation_index,]
## ===============================================
## Part3 Summarize dataset
## ===============================================
# Dimension of dataset
dim(dataSet)
dim(validation)
# use the remaining 80% of data to ?raining and testing the models
dataset <- dataSet[validation_index,]
dim(dataSet)
# use the remaining 80% of data to ?raining and testing the models
dataSet <- dataSet[validation_index,]
dim(dataSet)
sapply(dataSet, class)
head(dataSet)
levels(dataSet)
levels(dataSet$classIdentifier)
levels(dataSet$classIdentifier)
dataSet
validation
levels(validation$classIdentifier)
percentage <- prop.table(table(dataSet$classIdentifier)) * 100
cbind(freq=table(dataSet$classIdentifier), percentage=percentage)
# Statistical Summary
summary(dataSet)
# Univariate Plots
x <- dataset[,2:14]
y <- dataset[,1]
par(mfrow=c(1,4))
for(i in 2:14) {
boxplot(x[,i], main=names(dataSet)[i])
}
par(mfrow=c(2,14))
for(i in 2:14) {
boxplot(x[,i], main=names(dataSet)[i])
}
plot(y)
plot(y)
plot(y)
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
install.packages("e1071")
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(classIdentifier~., data=dataSet, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# Random Forest
set.seed(7)
fit.rf <- train(classIdentifier~., data=dataSet, method="rf", metric=metric, trControl=control)
install.packages("e1071")
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
install.packages("e1071")
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
install.packages("e1071")
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
install.packages("e1071")
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
install.packages("caret")
library(caret)
install.packages("caret")
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
fit.knn <- train(classIdentifier~., data=dataSet, method="knn", metric=metric, trControl=control)
set.seed(7)
fit.knn <- train(classIdentifier~., data=dataSet, method="knn", metric=metric, trControl=control)
set.seed(7)
fit.rf <- train(classIdentifier~., data=dataSet, method="rf", metric=metric, trControl=control)
# Levels of class
levels(dataSet$classIdentifier)
is.factor(dataSet)
dataS
dataSet
# Important
# You need to make sure your dataset has factor module!!!
# is.factor() can show if dataset has factor
# as.factor() can make attribute be factor
as.factor(classIdentifier)
# Important
# You need to make sure your dataset has factor module!!!
# is.factor() can show if dataset has factor
# as.factor() can make attribute be factor
as.factor(dataSet.classIdentifier)
# Important
# You need to make sure your dataset has factor module!!!
# is.factor() can show if dataset has factor
# as.factor() can make attribute be factor
dataSet$classIdentifier <- as.factor(dataSet$classIdentifier)
is.factor(dataSet)
dataSet$classIdentifier <- as.factor(dataSet$classIdentifier)
is.factor(dataSet)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
install.packages("e1071")
library(e1071)
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
install.packages("e1071")
set.seed(7)
fit.lda <- train(classIdentifier~., data=dataSet, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(classIdentifier~., data=dataSet, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# Random Forest
set.seed(7)
fit.rf <- train(classIdentifier~., data=dataSet, method="rf", metric=metric, trControl=control)
results <- resamples(list(lda=fit.lda, knn=fit.knn, rf=fit.rf))
summary(results)
dotplot(results)
# summarize Best Model
print(fit.lda)
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$Species)
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$classIdentifier)
validation$classIdentifier <- as.factor(validation$classIdentifier)
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$classIdentifier)
