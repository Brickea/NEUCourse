{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Sep Summary</div>\n",
    "<div style=\"text-align: right\">Zixiao Wang, 21 October 2019, with material from Dino Konstantopoulos and John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for Oct, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian\n",
    "\n",
    "#### 1. Day 1\n",
    "\n",
    "* [Bayesian1 Bayesian Estimation](../Lecture5-Bayesian/Lecture5d1/bayesian-1-(horny-boston-sloths).ipynb) or [least edition](../Lecture5-Bayesian/Lecture5d2/bayesian-1-(horny-boston-sloths).ipynb)\n",
    "    * **Problem**\n",
    "    \n",
    "    How to understand\n",
    "    \n",
    "     In classical statistical estimation, the parameters are point estimates, **and if you want an error range, you need to do voodoo math like the T-test.** In Bayesian statistical estimation, the parameters are pdfs and thus naturally yield the probable value (the mean of the pdf), and its error (the standard deviation of the pdf).\n",
    "    \n",
    "    * Bayesian inference\n",
    "    * The pain in Bayes' formula\n",
    "    * Markov-Chain Monte Carlo\n",
    "    * Flipping a coin\n",
    "        * credible interval\n",
    "    * Flaws of frequentist statistics\n",
    "        * **p-values** \n",
    "        * **Confidence Intervals** (C.I)\n",
    "        * Confidence Intervals (C.I) are not **probability distributions** and therefore they ***do not*** provide the most probable value for a parameter and the most probable values.\n",
    "    * Bayes Theorem\n",
    "    * Bayes factor\n",
    "    * High Density interval (HDI)\n",
    "    \n",
    "* Bayesian Estimation Note\n",
    "    - [Credible interval VS. Confidence](https://freakonometrics.hypotheses.org/18117)\n",
    "        - To summary: \n",
    "            - the Credible interval means that the probability of generated data from certain method will be in the interval\n",
    "            - the Confidence interval mean that the probability of observed data will be in the interval \n",
    "    - HDI VS. Confidence Intervals (C.I)\n",
    "        - The HDI is a probability but the C.I is not a probability.\n",
    "        -  Since HDI is a probability, the 95% HDI gives the 95% most credible values. It is also guaranteed that 95 % values will lie in this interval, unlike C.I\n",
    "    - Python Programming\n",
    "        - PyMC3 is not the only MCMC package. Sampyl is another, simpler one. Read about it. Here too. Let's install it. Better use an Anaconda terminal. If that does not work, try the below.\n",
    "    \n",
    "\n",
    "* [T-test](../Lecture5-Bayesian/Lecture5d1/continuous-outcome-classical-stats-(T-test).ipynb)\n",
    "    * **Problem:**\n",
    "    \n",
    "    What does this work for?\n",
    "    ```python\n",
    "    stats.t.ppf(q=0.05,  # Quantile to check\n",
    "    df=42)  # Degrees of freedom\n",
    "    ```\n",
    "     \n",
    "    * Bayesian vs Frequentist Statistics\n",
    "    * T-test and the p-ratio(p-value)\n",
    "        * The T-test is a statistical test, based on the p-value\n",
    "        * The T-test is used to determine whether a numeric sampling differs significantly from the population, or whether two samples differ from one another\n",
    "        * This probability of seeing a result as strange or more strange than the one observed is known as the [p-value](https://en.wikipedia.org/wiki/P-value).\n",
    "    * Classical Statistical Hypothesis Testing for 2 groups with continuous outcome - T-test\n",
    "        * Terminology in T-test\n",
    "            * A confidence interval is a range of sample values above and below a point estimate of a parameter (like the mean) that captures the true population parameter at some predetermined confidence level.\n",
    "            * The significance level (denoted by  𝛼 ) is 1 - the confidence interval bound. So, a significance level of 0.05 corresponds to a confidence level of 95%.\n",
    "            * Margin of error: The way you calculate the margin of error depends on whether you know the spread of the population or not. If you know the standard deviation  𝜎  of the population (a measure of spread), the margin of error is equal to:\n",
    "            $$ z ∗ \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "                * Z Where $z$ is a number known as the [z-critical value](https://en.wikipedia.org/wiki/Z-test). The **z-critical value** is the *number* of standard deviations you'd have to go from the mean of the distribution to capture the proportion of the data associated with the desired confidence level. \n",
    "\n",
    "                * For instance, we know that roughly 95% of the data in a *normal* (gaussian) distribution lies within 2 standard deviations of the mean, so we use 2 as the `z-critical value` for a 95% confidence interval. For all other distributions, you use the quantile function `stats.t.ppf` to compute $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Day 1 Markov-Chain Monte Carlo Introduction\n",
    "\n",
    "[Author](https://blog.csdn.net/dreamflylin/article/details/6315736)\n",
    "\n",
    "来源：MCMC In practice第一章introducing Markov chain monte carlo\n",
    "\n",
    "    下午看bayesian computation with R 的第六章的时候一头雾水，搞不清楚bayesian, MCMC ，M-H，以及Gibbs sampling这些概念之间的关系是什么\n",
    "    后来看了这本书的第一章，总算搞清楚了，下面总结下这些概念之间的关系：\n",
    "\n",
    "### 1. MCMC方法主要是为了解决有些baysian推断中参数期望E(f(v)|D)不能直接计算得到的问题的。\n",
    "\n",
    "> 其中v是要估计的参数，D是数据观察值\n",
    "\n",
    "### 2. Markov chain monte carlo概念包含了两部分：markov chain 和monte carlo integration。\n",
    "\n",
    "#### 2.1 首先是monte carlo integration： monte carlo integration是利用采样的方法解决参数期望不能直接计算解决的问题的：\n",
    "    \n",
    "        即根据v的后验概率密度函数对v进行n次随机采样，计算n个f(v)，然后将这n个值求平均。根据大数定理当n足够大并且采样服从独立原则的时候，该值趋向于期望的真实值。但是当v的后验概率函数很难得到的时候该方法并不适用。\n",
    "        \n",
    "        \n",
    "#### 2.2 而在此基础上产生的 Markov chain monte carlo，虽然也是通过采样的方法进行的，却将马尔科夫链的概念引进来。它的想法是这样的：\n",
    "    \n",
    "        如果某条马尔科夫链具有irreducible 和aperiodic的特性的时候，该马尔科夫链具有一个唯一的静态点，即Pt=Pt-1;因此当马尔科夫链足够长后（设为N），产生的值会收敛到一个恒定的值（m）。这样对f（v）产生马尔科夫链，在N次之后f（v）的值收敛于恒定的值m，一般假设n>N后，f(v)服从N(m,scale)的正态分布。\n",
    "        \n",
    "        即当n足够大的时候，用马尔科夫链产生的f(v)相当于在N(m,scale)独立抽样产生的值。\n",
    "\n",
    "### 3. 如何产生具有这样特性的马尔科夫链：主要的方法是M-H算法\n",
    "\n",
    "[Metroplis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\n",
    "\n",
    "        M-H算法有两部分组成：\n",
    "        1. 根据条件概率密度函数，抽样得到下一个时间点的参数值Vt+1；\n",
    "        2. 计算产生的这个值的接受概率a。如果a有显著性，就接受抽样得到的值，否则下一时间点的值保持不变。\n",
    "        在1中引入了proposal distribution的概念。在参数取值连续的情况下，后一个时间点的值服从一个分布，而这个分布函数只和前一个时间点的值有关q（.|Vt）\n",
    "  \n",
    "        a的计算这里不贴了，一般都一样。重要的计算完a之后，如果决定接受采样获得的值还是保持原来值不变。在这个问题上，一般的处理方法是假设a服从0~1的均匀分布，每次采样计算a后都从U(0,1)中随机抽样一个值a'，如果a>=a'则接受抽样的值，否则保持原来的值不变。\n",
    "\n",
    "    根据q（.|Vt）的不同，M-H算法又有不同的分类：\n",
    "        1. Metroplis algorithm :在这里假设q(Vt+1|Vt)=q(|Vt+1 - Vt|)因此a被化解。该方法叫做random walk metropolis。\n",
    "        2. independence sampler:在这个算法里，假设q(Vt+1|Vt)=q(Vt+1)\n",
    "\n",
    "对于多参数的情况，既可以同时产生多向量的马尔科夫链，又可以对每个参数分别进行更新：即，如果有h个参数需要估计，那么，在每次迭代的时候，分h次每次更新一个参数。\n",
    "\n",
    "### 4. Gibbs抽样，H-M算法的一个变体\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian\n",
    "\n",
    "#### 2. Day 2\n",
    "\n",
    "* [Bayesian 2 Bayesian Statistical Analysis](../Lecture5-Bayesian/Lecture5d2/bayesian-2-(radon-girlfriend)-start.ipynb)\n",
    "    * Why Bayesian Statistical? \n",
    "        * In the case of Big Data, we have so much data that we know exactly what the probability distributions may be. Uncertainties not required. Frequentist statistics work great in these use cases.\n",
    "        * Oftentimes however, even in the case of Big Data, there are conditions where not enough data is available, e.g. snowing with sun glare and white trucks. That is when accidents happen with autonomous vehicles!\n",
    "    * Probability Distributions\n",
    "        * Probability mass function (pmf)\n",
    "        * Probability distribution function (pdf)\n",
    "    * Expectation of a random variable\n",
    "    * Statistical Inference\n",
    "        * Statistical inference is the process of learning from incomplete or error-contaminated data. We account for this incompleteness and imperfection using either a sampling model with incomplete samples or errors in measurement.\n",
    "    * Statistical Hypothesis Testing \n",
    "        * The **de facto non-Bayesian (always frequentist)** standard for statistical inference is called **statistical hypothesis testing**. The goal of hypothesis testing is to evaluate a null hypothesis. \n",
    "        * The results of statistical hypothesis tests are very **easy to misinterpret**. But we'll review the principal ones in class. Just remember that your instructor told you that the math behind statistical hypothesis testing sucks! and that **the right model is a Bayesis model**. But if you interview for a big pharma company, and they do this kind of testing all the time, tell them how great you think it is ;-) Many companies are only in the process of discovering modern data science (and you can help them in this)!\n",
    "        * Instead of testing, **a more informative and effective approach for inference is based on estimation**, which can be frequentist or Bayesian. That is, rather than testing whether two statistical groups are different, let's estimate **how different they are** by modeling them first!, and then comparing models.\n",
    "        * Additionally, we include an estimate of uncertainty associated with that difference, which includes uncertainty due to our lack of knowledge of model parameters (**epistemic uncertainty 认知不确定性**) and uncertainty due to the inherent stochasticity of the modeling system (**aleatory uncertainty 任意不确定性**).\n",
    "    * Frequentist Statistics\n",
    "        * **Data** observed is considered **random**, because it is the realization of random processes and hence will vary each time one goes to observe the system.\n",
    "        * Model **parameters** are considered **fixed**. A parameter's true value is uknown but fixed. For example, Jesus Christ is a central parameter in the Christian World Model. Christians will say the world order may be random because of human misgivings, but Jesus Christ and his compassion is fixed and steadfast.\n",
    "        * Frequentist Statistics: In a frequentist world, new estimators need to be derived for every parameter  𝜃  introduced.\n",
    "        <div style=\"font-size:20px\">\n",
    "        \\\\[f(y \\; | \\; \\theta)\\\\]\n",
    "        </div>\n",
    "    * Bayesian\n",
    "        * Data is considered **fixed**. It used to be random, but once stored in your lab notebook or spreadsheet, it does not change.\n",
    "        * Model parameters may not be random, but Bayesians use probability distribtutions to describe their uncertainty in values, and are therefore ***treated as random***! In some cases, it is useful to consider parameters as having been sampled from probability distributions. For example, some Christians may postulate that world order is predetermined, however Jesus Christ's compassion may vary because sometimes he gets exasperated by his followers (yikes!).\n",
    "        <div style=\"font-size:20px\">\n",
    "        \\\\[p(\\theta \\; | \\; y)\\\\]\n",
    "        </div>\n",
    "    * Bayes' Formula\n",
    "        * The denominator \\\\(p(y)\\\\) usually ***cannot be computed directly***, and is actually the expression in the numerator integrated over all possible model parameters \\\\(\\theta\\\\):\n",
    "        * In the continuous case, this is Bayes' formula:\n",
    "        <div style=\"font-size: 120%;\">  \n",
    "        \\\\[Pr(\\theta\\;|\\;y) = \\frac{Pr(y\\;|\\;\\theta)Pr(\\theta)}{\\int Pr(y\\;|\\;\\theta)Pr(\\theta) d\\theta}\\\\]\n",
    "        </div>\n",
    "    * Bayesian Inference, in 3 Easy Steps\n",
    "        * Step 1: Specify a probability model\n",
    "        * Step 2: Calculate a posterior distribution\n",
    "        * Step 3: Check your model\n",
    "    * Lab #1: Radon gas (use variational inference to model data)\n",
    "        * Why are we using the log of the Radon measurement: To squash the measurement as much as possible into a single order of magnitude.\n",
    "        * Pick the model profile\n",
    "            * Recall that the first step in Bayesian inference is specifying a full probability model for the problem.\n",
    "            * This is a very **common approach to building a Bayesian model**: Get the histogram, see if it looks like Poisson, Normal, Exponential, or a bunch of other analytic pdf's we will or have introduced (like the Beta and Gamma distributions), and try to fit the data to a model of the distribution by varying the distribution's paramter(s).\n",
    "        * Choice of priors\n",
    "            * we will assume ***no prior knowledge*** (about the priors), and specify a **diffuse** prior for each parameter.\n",
    "            * Since the mean can take any real value (since it is on the log scale), we will use ***another normal distribution*** here, and specify a *large* variance to allow the possibility of very large or very small values:\n",
    "        $$\\mu \\sim N(0, 10^2)$$\n",
    "    * Introduce to PyMC3\n",
    "        * PyMC3 is a Python library for programming Bayesian analysis; \n",
    "        * It helps us solve tough inverse problems and extract a model from the data.\n",
    "        * **Very important to fit the Bayesian model**\n",
    "            * Guess the hyperparameter\n",
    "            * Fit the model\n",
    "        * Useful function:\n",
    "            * All Distribution(check the document for detail)\n",
    "            * ```fit``` function can help you fit the observation data to the model you guessed(by using variational inference to model data)\n",
    "            * ```plot_posterior``` function can plot the posterior you have just fit\n",
    "    * Model checking\n",
    "        * seaborn\n",
    "        ```python\n",
    "        sns.distplot(radon_samples, label='simulated')\n",
    "        sns.distplot(hennepin_radon, label='observed')\n",
    "        plt.legend()\n",
    "        ```\n",
    "    * Lab #2: Spying on my girlfriend ( Markov Chain Monte Carlo (MCMC) methods to model data)\n",
    "        *  A Poisson random variable is a more appropriate model for this type of count data\n",
    "        * Useful function:\n",
    "            * ```pm.math.switch```\n",
    "            \n",
    "            > lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)\n",
    "            \n",
    "            This code creates a new function lambda_, but really we can think of it as a random variable: the random variable  𝜆  from above. The switch() function assigns lambda_1 or lambda_2 as the value of lambda_, depending on what side of tau we are on. The values of lambda_ up until tau are lambda_1 and the values afterwards are lambda_2.\n",
    "            * step = pm.Metropolis() Choose Metropolist as the method to fit the data\n",
    "            * trace = pm.sample(4000, tune=1000, step=step) Get 4000 posterior sample\n",
    "                * tune: Number of iterations to tune, defaults to 500. Ignored when using ‘SMC’. Samplers adjust the step sizes, scalings or similar during tuning. Tuning samples will be drawn in addition to the number specified in the draws argument, and will be discarded unless discard_tuned_samples is set to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian\n",
    "\n",
    "#### 2. Day 2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
