{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Sep Summary</div>\n",
    "<div style=\"text-align: right\">Zixiao Wang, 21 October 2019, with material from Dino Konstantopoulos and John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for Oct, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian\n",
    "\n",
    "#### 1. Day 1\n",
    "\n",
    "* [Bayesian1 Bayesian Estimation](../Lecture5-Bayesian/Lecture5d1/bayesian-1-(horny-boston-sloths).ipynb) or [least edition](../Lecture5-Bayesian/Lecture5d2/bayesian-1-(horny-boston-sloths).ipynb)\n",
    "    * **Problem**\n",
    "    \n",
    "    How to understand\n",
    "    \n",
    "     In classical statistical estimation, the parameters are point estimates, **and if you want an error range, you need to do voodoo math like the T-test.** In Bayesian statistical estimation, the parameters are pdfs and thus naturally yield the probable value (the mean of the pdf), and its error (the standard deviation of the pdf).\n",
    "    \n",
    "    * Bayesian inference\n",
    "    * The pain in Bayes' formula\n",
    "    * Markov-Chain Monte Carlo\n",
    "    * Flipping a coin\n",
    "        * credible interval\n",
    "    * Flaws of frequentist statistics\n",
    "        * **p-values** \n",
    "        * **Confidence Intervals** (C.I)\n",
    "        * Confidence Intervals (C.I) are not **probability distributions** and therefore they ***do not*** provide the most probable value for a parameter and the most probable values.\n",
    "    * Bayes Theorem\n",
    "    * Bayes factor\n",
    "    * High Density interval (HDI)\n",
    "    \n",
    "* Bayesian Estimation Note\n",
    "    - [Credible interval VS. Confidence](https://freakonometrics.hypotheses.org/18117)\n",
    "        - To summary: \n",
    "            - the Credible interval means that the probability of generated data from certain method will be in the interval\n",
    "            - the Confidence interval mean that the probability of observed data will be in the interval \n",
    "    - HDI VS. Confidence Intervals (C.I)\n",
    "        - The HDI is a probability but the C.I is not a probability.\n",
    "        -  Since HDI is a probability, the 95% HDI gives the 95% most credible values. It is also guaranteed that 95 % values will lie in this interval, unlike C.I\n",
    "    - Python Programming\n",
    "        - PyMC3 is not the only MCMC package. Sampyl is another, simpler one. Read about it. Here too. Let's install it. Better use an Anaconda terminal. If that does not work, try the below.\n",
    "    \n",
    "\n",
    "* [T-test](../Lecture5-Bayesian/Lecture5d1/continuous-outcome-classical-stats-(T-test).ipynb)\n",
    "    * **Problem:**\n",
    "    \n",
    "    What does this work for?\n",
    "    ```python\n",
    "    stats.t.ppf(q=0.05,  # Quantile to check\n",
    "    df=42)  # Degrees of freedom\n",
    "    ```\n",
    "     \n",
    "    * Bayesian vs Frequentist Statistics\n",
    "    * T-test and the p-ratio(p-value)\n",
    "        * The T-test is a statistical test, based on the p-value\n",
    "        * The T-test is used to determine whether a numeric sampling differs significantly from the population, or whether two samples differ from one another\n",
    "        * This probability of seeing a result as strange or more strange than the one observed is known as the [p-value](https://en.wikipedia.org/wiki/P-value).\n",
    "    * Classical Statistical Hypothesis Testing for 2 groups with continuous outcome - T-test\n",
    "        * Terminology in T-test\n",
    "            * A confidence interval is a range of sample values above and below a point estimate of a parameter (like the mean) that captures the true population parameter at some predetermined confidence level.\n",
    "            * The significance level (denoted by  ğ›¼ ) is 1 - the confidence interval bound. So, a significance level of 0.05 corresponds to a confidence level of 95%.\n",
    "            * Margin of error: The way you calculate the margin of error depends on whether you know the spread of the population or not. If you know the standard deviation  ğœ  of the population (a measure of spread), the margin of error is equal to:\n",
    "            $$ z âˆ— \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "                * Z Where $z$ is a number known as the [z-critical value](https://en.wikipedia.org/wiki/Z-test). The **z-critical value** is the *number* of standard deviations you'd have to go from the mean of the distribution to capture the proportion of the data associated with the desired confidence level. \n",
    "\n",
    "                * For instance, we know that roughly 95% of the data in a *normal* (gaussian) distribution lies within 2 standard deviations of the mean, so we use 2 as the `z-critical value` for a 95% confidence interval. For all other distributions, you use the quantile function `stats.t.ppf` to compute $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Day 1 Markov-Chain Monte Carlo Introduction\n",
    "\n",
    "[Author](https://blog.csdn.net/dreamflylin/article/details/6315736)\n",
    "\n",
    "æ¥æºï¼šMCMC In practiceç¬¬ä¸€ç« introducing Markov chain monte carlo\n",
    "\n",
    "    ä¸‹åˆçœ‹bayesian computation with R çš„ç¬¬å…­ç« çš„æ—¶å€™ä¸€å¤´é›¾æ°´ï¼Œæä¸æ¸…æ¥šbayesian, MCMC ï¼ŒM-Hï¼Œä»¥åŠGibbs samplingè¿™äº›æ¦‚å¿µä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆ\n",
    "    åæ¥çœ‹äº†è¿™æœ¬ä¹¦çš„ç¬¬ä¸€ç« ï¼Œæ€»ç®—ææ¸…æ¥šäº†ï¼Œä¸‹é¢æ€»ç»“ä¸‹è¿™äº›æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼š\n",
    "\n",
    "### 1. MCMCæ–¹æ³•ä¸»è¦æ˜¯ä¸ºäº†è§£å†³æœ‰äº›baysianæ¨æ–­ä¸­å‚æ•°æœŸæœ›E(f(v)|D)ä¸èƒ½ç›´æ¥è®¡ç®—å¾—åˆ°çš„é—®é¢˜çš„ã€‚\n",
    "\n",
    "> å…¶ä¸­væ˜¯è¦ä¼°è®¡çš„å‚æ•°ï¼ŒDæ˜¯æ•°æ®è§‚å¯Ÿå€¼\n",
    "\n",
    "### 2. Markov chain monte carloæ¦‚å¿µåŒ…å«äº†ä¸¤éƒ¨åˆ†ï¼šmarkov chain å’Œmonte carlo integrationã€‚\n",
    "\n",
    "#### 2.1 é¦–å…ˆæ˜¯monte carlo integrationï¼š monte carlo integrationæ˜¯åˆ©ç”¨é‡‡æ ·çš„æ–¹æ³•è§£å†³å‚æ•°æœŸæœ›ä¸èƒ½ç›´æ¥è®¡ç®—è§£å†³çš„é—®é¢˜çš„ï¼š\n",
    "    \n",
    "        å³æ ¹æ®vçš„åéªŒæ¦‚ç‡å¯†åº¦å‡½æ•°å¯¹vè¿›è¡Œnæ¬¡éšæœºé‡‡æ ·ï¼Œè®¡ç®—nä¸ªf(v)ï¼Œç„¶åå°†è¿™nä¸ªå€¼æ±‚å¹³å‡ã€‚æ ¹æ®å¤§æ•°å®šç†å½“nè¶³å¤Ÿå¤§å¹¶ä¸”é‡‡æ ·æœä»ç‹¬ç«‹åŸåˆ™çš„æ—¶å€™ï¼Œè¯¥å€¼è¶‹å‘äºæœŸæœ›çš„çœŸå®å€¼ã€‚ä½†æ˜¯å½“vçš„åéªŒæ¦‚ç‡å‡½æ•°å¾ˆéš¾å¾—åˆ°çš„æ—¶å€™è¯¥æ–¹æ³•å¹¶ä¸é€‚ç”¨ã€‚\n",
    "        \n",
    "        \n",
    "#### 2.2 è€Œåœ¨æ­¤åŸºç¡€ä¸Šäº§ç”Ÿçš„ Markov chain monte carloï¼Œè™½ç„¶ä¹Ÿæ˜¯é€šè¿‡é‡‡æ ·çš„æ–¹æ³•è¿›è¡Œçš„ï¼Œå´å°†é©¬å°”ç§‘å¤«é“¾çš„æ¦‚å¿µå¼•è¿›æ¥ã€‚å®ƒçš„æƒ³æ³•æ˜¯è¿™æ ·çš„ï¼š\n",
    "    \n",
    "        å¦‚æœæŸæ¡é©¬å°”ç§‘å¤«é“¾å…·æœ‰irreducible å’Œaperiodicçš„ç‰¹æ€§çš„æ—¶å€™ï¼Œè¯¥é©¬å°”ç§‘å¤«é“¾å…·æœ‰ä¸€ä¸ªå”¯ä¸€çš„é™æ€ç‚¹ï¼Œå³Pt=Pt-1;å› æ­¤å½“é©¬å°”ç§‘å¤«é“¾è¶³å¤Ÿé•¿åï¼ˆè®¾ä¸ºNï¼‰ï¼Œäº§ç”Ÿçš„å€¼ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªæ’å®šçš„å€¼ï¼ˆmï¼‰ã€‚è¿™æ ·å¯¹fï¼ˆvï¼‰äº§ç”Ÿé©¬å°”ç§‘å¤«é“¾ï¼Œåœ¨Næ¬¡ä¹‹åfï¼ˆvï¼‰çš„å€¼æ”¶æ•›äºæ’å®šçš„å€¼mï¼Œä¸€èˆ¬å‡è®¾n>Nåï¼Œf(v)æœä»N(m,scale)çš„æ­£æ€åˆ†å¸ƒã€‚\n",
    "        \n",
    "        å³å½“nè¶³å¤Ÿå¤§çš„æ—¶å€™ï¼Œç”¨é©¬å°”ç§‘å¤«é“¾äº§ç”Ÿçš„f(v)ç›¸å½“äºåœ¨N(m,scale)ç‹¬ç«‹æŠ½æ ·äº§ç”Ÿçš„å€¼ã€‚\n",
    "\n",
    "### 3. å¦‚ä½•äº§ç”Ÿå…·æœ‰è¿™æ ·ç‰¹æ€§çš„é©¬å°”ç§‘å¤«é“¾ï¼šä¸»è¦çš„æ–¹æ³•æ˜¯M-Hç®—æ³•\n",
    "\n",
    "[Metroplis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\n",
    "\n",
    "        M-Hç®—æ³•æœ‰ä¸¤éƒ¨åˆ†ç»„æˆï¼š\n",
    "        1. æ ¹æ®æ¡ä»¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ŒæŠ½æ ·å¾—åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„å‚æ•°å€¼Vt+1ï¼›\n",
    "        2. è®¡ç®—äº§ç”Ÿçš„è¿™ä¸ªå€¼çš„æ¥å—æ¦‚ç‡aã€‚å¦‚æœaæœ‰æ˜¾è‘—æ€§ï¼Œå°±æ¥å—æŠ½æ ·å¾—åˆ°çš„å€¼ï¼Œå¦åˆ™ä¸‹ä¸€æ—¶é—´ç‚¹çš„å€¼ä¿æŒä¸å˜ã€‚\n",
    "        åœ¨1ä¸­å¼•å…¥äº†proposal distributionçš„æ¦‚å¿µã€‚åœ¨å‚æ•°å–å€¼è¿ç»­çš„æƒ…å†µä¸‹ï¼Œåä¸€ä¸ªæ—¶é—´ç‚¹çš„å€¼æœä»ä¸€ä¸ªåˆ†å¸ƒï¼Œè€Œè¿™ä¸ªåˆ†å¸ƒå‡½æ•°åªå’Œå‰ä¸€ä¸ªæ—¶é—´ç‚¹çš„å€¼æœ‰å…³qï¼ˆ.|Vtï¼‰\n",
    "  \n",
    "        açš„è®¡ç®—è¿™é‡Œä¸è´´äº†ï¼Œä¸€èˆ¬éƒ½ä¸€æ ·ã€‚é‡è¦çš„è®¡ç®—å®Œaä¹‹åï¼Œå¦‚æœå†³å®šæ¥å—é‡‡æ ·è·å¾—çš„å€¼è¿˜æ˜¯ä¿æŒåŸæ¥å€¼ä¸å˜ã€‚åœ¨è¿™ä¸ªé—®é¢˜ä¸Šï¼Œä¸€èˆ¬çš„å¤„ç†æ–¹æ³•æ˜¯å‡è®¾aæœä»0~1çš„å‡åŒ€åˆ†å¸ƒï¼Œæ¯æ¬¡é‡‡æ ·è®¡ç®—aåéƒ½ä»U(0,1)ä¸­éšæœºæŠ½æ ·ä¸€ä¸ªå€¼a'ï¼Œå¦‚æœa>=a'åˆ™æ¥å—æŠ½æ ·çš„å€¼ï¼Œå¦åˆ™ä¿æŒåŸæ¥çš„å€¼ä¸å˜ã€‚\n",
    "\n",
    "    æ ¹æ®qï¼ˆ.|Vtï¼‰çš„ä¸åŒï¼ŒM-Hç®—æ³•åˆæœ‰ä¸åŒçš„åˆ†ç±»ï¼š\n",
    "        1. Metroplis algorithm :åœ¨è¿™é‡Œå‡è®¾q(Vt+1|Vt)=q(|Vt+1 - Vt|)å› æ­¤aè¢«åŒ–è§£ã€‚è¯¥æ–¹æ³•å«åšrandom walk metropolisã€‚\n",
    "        2. independence sampler:åœ¨è¿™ä¸ªç®—æ³•é‡Œï¼Œå‡è®¾q(Vt+1|Vt)=q(Vt+1)\n",
    "\n",
    "å¯¹äºå¤šå‚æ•°çš„æƒ…å†µï¼Œæ—¢å¯ä»¥åŒæ—¶äº§ç”Ÿå¤šå‘é‡çš„é©¬å°”ç§‘å¤«é“¾ï¼Œåˆå¯ä»¥å¯¹æ¯ä¸ªå‚æ•°åˆ†åˆ«è¿›è¡Œæ›´æ–°ï¼šå³ï¼Œå¦‚æœæœ‰hä¸ªå‚æ•°éœ€è¦ä¼°è®¡ï¼Œé‚£ä¹ˆï¼Œåœ¨æ¯æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œåˆ†hæ¬¡æ¯æ¬¡æ›´æ–°ä¸€ä¸ªå‚æ•°ã€‚\n",
    "\n",
    "### 4. GibbsæŠ½æ ·ï¼ŒH-Mç®—æ³•çš„ä¸€ä¸ªå˜ä½“\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian\n",
    "\n",
    "#### 2. Day 2\n",
    "\n",
    "* [Bayesian 2 Bayesian Statistical Analysis](../Lecture5-Bayesian/Lecture5d2/bayesian-2-(radon-girlfriend)-start.ipynb)\n",
    "    * Why Bayesian Statistical? \n",
    "        * In the case of Big Data, we have so much data that we know exactly what the probability distributions may be. Uncertainties not required. Frequentist statistics work great in these use cases.\n",
    "        * Oftentimes however, even in the case of Big Data, there are conditions where not enough data is available, e.g. snowing with sun glare and white trucks. That is when accidents happen with autonomous vehicles!\n",
    "    * Probability Distributions\n",
    "        * Probability mass function (pmf)\n",
    "        * Probability distribution function (pdf)\n",
    "    * Expectation of a random variable\n",
    "    * Statistical Inference\n",
    "        * Statistical inference is the process of learning from incomplete or error-contaminated data. We account for this incompleteness and imperfection using either a sampling model with incomplete samples or errors in measurement.\n",
    "    * Statistical Hypothesis Testing \n",
    "        * The **de facto non-Bayesian (always frequentist)** standard for statistical inference is called **statistical hypothesis testing**. The goal of hypothesis testing is to evaluate a null hypothesis. \n",
    "        * The results of statistical hypothesis tests are very **easy to misinterpret**. But we'll review the principal ones in class. Just remember that your instructor told you that the math behind statistical hypothesis testing sucks! and that **the right model is a Bayesis model**. But if you interview for a big pharma company, and they do this kind of testing all the time, tell them how great you think it is ;-) Many companies are only in the process of discovering modern data science (and you can help them in this)!\n",
    "        * Instead of testing, **a more informative and effective approach for inference is based on estimation**, which can be frequentist or Bayesian. That is, rather than testing whether two statistical groups are different, let's estimate **how different they are** by modeling them first!, and then comparing models.\n",
    "        * Additionally, we include an estimate of uncertainty associated with that difference, which includes uncertainty due to our lack of knowledge of model parameters (**epistemic uncertainty è®¤çŸ¥ä¸ç¡®å®šæ€§**) and uncertainty due to the inherent stochasticity of the modeling system (**aleatory uncertainty ä»»æ„ä¸ç¡®å®šæ€§**).\n",
    "    * Frequentist Statistics\n",
    "        * **Data** observed is considered **random**, because it is the realization of random processes and hence will vary each time one goes to observe the system.\n",
    "        * Model **parameters** are considered **fixed**. A parameter's true value is uknown but fixed. For example, Jesus Christ is a central parameter in the Christian World Model. Christians will say the world order may be random because of human misgivings, but Jesus Christ and his compassion is fixed and steadfast.\n",
    "        * Frequentist Statistics: In a frequentist world, new estimators need to be derived for every parameter  ğœƒ  introduced.\n",
    "        <div style=\"font-size:20px\">\n",
    "        \\\\[f(y \\; | \\; \\theta)\\\\]\n",
    "        </div>\n",
    "    * Bayesian\n",
    "        * Data is considered **fixed**. It used to be random, but once stored in your lab notebook or spreadsheet, it does not change.\n",
    "        * Model parameters may not be random, but Bayesians use probability distribtutions to describe their uncertainty in values, and are therefore ***treated as random***! In some cases, it is useful to consider parameters as having been sampled from probability distributions. For example, some Christians may postulate that world order is predetermined, however Jesus Christ's compassion may vary because sometimes he gets exasperated by his followers (yikes!).\n",
    "        <div style=\"font-size:20px\">\n",
    "        \\\\[p(\\theta \\; | \\; y)\\\\]\n",
    "        </div>\n",
    "    * Bayes' Formula\n",
    "        * The denominator \\\\(p(y)\\\\) usually ***cannot be computed directly***, and is actually the expression in the numerator integrated over all possible model parameters \\\\(\\theta\\\\):\n",
    "        * In the continuous case, this is Bayes' formula:\n",
    "        <div style=\"font-size: 120%;\">  \n",
    "        \\\\[Pr(\\theta\\;|\\;y) = \\frac{Pr(y\\;|\\;\\theta)Pr(\\theta)}{\\int Pr(y\\;|\\;\\theta)Pr(\\theta) d\\theta}\\\\]\n",
    "        </div>\n",
    "    * Bayesian Inference, in 3 Easy Steps\n",
    "        * Step 1: Specify a probability model\n",
    "        * Step 2: Calculate a posterior distribution\n",
    "        * Step 3: Check your model\n",
    "    * Lab #1: Radon gas (use variational inference to model data)\n",
    "        * Why are we using the log of the Radon measurement: To squash the measurement as much as possible into a single order of magnitude.\n",
    "        * Pick the model profile\n",
    "            * Recall that the first step in Bayesian inference is specifying a full probability model for the problem.\n",
    "            * This is a very **common approach to building a Bayesian model**: Get the histogram, see if it looks like Poisson, Normal, Exponential, or a bunch of other analytic pdf's we will or have introduced (like the Beta and Gamma distributions), and try to fit the data to a model of the distribution by varying the distribution's paramter(s).\n",
    "        * Choice of priors\n",
    "            * we will assume ***no prior knowledge*** (about the priors), and specify a **diffuse** prior for each parameter.\n",
    "            * Since the mean can take any real value (since it is on the log scale), we will use ***another normal distribution*** here, and specify a *large* variance to allow the possibility of very large or very small values:\n",
    "        $$\\mu \\sim N(0, 10^2)$$\n",
    "    * Introduce to PyMC3\n",
    "        * PyMC3 is a Python library for programming Bayesian analysis; \n",
    "        * It helps us solve tough inverse problems and extract a model from the data.\n",
    "        * **Very important to fit the Bayesian model**\n",
    "            * Guess the hyperparameter\n",
    "            * Fit the model\n",
    "        * Useful function:\n",
    "            * All Distribution(check the document for detail)\n",
    "            * ```fit``` function can help you fit the observation data to the model you guessed(by using variational inference to model data)\n",
    "            * ```plot_posterior``` function can plot the posterior you have just fit\n",
    "    * Model checking\n",
    "        * seaborn\n",
    "        ```python\n",
    "        sns.distplot(radon_samples, label='simulated')\n",
    "        sns.distplot(hennepin_radon, label='observed')\n",
    "        plt.legend()\n",
    "        ```\n",
    "    * Lab #2: Spying on my girlfriend ( Markov Chain Monte Carlo (MCMC) methods to model data)\n",
    "        *  A Poisson random variable is a more appropriate model for this type of count data\n",
    "        * Useful function:\n",
    "            * ```pm.math.switch```\n",
    "            \n",
    "            > lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)\n",
    "            \n",
    "            This code creates a new function lambda_, but really we can think of it as a random variable: the random variable  ğœ†  from above. The switch() function assigns lambda_1 or lambda_2 as the value of lambda_, depending on what side of tau we are on. The values of lambda_ up until tau are lambda_1 and the values afterwards are lambda_2.\n",
    "            * step = pm.Metropolis() Choose Metropolist as the method to fit the data\n",
    "            * trace = pm.sample(4000, tune=1000, step=step) Get 4000 posterior sample\n",
    "                * tune: Number of iterations to tune, defaults to 500. Ignored when using â€˜SMCâ€™. Samplers adjust the step sizes, scalings or similar during tuning. Tuning samples will be drawn in addition to the number specified in the draws argument, and will be discarded unless discard_tuned_samples is set to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian\n",
    "\n",
    "#### 2. Day 2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
