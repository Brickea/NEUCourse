{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Sep Summary</div>\n",
    "<div style=\"text-align: right\">Zixiao Wang, 15 October 2019, with material from Dino Konstantopoulos and John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for Sep, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1\n",
    "\n",
    "[Introduction about R](../Lecture1-Introducion&#32;to&#32;Data&#32;Science&#32;with&#32;R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advice\n",
    "\n",
    "Some students asked me for pointers for good books. I don't read programming books, they put me to sleep. Instead, i've indicated some good youtube videos below.\n",
    "\n",
    "But you know what, you don't need them. You already have one of the best guides in introductory data science. Not only at NU, or only in the USA. So, take advantage of him, and spend the rest of your time with dragons and magic, sports, or clubbing, instead ðŸ˜‰ Spend most of your time on your class python notebooks. By the end of the class, you'll be a good python programmer, good enough for most jobs out there.\n",
    "\n",
    "We covered a lot of material today and I encourage you to rerun every single cell and understand what is going on. In the last section, I moved code around to get you familiar with thinking of code as Lego blocks that you assemble together. And there are different ways of putting things together, and some ways may be better for others, but the one you pick is good for you and that's good enough for me (as long as you get the correct solution ðŸ™‚!\n",
    "\n",
    "Turtle is a neat drawing package, but don't try it if you don't already have programming experience, because you'll crash often.\n",
    "\n",
    "The genetic programming hw is similar to your R hw. Do what we did, but for different data. It's going to be challenging, but you will learn a lot. And we only have one weekend to learn basic python before we move on. There is so much more to learn, this semester!\n",
    "\n",
    "Why did I introduce genetic programming (GP) today? Because it teaches you about searching intelligently for solutions in the space of all possible solutions. Which is what you want to do when you try to build a model for data. How to find the best model? It's also how Darwinian evolution works (solutions with better fitness than others are more attractive and so tend to be picked more for sexual mating, which leads to even fitter descendents), so if it's pretty good for life on earth, it's also pretty good for computer problems. Also, a lot of machine learning algorithms use GP to optimize hyperparameters in their algorithms, so it's good to know.\n",
    "\n",
    "-dino\n",
    "\n",
    "* [Google Developer Python Course](https://www.youtube.com/playlist?list=PLfZeRfzhgQzTMgwFVezQbnpc1ck0I6CQl)\n",
    "\n",
    "* [30 minute crash course in Python](https://learnxinyminutes.com/docs/python/)\n",
    "\n",
    "* [Intro to Python for Data Science](https://campus.datacamp.com/courses/intro-to-python-for-data-science/chapter-1-python-basics?ex=1)\n",
    "\n",
    "* [Introduction to Deep Learning with Python](https://www.youtube.com/watch?v=S75EdAcXHKk)\n",
    "\n",
    "* [Optional: Lecture 1 | Machine Learning (Stanford)](https://www.youtube.com/watch?v=UzxYlbK2c7E)\n",
    "\n",
    "* [Install Anaconda (see next slide for instructions) and test drive here](http://conda.pydata.org/docs/test-drive.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming in Python \n",
    "\n",
    "#### 1. Day 1\n",
    "\n",
    "* A brief introduction to the language Python in 10 chapters [Part 1](../Lecture2-Just%20Big%20Numbers/Lecture2a/A%20brief%20introduction%20to%20the%20language%20Python%20in%2010%20chapters.ipynb)\n",
    "    * Difference between **extend** and **append**\n",
    "    * How the ```zip``` function work\n",
    "* A brief introduction to the language Python in 10 chapters [Part 2](../Lecture2-Just%20Big%20Numbers/Lecture2a/A%20brief%20introduction%20to%20the%20language%20Python%20in%2010%20chapters%2C%20Part%202.ipynb)\n",
    "    * When write ```%%file filename```, this will create a file named filename is current file direction\n",
    "    * How the decorators ```@``` in Python work?\n",
    "* A brief introduction to [Object Oriented programming in Python](../Lecture2-Just%20Big%20Numbers/Lecture2a/A%20brief%20introduction%20to%20Object%20Oriented%20programming%20in%20Python%20.ipynb)\n",
    "\n",
    "#### 2. Day 2\n",
    "* [Introduction-to-Python-Recursion](../Lecture2-Just%20Big%20Numbers/Lecture2b/Introduction-to-Python-Recursion-solution-forstudents.ipynb)\n",
    "    * turtle\n",
    "    ```python\n",
    "    import turtle\n",
    "    from turtle as Turtle\n",
    "\n",
    "    t = Turtle()\n",
    "\n",
    "    # Set the color and turtle shape invisible\n",
    "    t.screen.bgcolor(\"black\")\n",
    "    t.color(\"red\")\n",
    "    t.hideturtle()\n",
    "\n",
    "    # Draw the plots\n",
    "    t.fd(length)\n",
    "    t.left(90)\n",
    "    t.right(90)\n",
    "\n",
    "    # Move the pen\n",
    "    t.penup()\n",
    "    t.goto(points[0][0],points[0][1])\n",
    "    t.pendown()\n",
    "    ```\n",
    "\n",
    "    * random\n",
    "    ```python\n",
    "    import random\n",
    "\n",
    "    a = random.randint(0,300)\n",
    "    b = random.choice([\"blue\",\"green\",\"other color\"])\n",
    "\n",
    "    nums = random.choices(range(100), k=10)\n",
    "    # [8, 76, 98, 39, 12, 17, 71, 48, 22, 65]\n",
    "    ```\n",
    "* Labs in [Python Genetic Algorithms as application to Iteration OR Recursion](../Lecture2-Just%20Big%20Numbers/Lecture2b/Labs%20in%20Python%20Genetic%20Algorithms%20as%20application%20to%20Iteration%20OR%20Recursion.ipynb)\n",
    "    * random\n",
    "    ```python\n",
    "    import random\n",
    "\n",
    "    geneSet = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!.\"\n",
    "    test1,test2=random.sample(geneSet, 2)\n",
    "    # ('Y', 'c')\n",
    "    ```\n",
    "    * datetime\n",
    "    ```python\n",
    "    import datetime\n",
    "    strat_time = datetime.datetime.now()\n",
    "    action()\n",
    "    time_different = str(datetime.datetime.now() - start_time)\n",
    "    ```\n",
    "\n",
    "#### 3. [Assignment 2](../Lecture2-Just%20Big%20Numbers/assignment2/assignment2.ipynb)\n",
    "* Implenment Genetic algorithm in sorting numbers\n",
    "* Use turtle library in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability\n",
    "\n",
    "#### 1. Day 1\n",
    "\n",
    "* [Probability1 start](../Lecture3-Probability/probability1-start.ipynb)\n",
    "    * Dice (singular: Die)\n",
    "    * Urns, permutations, and combinations\n",
    "    * Working with transformations instead of collections\n",
    "    * Python Key words\n",
    "    ```python\n",
    "    reduce\n",
    "\n",
    "    operator.mul\n",
    "\n",
    "    fractions.Fraction\n",
    "\n",
    "    itertools.combinations\n",
    "\n",
    "    random.sample\n",
    "\n",
    "    math.factorial\n",
    "\n",
    "    count \"'foobar'.count('o')==2\"\n",
    "\n",
    "    transformations VS collections\n",
    "\n",
    "    yield\n",
    "\n",
    "    queue.Queue()\n",
    "\n",
    "    enumerate()\n",
    "\n",
    "    callable()\n",
    "    ```\n",
    "    \n",
    "> We talked about frequentist and Bayesian statistics, defined each, and took a first peek at Bayes' Theorem, a pillar of Data Science. Probabilities is all about counting: What's the number of alternate (quantum) universes that can exist? Computers are essentially counting machines. So we built a counting framework to let computers do the counting for us :-)\n",
    "\n",
    "* [Probability brithday](../Lecture3-Probability/probability1-birthdays.ipynb)\n",
    "    * The Birthday paradox\n",
    "    \n",
    "#### 2. Day 2\n",
    "\n",
    "* [Probability start 2](../Lecture3-Probability/probability-2-start-fa19.ipynb)\n",
    "    * Probability Distributions\n",
    "        * binomial distribution -> Bernoulli distribution\n",
    "        * probability mass function is for discrete variable\n",
    "        * probability distribution function is for continuous variable\n",
    "    * Python Key Words\n",
    "    ```python\n",
    "\n",
    "    dict.pmf()\n",
    "\n",
    "    numpy.np.aragne\n",
    "\n",
    "    scipy.stats.binom\n",
    "\n",
    "    matplotlib.pyplot as plt\n",
    "\n",
    "    range() or arange()\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.subplots()\n",
    "\n",
    "    self.update(data,**kwargs)\n",
    "\n",
    "    marginal probability\n",
    "    ```\n",
    "* [Genetic algorithm](../Lecture3-Probability/ga-polynomial-fit.ipynb)\n",
    "    * Python Key Words\n",
    "    ```python\n",
    "    random.choices\n",
    "\n",
    "    random.uniform\n",
    "\n",
    "    np.random.rand()\n",
    "    ```\n",
    "    * Fitness Proportionate Selection\n",
    "    \n",
    "#### 3. [Assignment 3](../Lecture3-Probability/Assignment3/Assignment%203.ipynb)\n",
    "* Conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to numpy and pandas and Classical Statistical Data Analysis MOM MLE\n",
    "\n",
    "#### 1. Day 1\n",
    "\n",
    "* [Introduction to the NumPy library](../Lecture4-Tools%20libraries/Lecture4a-Introduction%20to%20numpy%20pandas/introduction-to-numpy-fa19.ipynb)\n",
    "    * Python Key Words\n",
    "    ```python\n",
    "    ### 1. python:\n",
    "\n",
    "    open\n",
    "\n",
    "    string.split\n",
    "\n",
    "    map\n",
    "\n",
    "    filter\n",
    "\n",
    "    generator--yield\n",
    "\n",
    "    ### 2. numpy:\n",
    "\n",
    "    np.rnadom.randint(10,size=(4,5,6))\n",
    "\n",
    "    np.array.ndim()\n",
    "\n",
    "    np.array.shape()\n",
    "\n",
    "    np.array.size()\n",
    "\n",
    "    reshape\n",
    "\n",
    "    concatenation\n",
    "\n",
    "    split\n",
    "\n",
    "    numpy.linspace(start,stop,num)\n",
    "\n",
    "    numpy.meshgrid(np.array,np.array)\n",
    "\n",
    "    numpy.mat\n",
    "\n",
    "    np.random.rand(m,n)\n",
    "\n",
    "    ### 3. matplotlib:\n",
    "\n",
    "    matplotlib.imshow(2,cmap=plt.cm.gray)\n",
    "\n",
    "    all posibilities of cmap\n",
    "\n",
    "    matplotlib.colorbar()\n",
    "    ```\n",
    "* [Introduction to Python Pandas](../Lecture4-Tools%20libraries/Lecture4a-Introduction%20to%20numpy%20pandas/introduction-to-pandas-fa19.ipynb)\n",
    "    * Python Key Words\n",
    "    ```python\n",
    "    ### 1. pandas\n",
    "\n",
    "    pd.Series -> Vector\n",
    "\n",
    "    pd.DataFrames\n",
    "\n",
    "    pd.DataFrames work independently of index by using numpy.matrices\n",
    "\n",
    "    pd.datareader\n",
    "\n",
    "    pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "\n",
    "    pandas_datareader.DataReader\n",
    "\n",
    "    shape\n",
    "\n",
    "    info\n",
    "\n",
    "    aapl['Volume'].plot()\n",
    "\n",
    "    DataFrame.corr()\n",
    "\n",
    "    pd.read_csv()\n",
    "\n",
    "    pd.DataFrames.sort_values\n",
    "\n",
    "    pd.DataFrames.iloc\n",
    "\n",
    "    pd.sum()\n",
    "\n",
    "    pd.DataFrames.reset_index()\n",
    "\n",
    "    pd.DataFrames.groupby()\n",
    "\n",
    "    pd.DataFrames.plot(kind)\n",
    "\n",
    "    pd.DataFrames.head()\n",
    "\n",
    "    pd.DataFrames.tail()\n",
    "\n",
    "    pd.DataFrames.describe()\n",
    "\n",
    "    pd.DataFrames.plotting.autocorrelation_plot()\n",
    "\n",
    "    ### 2. numpy\n",
    "\n",
    "    np.diff()\n",
    "\n",
    "    np.log\n",
    "\n",
    "    np.linspcae\n",
    "\n",
    "    ### 3. others\n",
    "\n",
    "    scipy.single.lombscargle\n",
    "\n",
    "    PrettyPandas\n",
    "\n",
    "    seaborn\n",
    "\n",
    "    Clustergrammer\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Day 2\n",
    "* [Random Forest](../Lecture4-Tools%20libraries/Lecture4a-forest%20regression/dino-pandas-forest-regression.ipynb)\n",
    "    * RUN HERE FOR SIMPLE STANDALONE VERSION\n",
    "        * Using Skicit-learn to split data into training and testing sets\n",
    "        * Split the data into training and testing sets\n",
    "        * Import the model we are using\n",
    "        * Instantiate model \n",
    "        * Train the model on training data\n",
    "        * Use the forest's predict method on the test data\n",
    "        * Calculate the absolute errors\n",
    "* [Classical Statistical Data Analysis MOM MLE](../Lecture4-Tools%20libraries/Lecture4D2/mom-mle-start.ipynb)\n",
    "    * How to understand MOM and MLE [link](https://blog.csdn.net/huguozhiengr/article/details/81607637)\n",
    "    * Method of Moments(MOM)\n",
    "        * The method of moments simply assigns the empirical (coming from the data) mean and variance to their theoretical counterparts (coming from the model, in some case like Gamma!), so that we can solve for the parameters.\n",
    "    * Maximum Likelihood Estimation(MLE)\n",
    "        * MLE is a method of estimating the parameters of a statistical model, given observations. MLE attempts to find the parameter values that maximize the likelihood function, given the observations.\n",
    "    * KS test\n",
    "        * We then either verify by plotting the data and the model with the MLE parameter(s) and see if it's a good match, or use goodness of fit tests like the KS test to get a more objective estimate. \n",
    "    *  Birnbaum-Sanders distribution\n",
    "    * Summary\n",
    "        * MOM equates the empirical and theoretical moments to yield the parameters of your model\n",
    "        * MLE gives you the value which maximises the Likelihood P(D|Î¸).\n",
    "        * How to numerically evaluate the parameters of your model using the first two methods. You do not need to know the math (but if you understand it, that is a good thing!)\n",
    "        \n",
    "#### 3. [Assignment 4](../Lecture4-Tools%20libraries/Assignment%204/001058840_ZixiaoWang_4.ipynb)\n",
    "* Data set cleaning\n",
    "* RF to fit the data set\n",
    "* Use MLE to find the best model parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
