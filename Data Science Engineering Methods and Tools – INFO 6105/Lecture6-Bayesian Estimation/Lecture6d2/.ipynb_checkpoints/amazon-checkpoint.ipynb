{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 6 Day 2</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 10 October 2019, with material from Justin Silverman, Thomas Wiecki, and Ravin Kumar</div>\n",
    "\n",
    "# Chief Bayesian Officer (CBO) at Amazon\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"images/amazon.jpg\", width=400 />\n",
    "</center>\n",
    "\n",
    "One of you asked me the other day, ***professor, can you give me a practical application of Bayesian estimation***? Well, I thought, weather statistical modeling sounds pretty practical. But I know what he meant, something ***experiential*** that is more **tangible** than theory. \n",
    "\n",
    "So, have you ever wondered how Amazon decides to buy how mamy of those widgets you cannot live without so that you can then order them from their Web site? I did, for my [DevOps CSYE 7220](http://catalog.northeastern.edu/course-descriptions/csye/) class, where we use DevOps tools and practices to build duck-chasing robots.\n",
    "\n",
    "What if we build a model and actually incorporate the model estimate directly into a decision making process? Can we use an optimizer to find the best decision(s) not only under the most likely scenario, but under *all* possible scenarios? Is that how Amazon does it? I mean, I don't know, I don't work at Amazon. But this sounds like a good approach to the problem.\n",
    "\n",
    "This moves Bayesian modeling from something that informs a decision to something that *makes* a decisio.\n",
    "\n",
    "This problem is called [**supply chain optimization**](https://en.wikipedia.org/wiki/Supply-chain_optimization), and that is something you can definitely mention in any interview that asks you about something concrete you have learned from yor Data Science curriculum that can help the company.\n",
    "\n",
    "But first, we need a quick introduction to **Bayesian Decision Theory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bayesian Decision Theory\n",
    "\n",
    "[Bayesian Decision Theory](https://wiki.lesswrong.com/wiki/Bayesian_decision_theory), a useful [tool](https://en.wikipedia.org/wiki/Bayes_estimator) that provides a **formalism** for **decision-making** under **uncertainty**.\n",
    "\n",
    "It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs. An agent operating under such a decision theory uses the concepts of Bayesian statistics to estimate the expected value of its actions, and update its expectations based on new information.\n",
    "\n",
    "It is used in a diverse range of applications including but definitely not limited to finance for guiding [investment strategies](https://en.wikipedia.org/wiki/Investment_strategy), in engineering for designing [control systems](https://en.wikipedia.org/wiki/Control_system), or in [Sport analytics](https://en.wikipedia.org/wiki/Sports_analytics).\n",
    "\n",
    "We first need to introduce some formal way in which we discuss **decisions**. For a given problem, imagine that there is a space in which all of my possible decisions live. This space can be *discrete*, *continuous*, *multivariate*, or any number of crazy things based on the problem at hand. In what follows we will denote our decision space (regardless of what exactly it is) by $A$ and a decision $a ∈ A$.\n",
    "\n",
    "For example, all trade decisions in the NFL. $a$ might represent a trade of Tom Brady.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"images/tombrady.png\", width=200 />\n",
    "</center>\n",
    "\n",
    "Oh my god, what am I saying?! I'm going to get ***lynched***~ Scrap that! Let's say I am trying to decide a price at which to list my used iPhone I want to sell. In this case I may denote my decision space as the entire positive real line such that $a ∈ [0,+∞)$.\n",
    "\n",
    "In Bayesian decision theory we are concerned with choosing between these different decision based on some **information**. Like our decision space, there is flexibility in what our information may be (univariate, multivariate, continuous, discrete, etc…). Whatever that information is, let's denote the space that our information lives in by $Θ$ and a piece of information (one element of this space) by $θ$ such that $θ ∈ Θ$.\n",
    "\n",
    "For example, I may want to use a model fit to previous online listings to predict the probability that my phone will sell at a given listing price. In this case my information may be the probability that my phone will sell at the specified price such that $Θ ∈ [0,1]$.\n",
    "\n",
    "So how do we determine the ***best*** decision? This requires that we first define some notion of what are we trying to do. The formal object that we use to do this goes by many names depending on the field: let's refer to it as a **Loss function** (L). The crucial idea is that this is a function that allows us to quantify how bad/good a given decision ($a$) is given some information ($θ$).\n",
    "\n",
    "What does it mean to ***quantify***? By convention, it means a real number (between −∞ and +∞). This real value will reflect the ***loss we feel*** if we choose decision $a$ given our information $θ$. We may denote this loss function as\n",
    "\n",
    "$$L:Θ × A → R$$\n",
    "\n",
    "The crucial idea is that the loss function ties together our decision space (A) and our information space (Θ).\n",
    "\n",
    "The hard part of decision theory is often the choice of the loss function.\n",
    "\n",
    "For example, let's say I want to ***maximize the amount of money*** I expect to make (my expected return). In this case I may choose a simple loss function of the following form $L(θ,a) = −θ\\;a$. Recall that in this example $θ$ is a probability that the phone will sell and $a$ is the price I list it at. The negative sign is there because in this case we want to maximize our return so I am denoting this as negative loss.\n",
    "\n",
    "#### Connection to Bayes\n",
    "\n",
    "Rather than just dealing with a single known value for $θ ∈ Θ$ we now work with a **probability distribution** representing our belief in the **true** value of $θ$ which we will denote by $p(θ)$. To connect this to Bayesian statistics, we may have a posterior distribution over $θ$ conditioned on some observed data $y$. It is because Bayesian statistics is often about calculating these posterior distributions that most Bayesian texts on decision theory discuss this uncertainty of information as $p(θ\\;|\\;y)$\n",
    "\n",
    "So how do we figure out the loss associated with individual decisions when we don’t even know the information we want to use to make a decision? The answer is that we turn to probability theory and instead calculate the **Expected Loss** we would feel if we choose a given action given our beliefs (our probability distribution) about θ. We calculate this **Expected loss** just like we would calculate the *expectation* of any other function of a random variable:\n",
    "\n",
    "$$\\text{Expected Loss}(a) = \\int_\\Theta\\mathcal{L}(\\theta, a)p(\\theta)d\\theta$$\n",
    "\n",
    "Essentially the expected loss sums up the loss we would feel for a given decision $a$ over all possible values of our information $θ$, but weighted by the probability of $θ$. \n",
    "\n",
    "In this form the Expected Loss does some pretty remarkable things in terms of decision making. In particular, the Expected Loss balances between how probable each value of $θ$ is and the loss associated with each value of $θ$ for a given $a$.\n",
    " \n",
    "We can approximate this integral by using $N$ samples $(θ_1,…,θ_N)$ from the distribution $p(θ)$:\n",
    " \n",
    "$$\\text{Expected Loss}(a) \\approx \\frac{1}{N}\\sum_{n=1}^N\\mathcal{L}(\\theta_n, a)$$\n",
    " \n",
    "Finally, we need to pick a given value of $a$ that is ***best***. By ***best*** we will mean the decision that ***has the lowest expected loss***. Let's call the decision that minimizes the Expected Loss the **Bayes Action** and denote it $\\hat{a}$ such that:\n",
    "\n",
    "$$\\hat{a} \\approx \\underset{a\\in \\mathcal{A}}{\\text{argmin}} \\frac{1}{N}\\sum_{n=1}^N\\mathcal{L}(\\theta_n, a)$$\n",
    "\n",
    "This part can get ***computationally intensive*** quickly. Even if we were not using simulations and instead solving this optimization explicitly, analytical closed form solutions are only possible for a select set of loss functions and distributions $p(θ)$, much in the same way closed form solutions for the posterior distribution are only available for a few likelihood functions like the gaussian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Amazon robotic kit supply chain optimization\n",
    "\n",
    "So we purchased 3 robotic kits for the class. The first two shipped really fast. The NVIDIA one is expected in a ***month***. And I wondered, how does Amazon know how to stock the robots in its wharehouses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "#import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Robotic kits at Amazon\n",
    "\n",
    "In my DevOps class (CSYE 7220), these are the 3 robotic kits we purchased from Amazon:\n",
    "\n",
    "- [KOOKYE Robot Car Chassis Arduino](https://www.amazon.com/gp/product/B07H2PMSY7/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=UTF8&psc=1)\n",
    "\n",
    "- [SunFounder Smart Video Car Kit V2.0 Raspberry Pi 4](https://www.amazon.com/gp/product/B06XWSVLL8/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1)\n",
    "\n",
    "- [XiaoR Geek JetBot AI Kit Powered by NVIDIA Jetson Nano](https://www.amazon.com/gp/product/B07VY1K7DK/ref=ppx_yo_dt_b_asin_title_o02_s00?ie=UTF8&psc=1)\n",
    "\n",
    "So, let's say that there are only *three* suppliers Amazon can order robot processors from: Arduino, Raspberry PI, and NVIDIA. These suppliers have different prices, different quality items, and different maximum amounts they can ship Amazon within a certain timeframe. \n",
    "\n",
    "- Almost every retailer has this problem of deciding how much to order given yield and holding cost. A similar problem also occurs in insurance where you need to sell contracts which have some risk of becoming claims. Or in online advertising where you need to decide how much to bid on clicks given a budget. Or in the financial investment industry.\n",
    "\n",
    "In manufacturing, **yield** describes the ratio of usable outputs as a percentage of total output. Here we will use it as a synonym for **profit** to Amazon. Yield is never 100% because some kits are returned by unsatisfied customers. \n",
    "\n",
    "We don't know yeild, but Amazon can certainly try to estimate it using Bayesian estimation, and you can help them do it!\n",
    "\n",
    "We know the prices and order sizes, but the true yield distribution is **unobserverable** before selling because many of these kits are returned for refunds, possibly with manufacturing defects. \n",
    "\n",
    "So, here are some unobservable parameters: `SUPPLIER_YIELD` and `SUPPLIER_YIELD_SD`. Let's simulate them, but we will assume we don't know them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPLIER_YIELD = np.array([.9, .5, .8]) # unknown\n",
    "SUPPLIER_YIELD_SD = np.array([.1, .2, .2]) # unknown\n",
    "PRICES = [220.0, 100.0, 120.0] # known\n",
    "MAX_ORDER_SIZE = [100, 80, 100] # known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **yield** represents the profit on robotic kits that are sold by Amazon, known from historical data. \n",
    "\n",
    "90% for Arduino, only 50% for RaspberryPi (because for example, almost half of the sold kits are returned with defects), and 80$ for NVIDIA, with standard eviation `SUPPLIER_YIELD_SD`.\n",
    "\n",
    "Since we can't directly observe the true yield, we will have to **estimate** it from previous batches Amazon ordered from suppliers. \n",
    "\n",
    "Let's assume Amazon has ordered different times from different suppliers, *in batches*. For example, as supplier 3, NVIDIA, only opened up shop recently, Amazon only ordered *twice* from there. \n",
    "\n",
    "The [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is a ***good*** model for the random behavior of **percentages** and **proportions**, in the same way Poisson is a good model for integer counts. That's perfect for simulating yield.\n",
    "\n",
    "These are our past orders for each supplier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = [30, 20, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our (simulated) yields (profits) for Amazon's **historic** batch orders from manufacturers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "data = []\n",
    "for supplier_yield, supplier_yield_sd, n_obs in zip(SUPPLIER_YIELD, SUPPLIER_YIELD_SD, N_OBS):\n",
    "    data.append(pm.Beta.dist(mu=supplier_yield, sd=supplier_yield_sd, shape=n_obs).random())\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the histogram for the yield (profit) to Amazon on each robotic kit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data).T\n",
    "data_tidy = data_df.unstack().to_frame('yield')\n",
    "data_tidy.index = data_tidy.index.set_names(['supplier', 'obs'])\n",
    "g = sns.FacetGrid(data=data_tidy.reset_index().dropna(), col='supplier')\n",
    "g.map(sns.distplot, 'yield', kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have simulated **historic data** for yeidl, and that is all Amazon has to try and estimate true profit on every robotic kit supplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics of the middleman\n",
    "\n",
    "In order to assess how many robotic kits to buy, Amazon first needs to know how many robotic kits is can sell. If Amazon buys too few, it is leaving money on the table, if it buys too many it will have to put them in storage which costs money (`HOLDING_COST`). \n",
    "\n",
    "Let's assume that if Amazon can sell a robotic kit for \\$500, it costs Amazon \\$100 in holding cost. Tha's about the holding cost ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALES_PRICE = 500 \n",
    "HOLDING_COST = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define our [**loss function**](https://en.wikipedia.org/wiki/Loss_function) which takes as inputs how many robotic kits Amazon has in stock, how many robotic kits customers want, at what price Amazon bought the kit, at what price it can sell the kit, and what the holding costs are per kit.\n",
    "\n",
    "See [here](https://en.wikipedia.org/wiki/Profit_margin) for a quick intro to econometrics.\n",
    "\n",
    "Vectorization (`@np.vectorize`) is a fairly well-known to data scientists and is used [routinely](https://towardsdatascience.com/data-science-with-python-turn-your-conditional-loops-to-numpy-vectors-9484ff9c622e) in coding, to speed up the overall data transformation, where simple mathematical transformations are performed over an iterable object such as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def loss(in_stock, demand, buy_price, sales_price=SALES_PRICE, holding_cost=HOLDING_COST):\n",
    "    \n",
    "    # How much does Amazon earn per sell\n",
    "    margin = sales_price - buy_price\n",
    "    \n",
    "    # Do Amazon have more in stock than demanded?\n",
    "    if in_stock > demand:\n",
    "        total_profit = demand * margin\n",
    "        \n",
    "        # everything left over after demand was met goes into holding\n",
    "        total_holding_cost = (in_stock - demand) * holding_cost\n",
    "        reward = total_profit - total_holding_cost\n",
    "        \n",
    "    else:\n",
    "        # Can only sell what is in stock, no storage required\n",
    "        reward = in_stock * margin\n",
    "    \n",
    "    # Usually we minimize, so invert\n",
    "    return -reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our loss function. Note this just an *example loss function* for our own data experiment. Other loss functions might look completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_stock = np.arange(0, 100)\n",
    "plt.scatter(in_stock, -loss(in_stock, 50, 50)); \n",
    "plt.axvline(50, c='k', ls='-', label='assumed demand');\n",
    "plt.xlabel('in stock'); \n",
    "plt.ylabel('profit (neg loss)'); \n",
    "sns.despine(); \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if customer demand is 50 robotic kits, Amazon maximizes profit if it has 50 kits in stock. Having fewer kits eats into profits at a greater rate than ordering excess kits because in this setup margins are larger than the holding cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need an estimate of **demand**. As Amazon has a long history of selling robotic kits, is has a pretty good idea of what the distribution looks like, but let's assume that we don't know the true underlying parameters and only have access to ***samples***. \n",
    "\n",
    "Demand is a **count**, so we use a count model. i.e. a **Poisson model**. We simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_samples = stats.poisson(60, 40).rvs(1000)\n",
    "sns.distplot(demand_samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate our **profit function** over every demand we observed historically. Setting kits in stock to `100`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(demand_samples, -loss(in_stock=100, demand=demand_samples, buy_price=10))\n",
    "plt.xlabel('demand'); plt.ylabel('profit (neg loss)'); plt.axvline(100, c='k', ls='--', label='assumed in stock');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to demand, the loss-function behaves differently: with less demand than what we have in stock, we earn less (because we sell fewer robotic kits but also have to pay holding costs), but as demand exceeds the number of robotic kits we have in stock our profit stays flat because we can't sell more than what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating yield with a Bayesian model\n",
    "\n",
    "Let's use [`PyMC3`](http://pymc.io) to build a model to estimate the yield of every robotic kit supplier, using a Beta as likelihood, and half gaussians as its parameters.\n",
    "\n",
    "- I know, I know, ***how did you know, professor***? Because half guassians are typical priors for the Beta. You wouldn't know this, so you would try different priors before you locate the good ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors on alpha and beta parameters for each supplier\n",
    "    α = pm.HalfNormal('α', sd=10., shape=3)\n",
    "    β = pm.HalfNormal('β', sd=10., shape=3)\n",
    "    \n",
    "    # Different likelihood for every supplier because we have different\n",
    "    # number of data points\n",
    "    for i, d in enumerate(data):\n",
    "        pm.Beta(f'supplier_yield_obs_{i}', \n",
    "            alpha=α[i], beta=β[i],\n",
    "            observed=d)\n",
    "    \n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "[arviz](https://pypi.org/project/arviz/0.2.0/) is a Python package for exploratory analysis of Bayesian models.\n",
    "\n",
    "```(python)```\n",
    "pip install arviz\n",
    "```\n",
    "(did not work for me so well)\n",
    "\n",
    "Its `.plot_energy()` API helps to diagnose poor exploration by gradient-based algorithms like [HMC](https://arxiv.org/pdf/1701.02434.pdf) or NUTS.\n",
    "\n",
    "Compared to using a Gaussian random walk proposal distribution in the Metropolis–Hastings algorithm, [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo) (HMC) reduces the correlation between successive sampled states by proposing moves to distant states which maintain a high probability of acceptance. It's a better MCMC algorithm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "# make sure convergence looks good\n",
    "az.plot_energy(trace);\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate possible future scenarios\n",
    "\n",
    "In order to perform Bayesian Decision Making we need an estimate of what the future might look like. So, we just need to sample from the [posterior predictive distribution](https://en.wikipedia.org/wiki/Posterior_predictive_distribution) of our model ,which generates ***fake data*** based on **estimated posteriors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    post_pred = pm.sample_ppc(trace, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_yield_post_pred = pd.DataFrame({k: v[:, 1] for k, v in post_pred.items()})\n",
    "data_tidy = supplier_yield_post_pred.unstack().to_frame('yield')\n",
    "data_tidy.index = data_tidy.index.set_names(['supplier', 'obs'])\n",
    "g = sns.FacetGrid(data=data_tidy.reset_index().dropna(), col='supplier')\n",
    "g.map(sns.distplot, 'yield', kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows, given the data and our model, what we can expect to observe. Note that these predictions take **uncertainty** into account. Kookye is a great performer for Amazon. For supplier 2, ***we have a lot of uncertainty*** because we only observed *very few* data points. NVIDIA is fairly new, with a lower batting average than Kookye.\n",
    "\n",
    "Given these estimates we can write a function that converts orders Amazon receives, to how many kits are unreturned (happy customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_yield_and_price(orders, supplier_yield=np.array([.9, .5, .8]), prices=PRICES):\n",
    "    orders = np.asarray(orders)\n",
    "    \n",
    "    full_yield = np.sum(supplier_yield * orders)\n",
    "    price_per_item = np.sum(orders * prices) / np.sum(orders)\n",
    "    \n",
    "    return full_yield, price_per_item\n",
    "\n",
    "# Given these customer orders, \n",
    "calc_yield_and_price([100, 60, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given these (randomly picked) order amounts to each supplier and some deterministic yield, customers would be happy with  168 robotic kits at an effective price of \\$160 each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Decision Making\n",
    "\n",
    "Now we have to actually do the optimization. First, we need to specify our objective function which will compute total profit and effective price given a [posterior predictive](https://en.wikipedia.org/wiki/Posterior_predictive_distribution) sample. \n",
    "\n",
    "Once we have that and our demand (also a sample), we can compute Amazon's loss. As we have a distribution over possible scenarios, we compute the loss for each one and return the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(orders,\n",
    "              supplier_yield=supplier_yield_post_pred, \n",
    "              demand_samples=demand_samples, \n",
    "              max_order_size=MAX_ORDER_SIZE):\n",
    "    \n",
    "    orders = np.asarray(orders)\n",
    "    losses = []\n",
    "    \n",
    "    # Negative orders are impossible, indicated by np.inf\n",
    "    if np.any(orders < 0):\n",
    "        return np.inf\n",
    "    \n",
    "    # Ordering more than the supplier can ship is also impossible\n",
    "    if np.any(orders > MAX_ORDER_SIZE):\n",
    "        return np.inf\n",
    "    \n",
    "    # Iterate over post pred samples provided in supplier_yield\n",
    "    for i, supplier_yield_sample in supplier_yield.iterrows():\n",
    "        full_yield, price_per_item = calc_yield_and_price(\n",
    "            orders,\n",
    "            supplier_yield=supplier_yield_sample\n",
    "        )\n",
    "        \n",
    "        # evaluate loss over each sample with one sample from the demand distribution\n",
    "        loss_i = loss(full_yield, demand_samples[i], price_per_item)\n",
    "        \n",
    "        losses.append(loss_i)\n",
    "        \n",
    "    return np.asarray(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have all our required functions, let's put things into `scipy`'s optimizer and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the optimization, we're just including the max order sizes as bounds\n",
    "bounds = [(0, max_order) for max_order in MAX_ORDER_SIZE]\n",
    "starting_value = [50., 50., 50.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize the expected loss under all possible scenarios\n",
    "opt_stoch = optimize.minimize(lambda *args: np.mean(objective(*args)), \n",
    "                              starting_value, \n",
    "                              bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal order amount from every supplier is thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_stoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately my scipy optimization fails to converge, and the results are most probably non-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so you have a model with optimal order amounts for each robotic kit for Amazon. But how about just taking the means of the historic yield distribution for each supplier? Wouldn't that be *easier*?\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Let's compare the naive method of just using the means, a point estimate, to expected profit in a Bayesian simulation study.\n",
    "\n",
    "Instead of samples from the posterior predictive, we can just pass a single sample, ***the mean***, into our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_yield_mean = pd.DataFrame([np.mean(d) for d in data]).T\n",
    "supplier_yield_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the demand we expect on average (assume `100`). This way we can still use the above objective function but the loop will just run once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_non_stoch = optimize.minimize(lambda *args: objective(*args, \n",
    "                                                          supplier_yield=supplier_yield_mean, \n",
    "                                                          demand_samples=[100]), \n",
    "                                  starting_value, \n",
    "                                  bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal order amount from every supplier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_non_stoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are certainly different. The full Bayesian treatment is different. Which one is actually better in terms of our profit?\n",
    "\n",
    "To answer that question, we will generate new data from our true generative model and compute the profit in each new scenario given the order amounts from the two optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "data_new = []\n",
    "for supplier_yield, supplier_yield_sd, n_obs in zip(SUPPLIER_YIELD, SUPPLIER_YIELD_SD, N_OBS):\n",
    "    data_new.append(pm.Beta.dist(mu=supplier_yield, sd=supplier_yield_sd, shape=1000).random())\n",
    "data_new = pd.DataFrame(data_new).T\n",
    "data_new.head().add_prefix(\"Robotic Kit \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_loss_stoch = -objective(opt_stoch.x, supplier_yield=data_new) / demand_samples\n",
    "neg_loss_non_stoch = -objective(opt_non_stoch.x, supplier_yield=data_new) / demand_samples\n",
    "sns.distplot(neg_loss_stoch, label='stochastic', kde=False)\n",
    "plt.axvline(np.mean(neg_loss_stoch), label='expected stochastic')\n",
    "sns.distplot(neg_loss_non_stoch, label='non-stochastic', kde=False)\n",
    "plt.axvline(np.mean(neg_loss_non_stoch), color='orange', label='expected non-stochastic')\n",
    "plt.legend(); plt.xlabel('Profit (negative loss)'); plt.ylabel('Occurances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Expected profit of Bayesian model = $%.2f' % np.mean(neg_loss_stoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Expected profit of naive model = $%.2f' % np.mean(neg_loss_non_stoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we demonstrated an expected increase in profit of around 10% for the Bayesian model, which is millions of $ to Amazon!\n",
    "\n",
    "Unfortuantely our scipy optimization did not converge and our results are probably skewed wrong. That's a bummer. I'll fix that for next time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Once we have a Bayesian model and an objective function we can apply **Bayesian Decision Theory** to make better decisions. There is a [mathematical proof](http://www.ee.columbia.edu/~vittorio/BayesProof.pdf) that shows this to be optimal. But there are also more practical and intuitive reasons. The first major reason is that we do not just optimize over the most likely future scenario, but *all* possible future scenarios. \n",
    "\n",
    "This specialized data science use case is applicable to other situations in other industries, for example New and Used car lots.\n",
    "\n",
    "## Reference\n",
    "\n",
    "- James Berger, ***Statistical Decision Theory and Bayesian Analysis***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
