{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:right;\">Statistical Translation</div>\n",
    "<div style=\"text-align:right;\">Zixiao Wang with materials from Dino Konstantopoulos</div>\n",
    "<div style=\"text-align:right;\">2020.2.18</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import multiprocessing\n",
    "from operator import itemgetter\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "def load_corpus(corpus):\n",
    "    with open(corpus, 'r') as f:\n",
    "        sents = f.read().split('\\n')\n",
    "    sents = [sent.lower() for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    sents = load_corpus(corpus)\n",
    "    vocab = set()\n",
    "    for sent in sents:\n",
    "        for word in sent.split():\n",
    "            vocab.add(word)\n",
    "    return list(vocab)\n",
    "\n",
    "def init_t(f_sents, e_sents, e_vocab):\n",
    "    t = {e: {} for e in e_vocab}\n",
    "    for f_sent, e_sent in zip(f_sents, e_sents):\n",
    "        for e in e_sent.split():\n",
    "            for f in f_sent.split():\n",
    "                t[e][f] = 1 / len(e_vocab)\n",
    "    return t\n",
    "\n",
    "def distance(t_1, t_2):\n",
    "    delta = 0\n",
    "    for e in list(t_1.keys()):\n",
    "        for f in list(t_1[e].keys()):\n",
    "            delta += (t_1[e][f] - t_2[e][f]) ** 2\n",
    "    return math.sqrt(delta)\n",
    "\n",
    "def is_converged(t_prev, t_curr, epsilon):\n",
    "    delta = distance(t_prev, t_curr)\n",
    "    return delta < epsilon, delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_iter(f_sents, e_sents, f_vocab, e_vocab, t_prev):\n",
    "    t = deepcopy(t_prev)\n",
    "\n",
    "    # Initialize count(e|f) and total(f)\n",
    "    count = {e: {f: 0 for f in f_vocab}\n",
    "             for e in e_vocab}\n",
    "    total = {f: 0 for f in f_vocab}\n",
    "\n",
    "    for f_sent, e_sent in zip(f_sents, e_sents):\n",
    "        fs = f_sent.split()\n",
    "        es = e_sent.split()\n",
    "        # In fact s_total is a float,\n",
    "        # we make it a dict for better readability\n",
    "        s_total = {e: 0 for e in e_vocab}\n",
    "\n",
    "        # Compute normalization\n",
    "        # Eq 4.13 denominator part\n",
    "        for e in es:\n",
    "            s_total[e] = 0\n",
    "            for f in fs:\n",
    "                s_total[e] += t[e][f]\n",
    "\n",
    "        # Collect counts\n",
    "        for e in es:\n",
    "            for f in fs:\n",
    "                # Eq 4.14 numerator part\n",
    "                count[e][f] += t[e][f] / s_total[e]\n",
    "                # Eq 4.14 denominator part\n",
    "                total[f] += t[e][f] / s_total[e]\n",
    "\n",
    "    # Estimate probabilities\n",
    "    # Eq 4.14\n",
    "    for e in t.keys():\n",
    "        for f in t[e].keys():\n",
    "            t[e][f] = count[e][f] / total[f]\n",
    "\n",
    "    return t\n",
    "\n",
    "def train(f_corpus, e_corpus, epsilon, iter_num, save_dir, save_iteration=False, save_alignment=True):\n",
    "    f_sents = load_corpus(f_corpus)\n",
    "    e_sents = load_corpus(e_corpus)\n",
    "    f_vocab = build_vocab(f_corpus)\n",
    "    e_vocab = build_vocab(e_corpus)\n",
    "\n",
    "    t_prev = init_t(f_sents, e_sents, e_vocab)\n",
    "\n",
    "    converged = False\n",
    "    i = 0\n",
    "    if save_iteration:\n",
    "        output_iteration(t_prev, save_dir, i)\n",
    "\n",
    "    while not converged and i < iter_num:\n",
    "        t = train_iter(f_sents, e_sents, f_vocab, e_vocab, t_prev)\n",
    "        # t = train_iter_parallel(f_sents, e_sents, f_vocab, e_vocab, t_prev, multiprocessing.cpu_count())\n",
    "        converged, delta = is_converged(t_prev, t, epsilon)\n",
    "        i += 1\n",
    "\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())) +\n",
    "              '  Iteration {} finished! Delta = {}.'.format(i, delta))\n",
    "        if save_iteration:\n",
    "            output_iteration(t, save_dir, i)\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())) +\n",
    "                  '    Iteration information saved!')\n",
    "        if save_alignment:\n",
    "            output_alignment(t, save_dir)\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())) +\n",
    "                  '    Alignment result saved!')\n",
    "\n",
    "        t_prev = t\n",
    "    return t\n",
    "\n",
    "def output_iteration(t, save_dir, iter_num):\n",
    "    if iter_num == 0:\n",
    "        e_vocab = list(t.keys())\n",
    "        df = pd.DataFrame(columns=['e', 'f', str(iter_num)+' it.'], index=[0])\n",
    "        for e in e_vocab:\n",
    "            for f in t[e].keys():\n",
    "                df.loc[df.index.max() + 1] = [e, f, t[e][f]]\n",
    "        df.drop([0], inplace=True)\n",
    "        df.to_csv(save_dir + 'iterations.csv', index=False)\n",
    "    else:\n",
    "        df = pd.read_csv(save_dir + 'iterations.csv')\n",
    "        df[str(iter_num) + ' it.'] = list(map(lambda e, f: t[e][f], df['e'], df['f']))\n",
    "        df.to_csv(save_dir + 'iterations.csv', index=False)\n",
    "\n",
    "def output_alignment(t, save_dir):\n",
    "    # Save all candidate alignment with probabilities.\n",
    "    with open(save_dir + 'alignment_all.json', 'w') as f:\n",
    "        json.dump(t, f, ensure_ascii=False)\n",
    "\n",
    "    # Save one to one alignment without probabilities.\n",
    "    res = {e: sorted(fs.items(),\n",
    "                     key=itemgetter(1),\n",
    "                     reverse=True)[0][0]\n",
    "           for e, fs in t.items()}\n",
    "    with open(save_dir + 'alignment.json', 'w') as f:\n",
    "        json.dump(res, f, ensure_ascii=False)\n",
    "\n",
    "    # Save one to one alignment with probabilities in descending order.\n",
    "    res = {e: sorted(fs.items(),\n",
    "                     key=itemgetter(1),\n",
    "                     reverse=True)[0]\n",
    "           for e, fs in t.items()}\n",
    "    res = sorted(res.items(),\n",
    "                 key=lambda x: x[1][1],\n",
    "                 reverse=True)\n",
    "    with open(save_dir + 'alignment.txt', 'w') as f:\n",
    "        for e, f_prob in res:\n",
    "            f.write('{}\\t{}\\t{}\\n'.format(e, f_prob[0], f_prob[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce the statistical translation by statistical tagger\n",
    "\n",
    "We can use the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) to build a [POS tagger](https://en.wikipedia.org/wiki/Part-of-speech_tagging), first using a simple [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model (***most probable POS by count***), then using a **Hidden Markov Model** (HMM) that gets *transition* and *emission* probabilities from [POS bigrams](https://en.wikipedia.org/wiki/Bigram) (given a POS, what's the most probable ***next*** POS in the sentence?).\n",
    "\n",
    "We can divide the Brown corpus into training and test sets, and compare accuraces for BOW and HMM models.\n",
    "\n",
    "We can use some advanced python structures that are often used in Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical translation\n",
    "\n",
    "Basically, the statistical translation is to bulid a mapping function base on probability. Like how many times that one word in one language will translate in other language.\n",
    "\n",
    "Let's say we want to translate \"你好吗\" into \"How are you?\"\n",
    "\n",
    "We need to know the probability to transfer \"你\" into \"You\", etc.\n",
    "\n",
    "In order to train translation, we need some parallel corpus for two language.\n",
    "\n",
    "Here I'm going to use Chinese-English paralled corpus\n",
    "\n",
    "In order to gain a higher accurace, we also use Bigram method to improve the performace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Use the methodology in this notebook to build a statistical language translator, *from your language to english*. So, from Hindi or Chinese to English. Teams of **3** students. You *have* to use a Hidden Markov Model and `pomegranate` as your HMM library, to ensure all student teams start from the same baseline. Start from a Most Frequent Word (BOW) translation baseline, then move on to a Hidden Markov Model to improve translation. How much can you improve it by? The translation engine with the best accuracy, per language, will be presented in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict, namedtuple, OrderedDict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import os\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_file = './data/fbis.en.10k'\n",
    "zh_file = './data/fbis.zh.10k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['伊犁', '大规模', '开展', '“', '面', '对', '面', '”', '宣讲', '活动\\n']\n",
      "56\n",
      "['新华社', '乌鲁木齐', '2', '月', '1', '日', '电', '(', '记者', '樊英利', '、', '丁建刚', '、', '李秀芩', ')', '为', '促进', '民族', '团结', ',', '维护', '社会', '安定', ',', '近年', '来', '新疆', '伊犁', '创造', '性地', '开展', '了', '“', '面', '对', '面', '”', '宣讲', '活动', ',', '让', '各', '族', '群众', '听到', '了', '党', '和', '政府', '的', '声音', ',', '受到', '热烈', '欢迎', '。\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(zh_file,'r') as test_zh:\n",
    "    for i in range(2):\n",
    "        line = test_zh.readline().split(\" \")\n",
    "        print(len(line))\n",
    "        print(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['\"', 'Yili', 'Launches', 'Large-Scale', \"'\", 'Face-to-face', \"'\", 'Propaganda', 'Activity', '\"\\n']\n",
      "64\n",
      "['Urumqi', ',', '1', 'Feb', '(', 'Xinhua', ')', '--', 'Over', 'the', 'past', 'few', 'years', ',', 'in', 'order', 'to', 'promote', 'nationality', 'solidarity', 'and', 'safeguard', 'social', 'stability', ',', 'Xinjiang', \"'s\", 'Yili', 'has', 'launched', 'in', 'a', 'creative', 'way', 'a', '\"', 'face-to-face', '\"', 'propaganda', 'activity', 'to', 'let', 'the', 'people', 'of', 'all', 'nationalities', 'hear', 'the', 'voice', 'of', 'the', 'party', 'and', 'government', ',', 'and', 'the', 'activity', 'has', 'been', 'warmly', 'welcomed', '.\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(en_file,'r') as test_en:\n",
    "    for i in range(2):\n",
    "        line = test_en.readline().split(\" \")\n",
    "        print(len(line))\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corpus):\n",
    "    with open(corpus, 'r') as f:\n",
    "        sents = f.read().split('\\n')\n",
    "    sents = [sent.lower() for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    sents = load_corpus(corpus)\n",
    "    vocab = set()\n",
    "    for sent in sents:\n",
    "        for word in sent.split():\n",
    "            vocab.add(word)\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/vocabulart_en.txt','w') as test:\n",
    "    for sent in build_vocab(en_file):\n",
    "        test.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', '引言']\n",
      "[':', '略论']\n",
      "['summary', '略论']\n"
     ]
    }
   ],
   "source": [
    "with open('./data/alignment.txt') as test:\n",
    "    for i in range(3):\n",
    "        print((test.readline().split(\"\\t\"))[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(build_vocab(zh_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(f_corpus='./data/fbis.zh.10k',e_corpus='./data/fbis.en.10k',epsilon=1e3,iter_num=10,save_dir='./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))\n",
    "\n",
    "def read_tags(filename):\n",
    "    \"\"\"Read a list of word tag classes\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
